import streamlit as st
import pandas as pd
import re
import requests # Gemini APIå‘¼ã³å‡ºã—ç”¨
import time # ãƒªãƒˆãƒ©ã‚¤ç”¨
import json # --- JSONãƒ‘ãƒ¼ã‚¹ã®ãŸã‚ã«è¿½åŠ  ---
import plotly.graph_objects as go # --- â–¼ Plotly ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ ---
import plotly.express as px # --- â–¼ Plotly Express (ã‚«ãƒ©ãƒ¼ãƒ‘ãƒ¬ãƒƒãƒˆç”¨) ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ ---
from janome.tokenizer import Tokenizer
from wordcloud import WordCloud
import networkx as nx 
from networkx.algorithms.community import greedy_modularity_communities
import matplotlib.pyplot as plt
import matplotlib.font_manager
from collections import Counter
from itertools import combinations
import japanize_matplotlib # Matplotlibã®æ—¥æœ¬èªåŒ– (WordCloud ã¨ NetworkX ã§ä¾ç„¶ã¨ã—ã¦å¿…è¦)
import numpy as np 
from scipy.stats import chi2_contingency 
import io 
import base64 
from streamlit.components.v1 import html 

# --- 1. ã‚¢ãƒ—ãƒªã®åŸºæœ¬è¨­å®š ---
st.set_page_config(page_title="çµ±è¨ˆï¼‹AI çµ±åˆã‚¢ãƒŠãƒ©ã‚¤ã‚¶ãƒ¼ (Plotly Ver.)", layout="wide")
st.title("çµ±è¨ˆï¼‹AI çµ±åˆãƒ†ã‚­ã‚¹ãƒˆã‚¢ãƒŠãƒ©ã‚¤ã‚¶ãƒ¼ ğŸ“ŠğŸ¤– (Plotly Ver.)")
st.write("Excelã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã€ãƒ†ã‚­ã‚¹ãƒˆåˆ—ã¨åˆ†æè»¸ï¼ˆå±æ€§ï¼‰ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚çµ±è¨ˆåˆ†æã¨AIã«ã‚ˆã‚‹è¦ç´„ãƒ»ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ†æã‚’åŒæ™‚ã«å®Ÿè¡Œã—ã¾ã™ã€‚")

# --- 2. å½¢æ…‹ç´ è§£æï¼†ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰è¨­å®š (ã‚­ãƒ£ãƒƒã‚·ãƒ¥) ---
@st.cache_resource 
def get_tokenizer():
    return Tokenizer()

BASE_STOPWORDS = set([
    'ã®', 'ã«', 'ã¯', 'ã‚’', 'ãŸ', 'ã§ã™', 'ã¾ã™', 'ãŒ', 'ã§', 'ã‚‚', 'ã¦', 'ã¨', 'ã—', 'ã‚Œ', 'ã•', 'ã‚ã‚‹', 'ã„ã‚‹', 'ã™ã‚‹',
    'ãªã„', 'ã“ã¨', 'ã‚‚ã®', 'ã“ã‚Œ', 'ãã‚Œ', 'ã‚ã‚Œ', 'ã‚ˆã†', 'ãŸã‚', 'äºº', 'ä¸­', 'ç­‰', 'æ€ã†', 'ã„ã†', 'ãªã‚‹', 'æ—¥', 'æ™‚',
    'ãã ã•ã‚‹', 'ã„ãŸã ã', 'ã—ã‚Œã‚‹', 'ãã‚‹', 'ãŠã‚‹', 'ã‚Œã‚‹', 'ã‚‰ã‚Œã‚‹', 'ã›ã‚‹', 'ã•ã›ã‚‹', 'ã§ãã‚‹', 'ãªã‚‹', 'ã‚„ã‚‹', 'ã„ã',
    'è¡Œã†', 'è¨€ã†', 'ç”³ã—ä¸Šã’ã‚‹', 'ã¾ã„ã‚‹', 'è¦‹ã‚‹', 'ã“ã“', 'ãã“', 'ã‚ãã“', 'ã“ã¡ã‚‰', 'ãã¡ã‚‰', 'ã‚ã¡ã‚‰', 'ã“ã®', 'ãã®', 'ã‚ã®'
])

# AIåˆ†æã®æœ€å¤§æ–‡å­—æ•°åˆ¶é™ã‚’å®šç¾©
MAX_AI_INPUT_CHARS = 1000000

@st.cache_data 
def extract_words(text, _tokenizer): 
    if not isinstance(text, str):
        return []
    tokenizer = get_tokenizer()
    tokens = tokenizer.tokenize(text)
    words = []
    for token in tokens:
        if token.surface.isdigit(): continue
        if token.base_form.isdigit(): continue
        part_of_speech = token.part_of_speech.split(',')[0]
        if part_of_speech in ['åè©', 'å‹•è©', 'å½¢å®¹è©']:
            if len(token.base_form) > 1: 
                words.append(token.base_form)
    return words

# --- 3. Gemini AI åˆ†æé–¢æ•° (ä¼šè©±å¯¾å¿œç‰ˆ) ---

# --- â–¼ ä¿®æ­£ç‚¹: AI ã‚µãƒãƒªãƒ¼ (è¡¨å½¢å¼) ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ ---
# 1. ã‚·ãƒ³ãƒ—ãƒ«ãªè¦ç´„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ (å›ºå®š)
SYSTEM_PROMPT_SIMPLE = """ã‚ãªãŸã¯ã€ãƒ†ã‚­ã‚¹ãƒˆãƒã‚¤ãƒ‹ãƒ³ã‚°ã®å°‚é–€å®¶ã§ã™ã€‚
ä¸ãˆã‚‰ã‚ŒãŸ[åˆ†æå¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆ]ã¨ã€AIã«ã‚ˆã‚‹[ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ†æçµæœJSON]ã®ä¸¡æ–¹ã‚’å‚ç…§ã—ã€åˆ†æã‚µãƒãƒªãƒ¼ã‚’ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³å½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚

[ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ†æçµæœJSON]
{cluster_json_data}

[ã‚ãªãŸã®ã‚¿ã‚¹ã‚¯]
{analysis_scope_instruction}
{attributeInstruction}

## 1. åˆ†æã‚µãƒãƒªãƒ¼
(å…¨ä½“ã®å‚¾å‘ã‚’ç°¡æ½”ã«è¦ç´„)

## 2. ä¸»è¦ãªãƒ†ãƒ¼ãƒ
(**é‡è¦**: ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¯ã€**ä¸Šè¨˜[ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ†æçµæœJSON]ã«å³å¯†ã«å¾“ã„**ã€ä»¥ä¸‹ã®**Markdownãƒ†ãƒ¼ãƒ–ãƒ«å½¢å¼**ã§è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚)

| ä¸»è¦ãƒ†ãƒ¼ãƒï¼ˆã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ï¼‰ | å‰²åˆ (%) | æ¦‚è¦ (ã‚µãƒ–ãƒˆãƒ”ãƒƒã‚¯ã‚’å«ã‚€) |
| :--- | :---: | :--- |
| [ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼Aã®åå‰] | [ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼Aã®å‰²åˆâ€»] | [ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼Aã®ã‚µãƒ–ãƒˆãƒ”ãƒƒã‚¯ã‚’ç°¡æ½”ã«ã¾ã¨ã‚ãŸæ¦‚è¦] |
| [ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼Bã®åå‰] | [ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼Bã®å‰²åˆâ€»] | [ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼Bã®ã‚µãƒ–ãƒˆãƒ”ãƒƒã‚¯ã‚’ç°¡æ½”ã«ã¾ã¨ã‚ãŸæ¦‚è¦] |
| ... | ... | ... |
â€»å‰²åˆã¯ã€ã‚µãƒ–ãƒˆãƒ”ãƒƒã‚¯ã® `value` ã®åˆè¨ˆã‚’ % ã«ã—ãŸã‚‚ã®ã§ã™ã€‚ (ä¾‹: 15.2)

## 3. ãƒã‚¸ãƒ†ã‚£ãƒ–ãªæ„è¦‹
(ä¸ãˆã‚‰ã‚ŒãŸ[åˆ†æå¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆ]å…¨ä½“ã‹ã‚‰ã€å…·ä½“çš„ãªè‰¯ã„ç‚¹ã‚’å¼•ç”¨ã—ã¤ã¤ãƒªã‚¹ãƒˆã‚¢ãƒƒãƒ—ã—ã¦ãã ã•ã„ã€‚**å¼•ç”¨ã™ã‚‹éš›ã¯ã€[è¡Œç•ªå·: XX] ã‚‚å«ã‚ã¦ãã ã•ã„ã€‚**)

## 4. ãƒã‚¬ãƒ†ã‚£ãƒ–ãªæ„è¦‹ãƒ»èª²é¡Œ
(ä¸ãˆã‚‰ã‚ŒãŸ[åˆ†æå¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆ]å…¨ä½“ã‹ã‚‰ã€å…·ä½“çš„ãªä¸æº€ã‚„æ”¹å–„ç‚¹ã‚’å¼•ç”¨ã—ã¤ã¤ãƒªã‚¹ãƒˆã‚¢ãƒƒãƒ—ã—ã¦ãã ã•ã„ã€‚**å¼•ç”¨ã™ã‚‹éš›ã¯ã€[è¡Œç•ªå·: XX] ã‚‚å«ã‚ã¦ãã ã•ã„ã€‚**)

## 5. å°‘æ•°ã ãŒæ³¨ç›®ã™ã¹ãæ„è¦‹
(ä¸ãˆã‚‰ã‚ŒãŸ[åˆ†æå¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆ]å…¨ä½“ã‹ã‚‰ã€ä»¶æ•°ã¯å°‘ãªã„ï¼ˆä¾‹: 1ã€œ2ä»¶ï¼‰ã‹ã‚‚ã—ã‚Œãªã„ãŒã€éå¸¸ã«é‡è¦ã€ãƒ¦ãƒ‹ãƒ¼ã‚¯ã€ã¾ãŸã¯ç¤ºå”†ã«å¯Œã‚€æ„è¦‹ãŒã‚ã‚Œã°ã€ã“ã“ã«æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚**å¼•ç”¨ã™ã‚‹éš›ã¯ã€[è¡Œç•ªå·: XX] ã‚‚å«ã‚ã¦ãã ã•ã„ã€‚**)

{has_attribute}

## 7. ç·è©•ã¨ãƒã‚¯ã‚¹ãƒˆã‚¢ã‚¯ã‚·ãƒ§ãƒ³
(åˆ†æã‹ã‚‰è¨€ãˆã‚‹ã“ã¨ã€æ¬¡ã«è¡Œã†ã¹ãã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’ææ¡ˆ)
"""
# --- â–² ä¿®æ­£å®Œäº† â–² ---


# 2. å­¦è¡“è«–æ–‡ç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ (å›ºå®š)
SYSTEM_PROMPT_ACADEMIC = """ã‚ãªãŸã¯ã€ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æã™ã‚‹è¨ˆé‡ãƒ†ã‚­ã‚¹ãƒˆåˆ†æï¼ˆãƒ†ã‚­ã‚¹ãƒˆãƒã‚¤ãƒ‹ãƒ³ã‚°ï¼‰ã®å°‚é–€å®¶ã§ã™ã€‚
ä¸ãˆã‚‰ã‚ŒãŸ[åˆ†æå¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆ]ã¨ã€AIã«ã‚ˆã‚‹[ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ†æçµæœJSON]ã®ä¸¡æ–¹ã‚’å‚ç…§ã—ã€å­¦è¡“è«–æ–‡å½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚

[ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ†æçµæœJSON]
{cluster_json_data}

[ã‚ãªãŸã®ã‚¿ã‚¹ã‚¯]
{analysis_scope_instruction}
{attributeInstruction}
ä»¥ä¸‹ã®æ§‹æˆã«å¾“ã£ã¦ã€ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³å½¢å¼ã§è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚

## 1. åˆ†æã®æ¦‚è¦ (Abstract)
(ãƒ‡ãƒ¼ã‚¿å…¨ä½“ã®ä¸»è¦ãªå‚¾å‘ã‚„ç‰¹ç­†ã™ã¹ãç‚¹ã‚’ã€å®¢è¦³çš„ãªè¦ç´„ã¨ã—ã¦2ã€œ3æ–‡ã§è¨˜è¿°ã™ã‚‹)

## 2. ä¸»è¦ãªçŸ¥è¦‹ (Key Findings)
(**é‡è¦**: ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¯ã€**ä¸Šè¨˜[ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ†æçµæœJSON]ã«å³å¯†ã«å¾“ã£ã¦**è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚JSONã® `children` (ä¸»è¦ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼) ã‚’ä¸»è¦ãªçŸ¥è¦‹ã¨ã—ã¦å–ã‚Šä¸Šã’ã€ãã®å†…å®¹ï¼ˆã‚µãƒ–ãƒˆãƒ”ãƒƒã‚¯ï¼‰ã¨å‰²åˆã«ã¤ã„ã¦å®¢è¦³çš„ã«è¨˜è¿°ã™ã‚‹ã€‚**å¼•ç”¨ã™ã‚‹éš›ã¯ã€[è¡Œç•ªå·: XX] ã‚‚å«ã‚ã‚‹ã“ã¨ã€‚**)

## 3. ãã®ä»–ã®æ³¨ç›®ã™ã¹ãæ‰€è¦‹ (Other Notable Findings)
(é »åº¦ã¯ä½ã„ï¼ˆä¾‹: 1ã€œ2ä»¶ï¼‰ã‚‚ã®ã®ã€åˆ†æä¸Šè¦‹éã”ã™ã¹ãã§ã¯ãªã„ç‰¹ç•°ãªæ„è¦‹ã€ã¾ãŸã¯å°†æ¥ã®èª²é¡Œã‚’ç¤ºå”†ã™ã‚‹ã‚ˆã†ãªæ„è¦‹ãŒã‚ã‚Œã°ã€ã“ã“ã«è¨˜è¿°ã™ã‚‹ã€‚**å¼•ç”¨ã™ã‚‹éš›ã¯ã€[è¡Œç•ªå·: XX] ã‚‚å«ã‚ã‚‹ã“ã¨ã€‚**)
{has_attribute}
## 5. è€ƒå¯Ÿã¨ä»Šå¾Œã®èª²é¡Œ (Discussion and Limitations)
(åˆ†æçµæœã‹ã‚‰å°ã‹ã‚Œã‚‹è€ƒå¯Ÿã‚„ç¤ºå”†ã‚’è¨˜è¿°ã™ã‚‹ã€‚ã¾ãŸã€ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰è¦‹ã‚‰ã‚Œã‚‹æ½œåœ¨çš„ãªèª²é¡Œã‚„ã€ã•ã‚‰ãªã‚‹åˆ†æã®æ–¹å‘æ€§ã«ã¤ã„ã¦ã‚‚è¨€åŠã™ã‚‹)
"""

# --- â–¼ ä¿®æ­£ç‚¹: JSONãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ (name ã«å‰²åˆã‚’å«ã‚ãªã„) ---
# 3. ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ†æ (JSONç”Ÿæˆç”¨) ã®ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
SYSTEM_PROMPT_CLUSTER_JSON = """ã‚ãªãŸã¯é«˜åº¦ãªãƒ†ã‚­ã‚¹ãƒˆã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å°‚é–€ã®ã‚¢ãƒŠãƒªã‚¹ãƒˆã§ã™ã€‚æä¾›ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æã—ã€ä¸»è¦ãªè¨€èª¬ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ï¼ˆ3ã€œ5å€‹ï¼‰ã¨ã€ãã‚Œã‚‰ã‚’æ§‹æˆã™ã‚‹ã‚µãƒ–ãƒˆãƒ”ãƒƒã‚¯ï¼ˆå„ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼å†…ã§3ã€œ5å€‹ï¼‰ã«åˆ†é¡ã—ã¦ãã ã•ã„ã€‚
{analysis_scope_instruction}

[ã‚¿ã‚¹ã‚¯]
1. ãƒ†ã‚­ã‚¹ãƒˆå…¨ä½“ã‚’èª­ã¿ã€ä¸»è¦ãªãƒ†ãƒ¼ãƒï¼ˆã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ï¼‰ã‚’3ã€œ5å€‹ç‰¹å®šã—ã¾ã™ã€‚
2. å„ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã‚’æ§‹æˆã™ã‚‹ã€ã‚ˆã‚Šè©³ç´°ãªã‚µãƒ–ãƒˆãƒ”ãƒƒã‚¯ã‚’3ã€œ5å€‹ç‰¹å®šã—ã¾ã™ã€‚
3. å„ã‚µãƒ–ãƒˆãƒ”ãƒƒã‚¯ãŒã€åˆ†æå¯¾è±¡ãƒ‡ãƒ¼ã‚¿**å…¨ä½“**ï¼ˆ{analyzed_items}ä»¶ï¼‰ã®ä¸­ã§å ã‚ã‚‹ãŠãŠã‚ˆãã®å‰²åˆï¼ˆ%ï¼‰ã‚’è¨ˆç®—ã—ã¾ã™ã€‚
4. `name` ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã«ã¯ `[ãƒˆãƒ”ãƒƒã‚¯å]` ã®ã¿ã‚’å…¥ã‚Œã¦ãã ã•ã„ï¼ˆ**å‰²åˆ(%)ã®æ–‡å­—åˆ—ã¯å«ã‚ãªã„ã§ãã ã•ã„**ï¼‰ã€‚
5. `value` ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã«ã¯ã€ã‚µãƒ–ãƒˆãƒ”ãƒƒã‚¯ã®å‰²åˆï¼ˆãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸ã®æ•°å€¤ã®ã¿ï¼‰ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚
6. **é‡è¦**: å‡ºåŠ›ã¯ã€æŒ‡å®šã•ã‚ŒãŸJSONã‚¹ã‚­ãƒ¼ãƒã«å³å¯†ã«å¾“ã£ã¦ãã ã•ã„ã€‚

[ä¾‹]
{{
  "name": "å…¨ä½“",
  "children": [
    {{
      "name": "ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼A",
      "children": [
        {{ "name": "ã‚µãƒ–ãƒˆãƒ”ãƒƒã‚¯A1", "value": 15.0 }},
        {{ "name": "ã‚µãƒ–ãƒˆãƒ”ãƒƒã‚¯A2", "value": 10.0 }}
      ]
    }},
    {{
      "name": "ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼B",
      "children": [
        {{ "name": "ã‚µãƒ–ãƒˆãƒ”ãƒƒã‚¯B1", "value": 20.0 }}
      ]
    }}
  ]
}}
"""
# --- â–² ä¿®æ­£å®Œäº† â–² ---

# 4. ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ†æ (ãƒ†ã‚­ã‚¹ãƒˆè§£é‡ˆç”¨) ã®ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
SYSTEM_PROMPT_CLUSTER_TEXT = """ã‚ãªãŸã¯ãƒ†ã‚­ã‚¹ãƒˆã‚¢ãƒŠãƒªã‚¹ãƒˆã§ã™ã€‚ä»¥ä¸‹ã®JSONã¯ã€ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ†æã—ãŸçµæœã§ã™ã€‚
{analysis_scope_instruction}

[åˆ†æçµæœJSON]
{json_data}

[ã‚ãªãŸã®ã‚¿ã‚¹ã‚¯]
ã“ã®JSONãƒ‡ãƒ¼ã‚¿ã‚’è§£é‡ˆã—ã€åˆ†æçµæœã‚’ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³å½¢å¼ã§åˆ†ã‹ã‚Šã‚„ã™ãèª¬æ˜ã—ã¦ãã ã•ã„ã€‚
**å¿…ãšä»¥ä¸‹ã®æ§‹æˆã«å¾“ã£ã¦ãã ã•ã„ã€‚**

## å‡¡ä¾‹ (è‰²ã¨ä¸»è¦ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼)
ï¼ˆã‚°ãƒ©ãƒ•ã®å„è‰²ï¼ˆä¾‹: è–„ã„é’ã€è–„ã„ç·‘ãªã©ï¼‰ãŒã€ã©ã®ä¸»è¦ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã«å¯¾å¿œã—ã¦ã„ã‚‹ã‹ã‚’ãƒªã‚¹ãƒˆå½¢å¼ã§èª¬æ˜ã—ã¦ãã ã•ã„ã€‚**è‰²ã¯è‡ªå‹•ã§å‰²ã‚Šå½“ã¦ã‚‰ã‚Œã¦ã„ã¾ã™ã€‚**ï¼‰
- [è‰²1 (ä¾‹: è–„ã„é’)]: [ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼Aã®åå‰]
- [è‰²2 (ä¾‹: è–„ã„ç·‘)]: [ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼Bã®åå‰]
- [è‰²3 (ä¾‹: è–„ã„ã‚ªãƒ¬ãƒ³ã‚¸)]: [ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼Cã®åå‰]
...

## AIã«ã‚ˆã‚‹ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã®è§£é‡ˆ
ï¼ˆæ¬¡ã«ã€å„ä¸»è¦ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ãŒã©ã®ã‚ˆã†ãªæ„è¦‹ã‚°ãƒ«ãƒ¼ãƒ—ãªã®ã‹ã‚’è©³ç´°ã«èª¬æ˜ã—ã¦ãã ã•ã„ã€‚ã‚µãƒ–ãƒˆãƒ”ãƒƒã‚¯ã«ã‚‚è§¦ã‚ŒãªãŒã‚‰ã€ãªãœãã®ã‚ˆã†ã«åˆ†é¡ã•ã‚ŒãŸã®ã‹ã‚’å…·ä½“çš„ã«è€ƒå¯Ÿã—ã¦ãã ã•ã„ã€‚ï¼‰
"""


# 5. ä¼šè©±ç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ (å¯å¤‰)
SYSTEM_PROMPT_CHAT = """ã‚ãªãŸã¯ã€ä¸ãˆã‚‰ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ï¼ˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼‰ã«é–¢ã™ã‚‹è³ªå•ã«ç­”ãˆã‚‹ã€å„ªç§€ãªãƒ‡ãƒ¼ã‚¿ã‚¢ãƒŠãƒªã‚¹ãƒˆã§ã™ã€‚
ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¯ `[è¡Œç•ªå·: XX] [å±æ€§...] || ãƒ†ã‚­ã‚¹ãƒˆ` ã®å½¢å¼ã§æä¾›ã•ã‚Œã¾ã™ã€‚
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®è³ªå•ã«å¯¾ã—ã€æä¾›ã•ã‚ŒãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã«åŸºã¥ã„ã¦ã€ç°¡æ½”ã‹ã¤çš„ç¢ºã«å›ç­”ã—ã¦ãã ã•ã„ã€‚
ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«å«ã¾ã‚Œã¦ã„ãªã„æƒ…å ±ã«ã¤ã„ã¦ã¯ã€ãã®æ—¨ã‚’æ­£ç›´ã«ä¼ãˆã¦ãã ã•ã„ã€‚
"""

# 6. æ„Ÿæƒ…åˆ†æ (JSONç”Ÿæˆç”¨) ã®ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
SYSTEM_PROMPT_SENTIMENT_JSON = """ã‚ãªãŸã¯é«˜åº¦ãªæ„Ÿæƒ…åˆ†æå°‚é–€ã®ã‚¢ãƒŠãƒªã‚¹ãƒˆã§ã™ã€‚
{analysis_scope_instruction}

[ã‚¿ã‚¹ã‚¯]
1. æä¾›ã•ã‚ŒãŸ[åˆ†æå¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆ]ã®å…¨ä»¶ã‚’èª­ã¿è¾¼ã¿ã¾ã™ã€‚
2. å„ãƒ†ã‚­ã‚¹ãƒˆãŒã€Œãƒã‚¸ãƒ†ã‚£ãƒ–ã€ã€Œãƒã‚¬ãƒ†ã‚£ãƒ–ã€ã€Œä¸­ç«‹ã€ã®ã©ã‚Œã«è©²å½“ã™ã‚‹ã‹ã‚’åˆ†é¡ã—ã¾ã™ã€‚
3. æœ€çµ‚çš„ã«ã€3ã¤ã®ã‚«ãƒ†ã‚´ãƒªãã‚Œãã‚Œã®**åˆè¨ˆä»¶æ•° (count)** ã¨ã€åˆ†æå¯¾è±¡ãƒ‡ãƒ¼ã‚¿å…¨ä½“ï¼ˆ{analyzed_items}ä»¶ï¼‰ã«å ã‚ã‚‹**å‰²åˆ (percentage)** ã‚’è¨ˆç®—ã—ã¾ã™ã€‚
4. **é‡è¦**: `count` ã®åˆè¨ˆã¯ã€åˆ†æå¯¾è±¡ã®ç·ä»¶æ•° {analyzed_items} ä»¶ã¨ä¸€è‡´ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
5. **é‡è¦**: å‡ºåŠ›ã¯ã€æŒ‡å®šã•ã‚ŒãŸJSONã‚¹ã‚­ãƒ¼ãƒã«å³å¯†ã«å¾“ã£ã¦ãã ã•ã„ã€‚
"""


def call_gemini_api(contents, system_instruction=None, generation_config=None):
    try: apiKey = st.secrets["GEMINI_API_KEY"]
    except Exception: return "AIåˆ†æã‚¨ãƒ©ãƒ¼: Streamlit Cloudã®Secretsã« `GEMINI_API_KEY` ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚"
    if not apiKey: return "AIåˆ†æã‚¨ãƒ©ãƒ¼: Streamlit Cloudã®Secretsã« `GEMINI_API_KEY` ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚"

    apiUrl = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-09-2025:generateContent?key={apiKey}"

    payload = {"contents": contents}
    if system_instruction:
        payload["systemInstruction"] = {"parts": [{"text": system_instruction}]}
    
    if generation_config:
        payload["generationConfig"] = generation_config

    try:
        response = None; delay = 1000
        for i in range(5):
            response = requests.post(apiUrl, json=payload, headers={'Content-Type': 'application/json'})
            if response.status_code == 200: break
            elif response.status_code == 429 or response.status_code >= 500: time.sleep(delay / 1000); delay *= 2
            else: response.raise_for_status()
        if response.status_code != 200: return f"AIåˆ†æå¤±æ•— (Status: {response.status_code})"

        result = response.json()
        candidates = result.get('candidates')
        if not candidates: return "AIå¿œç­”ã‚¨ãƒ©ãƒ¼: candidates is missing"
        content = candidates[0].get('content')
        if not content or not content.get('parts'): return "AIå¿œç­”ã‚¨ãƒ©ãƒ¼: content or parts is missing"
        text = content['parts'][0].get('text', '')
        if not text: return "AIã‹ã‚‰ã®å¿œç­”ãŒç©ºã§ã—ãŸã€‚"
        return text
    except Exception as e:
        if "403" in str(e): return "AIåˆ†æã‚¨ãƒ©ãƒ¼: 403 Forbidden. APIã‚­ãƒ¼/è¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
        return f"AIåˆ†æã‚¨ãƒ©ãƒ¼: {e}"

# --- 4. KWICï¼ˆæ–‡è„ˆæ¤œç´¢ï¼‰é–¢æ•° ---
def generate_kwic_html(df, text_column, keyword, max_results=100):
    if not keyword: return "<p>ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚</p>"
    try: search_pattern = keyword.replace('*', '.*'); kwic_pattern = re.compile(f'(.{{0,40}})({search_pattern})(.{{0,40}})', re.IGNORECASE)
    except re.error as e: return f"<p>ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}</p>"
    results = []
    for text in df[text_column].dropna():
        for match in kwic_pattern.finditer(text):
            if len(results) >= max_results: break
            left, center, right = match.groups()
            html_row = f'<div style="margin-bottom: 10px; padding: 5px; border-bottom: 1px solid #eee; font-family: sans-serif;"><span style="text-align: right; display: inline-block; width: 45%; color: #555; overflow: hidden; text-overflow: ellipsis; white-space: nowrap;">...{left}</span><span style="background-color: yellow; font-weight: bold; padding: 2px 0;">{center}</span><span style="text-align: left; display: inline-block; width: 45%; color: #555; overflow: hidden; text-overflow: ellipsis; white-space: nowrap;">{right}...</span></div>'
            results.append(html_row)
        if len(results) >= max_results: break
    if not results: return f"<p>ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€Œ{keyword}ã€ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚</p>"
    return f"<h4>ã€Œ{keyword}ã€ã®æ¤œç´¢çµæœ ({len(results)} ä»¶)</h4><div style='height:400px; overflow-y:scroll; border:1px solid #eee; padding:10px;'>" + "".join(results) + "</div>"


# --- 5. å±æ€§åˆ¥ ç‰¹å¾´èªï¼ˆã‚«ã‚¤äºŒä¹—æ¤œå®šï¼‰é–¢æ•° ---
@st.cache_data
def calculate_characteristic_words(_df, attribute_col, text_col, _stopwords_set):
    results = {}
    try: unique_attrs = _df[attribute_col].dropna().unique()
    except KeyError: return {"error": "å±æ€§åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"}
    if len(unique_attrs) < 2: return {"error": "æ¯”è¼ƒå¯¾è±¡ã®å±æ€§ãŒ2ã¤æœªæº€ã§ã™ã€‚"}
    all_words = set(word for sublist in _df['words'] for word in sublist if word not in _stopwords_set)
    total_docs = len(_df)
    for attr_value in unique_attrs:
        attr_df = _df[_df[attribute_col] == attr_value]; non_attr_df = _df[_df[attribute_col] != attr_value]
        total_docs_in_attr = len(attr_df); total_docs_not_in_attr = total_docs - total_docs_in_attr
        if total_docs_in_attr == 0 or total_docs_not_in_attr == 0: continue
        characteristic_words = []
        for word in all_words:
            a = sum(1 for words_list in attr_df['words'] if word in words_list); b = sum(1 for words_list in non_attr_df['words'] if word in words_list)
            c = total_docs_in_attr - a; d = total_docs_not_in_attr - b
            if (a+b) == 0 or (a+c) == 0 or (b+d) == 0 or (c+d) == 0: continue
            contingency_table = np.array([[a, b], [c, d]])
            try:
                with np.errstate(divide='ignore', invalid='ignore'): chi2, p, dof, expected = chi2_contingency(contingency_table)
                if p < 0.05 and a > expected[0, 0]: characteristic_words.append((word, p, chi2))
            except ValueError: continue
        characteristic_words.sort(key=lambda x: x[1]); results[attr_value] = characteristic_words[:20]
    return results

# --- Plotly Treemap ç”¨ã®ãƒ‡ãƒ¼ã‚¿å¤‰æ›é–¢æ•° ---
def parse_json_for_plotly(json_data_str):
    try:
        data = json.loads(json_data_str)
    except Exception as e:
        return None, None, None, f"JSONè§£æã‚¨ãƒ©ãƒ¼: {e}"

    labels = []
    parents = []
    values = []
    
    root_name = data.get('name', 'å…¨ä½“')
    labels.append(root_name)
    parents.append("")
    values.append(0) 
    
    clusters = data.get('children', [])
    if not clusters:
        return None, None, None, "JSONã« 'children' (ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼) ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"

    for cluster in clusters:
        # AIã¯å‰²åˆã‚’å«ã¾ãªã„åå‰ (ä¾‹: "ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼A") ã‚’è¿”ã™
        cluster_name = cluster.get('name', 'ä¸æ˜ãªã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼') 
        labels.append(cluster_name)
        parents.append(root_name)
        
        sub_topics = cluster.get('children', [])
        
        if not sub_topics:
             values.append(1) 
        else:
            cluster_total_value = 0
            for sub_topic in sub_topics:
                # AIã¯å‰²åˆã‚’å«ã¾ãªã„åå‰ (ä¾‹: "ã‚µãƒ–ãƒˆãƒ”ãƒƒã‚¯A1") ã‚’è¿”ã™
                sub_name = sub_topic.get('name', 'ä¸æ˜ãªãƒˆãƒ”ãƒƒã‚¯')
                sub_value = sub_topic.get('value', 0)
                
                if sub_value > 0:
                    labels.append(sub_name)
                    parents.append(cluster_name)
                    values.append(sub_value)
                    cluster_total_value += sub_value
            
            # è¦ªã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ (æ·±ã•1) ã® value ã‚’è¨­å®š (0 = è‡ªå‹•é›†è¨ˆ)
            values.append(0) 


    return labels, parents, values, None


# --- â–¼ ä¿®æ­£ç‚¹: `Plotly` ã‚’ä½¿ã£ãŸTreemapæç”»é–¢æ•° (ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«æ”¹å–„) ---
def create_plotly_treemap(json_data_str):
    """
    AIãŒç”Ÿæˆã—ãŸJSONãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã€Plotlyã‚’ä½¿ç”¨ã—ã¦
    ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªTreemapã‚’ç”Ÿæˆã—ã¾ã™ã€‚(ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«æ”¹å–„ç‰ˆ)
    """
    labels, parents, values, error = parse_json_for_plotly(json_data_str)
    
    if error:
        return None, error
        
    root_label = labels[0] # "å…¨ä½“"
    clusters = [l for l, p in zip(labels, parents) if p == root_label]
    num_clusters = len(clusters)

    # æŸ”ã‚‰ã‹ã„ãƒ‘ã‚¹ãƒ†ãƒ«ã‚«ãƒ©ãƒ¼ã®ãƒ‘ãƒ¬ãƒƒãƒˆ "Set3" ã‚’ä½¿ç”¨
    if num_clusters > 0:
        colors = px.colors.qualitative.Set3[:num_clusters]
    else:
        colors = px.colors.qualitative.Set3

    fig = go.Figure(go.Treemap(
        labels = labels,
        parents = parents,
        values = values,
        
        # â–¼ ä¿®æ­£ç‚¹: ãƒ†ã‚­ã‚¹ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã§ãƒ©ãƒ™ãƒ«(å¤ªå­—)ã¨å‰²åˆ(æ”¹è¡Œ)ã‚’æŒ‡å®š
        # %{percentRoot} ã¯å…¨ä½“ (root) ã«å¯¾ã™ã‚‹å‰²åˆ
        texttemplate="<b>%{label}</b><br>%{percentRoot:.1%}",
        
        hoverinfo="label+value+percent root", 
        
        # â–¼ ä¿®æ­£ç‚¹: è¦ª(ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼)ã”ã¨ã«è‰²ã‚’å‰²ã‚Šå½“ã¦ (Set3 ãƒ‘ãƒ¬ãƒƒãƒˆ)
        marker_colorscale='Set3',
        branchvalues="total", # è¦ªã®åˆè¨ˆãŒå­ã®åˆè¨ˆã«ãªã‚‹ã‚ˆã†ã«
        
        # â–¼ ä¿®æ­£ç‚¹: ãƒ†ã‚­ã‚¹ãƒˆã®è‡ªå‹•èª¿æ•´ã¨é‡ãªã‚Šé˜²æ­¢
        # é ˜åŸŸã«åã¾ã‚‹ã‚ˆã†ã«ãƒ•ã‚©ãƒ³ãƒˆã‚µã‚¤ã‚ºã‚’è‡ªå‹•èª¿æ•´
        # æœ€å°ã‚µã‚¤ã‚ºã‚’10ptã«è¨­å®šã—ã€ãã‚Œã‚ˆã‚Šå°ã•ããªã‚‹å ´åˆã¯ãƒ†ã‚­ã‚¹ãƒˆã‚’éè¡¨ç¤ºã«ã™ã‚‹
        uniformtext=dict(minsize=10, mode='hide'), 
        
        pathbar_textfont={'size': 16}
    ))
    
    fig.update_layout(
        margin = dict(t=50, l=10, r=10, b=10),
        title_text="ãƒˆãƒ”ãƒƒã‚¯æ§‹æˆ (Treemap)",
        title_font_size=20,
        # â–¼ ä¿®æ­£ç‚¹: ã‚«ãƒ©ãƒ¼ã‚¦ã‚§ã‚¤ã‚’ "Set3" ã«æ˜ç¤ºçš„ã«æŒ‡å®š
        colorway=px.colors.qualitative.Set3 
    )
    
    return fig, None
# --- â–² ä¿®æ­£å®Œäº† â–² ---


# --- æ„Ÿæƒ…åˆ†æå††ã‚°ãƒ©ãƒ•æç”»é–¢æ•° ---
def create_sentiment_pie_chart(json_data_str):
    try:
        data = json.loads(json_data_str)
        if not isinstance(data, list) or len(data) == 0:
             return None, "AIãŒç”Ÿæˆã—ãŸJSONã®å½¢å¼ãŒä¸æ­£ã§ã™ (ãƒªã‚¹ãƒˆã§ã¯ã‚ã‚Šã¾ã›ã‚“)ã€‚"
    except json.JSONDecodeError:
        return None, "AIãŒç”Ÿæˆã—ãŸJSONã®è§£æã«å¤±æ•—ã—ã¾ã—ãŸã€‚"
    except TypeError:
         return None, "AIã®å¿œç­”ãŒç©ºã¾ãŸã¯ä¸æ­£ã§ã™ã€‚"

    labels = []
    sizes = []
    colors = []
    
    color_map = {
        "ãƒã‚¸ãƒ†ã‚£ãƒ–": "#4CAF50", # ç·‘
        "ãƒã‚¬ãƒ†ã‚£ãƒ–": "#F44336", # èµ¤
        "ä¸­ç«‹": "#9E9E9E"       # ã‚°ãƒ¬ãƒ¼
    }
    
    for item in data:
        sentiment = item.get("sentiment", "ä¸æ˜")
        count = item.get("count", 0)
        
        if count > 0: 
            labels.append(f"{sentiment}\n({count:,}ä»¶)")
            sizes.append(count)
            colors.append(color_map.get(sentiment, "#BDBDBD")) 

    if not sizes:
        return None, "æç”»å¯¾è±¡ã¨ãªã‚‹æ„Ÿæƒ…ãƒ‡ãƒ¼ã‚¿ï¼ˆä»¶æ•° > 0ï¼‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"

    try:
        fig, ax = plt.subplots(figsize=(10, 6))
        
        wedges, texts, autotexts = ax.pie(
            sizes, 
            labels=labels, 
            colors=colors,
            autopct=lambda p: f"{p:.1f}%\n({int(p/100.*sum(sizes)):,d}ä»¶)",
            startangle=90,
            pctdistance=0.85, 
            labeldistance=1.1, 
            textprops={'color':'black', 'fontsize': 11} 
        )
        
        plt.setp(autotexts, color='white', fontweight='bold', fontsize=10)
        
        ax.set_title("æ„Ÿæƒ…åˆ†æï¼ˆãƒã‚¸ãƒ»ãƒã‚¬ãƒ»ä¸­ç«‹ï¼‰ã®å‰²åˆ", fontsize=18)
        ax.axis('equal')  
        
        plt.close(fig) 
        return fig, None
    except Exception as e:
        return None, f"å††ã‚°ãƒ©ãƒ•æç”»ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}"


# --- 7. WordCloudç”Ÿæˆé–¢æ•° ---
def generate_wordcloud(_words_list, font_path, _stopwords_set):
    filtered_words = [word for word in _words_list if word not in _stopwords_set]
    if not filtered_words: return None, "è¡¨ç¤ºã™ã‚‹å˜èªãŒã‚ã‚Šã¾ã›ã‚“ï¼ˆã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰é™¤å»å¾Œï¼‰"
    word_freq = Counter(filtered_words)
    try:
        if not font_path:
            fig_wc, ax = plt.subplots(figsize=(12, 6)); ax.text(0.5, 0.5, "æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“", ha='center', va='center', fontsize=16); ax.axis('off')
            return fig_wc, "æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
        else:
            wc = WordCloud(width=800, height=400, background_color='white', font_path=font_path, max_words=100).generate_from_frequencies(word_freq)
            fig_wc, ax = plt.subplots(figsize=(12, 6)); ax.imshow(wc, interpolation='bilinear'); ax.axis('off')
            plt.close(fig_wc) # ãƒ¡ãƒ¢ãƒªè§£æ”¾ã®ãŸã‚ã«Closeã™ã‚‹
            return fig_wc, None
    except Exception as e: return None, f"WordCloudç”Ÿæˆå¤±æ•—: {e}"

# --- 8. å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ç”Ÿæˆé–¢æ•° (æ”¹å–„ç‰ˆ) ---
def generate_network(_words_df, font_path, _stopwords_set):
    
    co_occur_counter = Counter()
    for words in _words_df:
        unique_words = sorted(list(set(word for word in words if word not in _stopwords_set)))
        for w1, w2 in combinations(unique_words, 2): co_occur_counter[(w1, w2)] += 1
    
    top_pairs = co_occur_counter.most_common(70) 
    
    if not top_pairs:
        return None, "å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ç”Ÿæˆä¸å¯ï¼ˆå…±èµ·ãƒšã‚¢ä¸è¶³ï¼‰ã€‚"

    G = nx.Graph()
    for (w1, w2), weight in top_pairs:
        G.add_edge(w1, w2, weight=weight)
        
    all_words_in_docs = [word for sublist in _words_df for word in sublist if word not in _stopwords_set]
    all_word_freq = Counter(all_words_in_docs)
    
    nodes_in_graph = list(G.nodes())
    node_sizes = []
    for node in nodes_in_graph:
        size = all_word_freq.get(node, 1) * 30 
        node_sizes.append(max(500, min(size, 5000))) 

    try:
        communities_generator = greedy_modularity_communities(G)
        communities = sorted(communities_generator, key=len, reverse=True)
        
        community_map = {}
        # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®è‰²ã‚‚ãƒ‘ã‚¹ãƒ†ãƒ« (Set3) ã«å¤‰æ›´
        cmap = plt.get_cmap('Set3', len(communities)) 
        
        for i, community in enumerate(communities):
            for node in community:
                community_map[node] = cmap(i)
        
        node_colors = [community_map[node] for node in G.nodes()]
    except Exception:
        node_colors = 'lightblue' 

    edge_weights = [d['weight'] * 0.3 for u,v,d in G.edges(data=True)] 

    try:
        fig_net, ax = plt.subplots(figsize=(18, 18)); 
        
        pos = nx.spring_layout(G, k=1.0, iterations=50) 
        
        nx.draw_networkx_nodes(
            G, 
            pos, 
            node_size=node_sizes,    
            node_color=node_colors   
        )
        
        nx.draw_networkx_edges(
            G, 
            pos, 
            width=edge_weights,     
            alpha=0.4, 
            edge_color='gray'
        )
        
        labels_kwargs = {'font_size': 9, 'font_family': 'IPAexGothic'} if font_path else {'font_size': 9}
        nx.draw_networkx_labels(G, pos, **labels_kwargs)
        
        ax.axis('off')
        plt.close(fig_net) 
        return fig_net, None
        
    except Exception as e:
        return None, f"ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æç”»ã‚¨ãƒ©ãƒ¼: {e}"


# å˜èªé »åº¦è¨ˆç®—é–¢æ•°
def calculate_frequency(_words_list, _stopwords_set, top_n=50):
    filtered_words = [word for word in _words_list if word not in _stopwords_set]
    if not filtered_words: return pd.DataFrame(columns=['Rank', 'Word', 'Frequency'])
    word_freq = Counter(filtered_words).most_common(top_n)
    freq_df = pd.DataFrame(word_freq, columns=['Word', 'Frequency'])
    freq_df['Rank'] = freq_df.index + 1
    return freq_df[['Rank', 'Word', 'Frequency']]

# HTMLãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆé–¢æ•°
def fig_to_bytes(fig):
    if fig is None: return None
    if isinstance(fig, plt.Figure):
        buf = io.BytesIO()
        fig.savefig(buf, format="png", bbox_inches="tight")
        buf.seek(0)
        return buf.getvalue()
    elif isinstance(fig, go.Figure):
        try:
            return fig.to_image(format="png", width=1200, height=700, scale=2)
        except ImportError:
            st.error("HTMLãƒ¬ãƒãƒ¼ãƒˆã¸ã®ç”»åƒåŸ‹ã‚è¾¼ã¿ã«ã¯ `kaleido` ãŒå¿…è¦ã§ã™ã€‚`requirements.txt` ã« `kaleido` ã‚’è¿½åŠ ã—ã¦ãã ã•ã„ã€‚")
            return None
        except Exception as e:
            st.error(f"Plotlyç”»åƒã®æ›¸ãå‡ºã—ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    return None

def fig_to_base64_png(fig):
    img_bytes = fig_to_bytes(fig)
    if img_bytes is None: return None
    return f"data:image/png;base64,{base64.b64encode(img_bytes).decode('utf-8')}"

def generate_html_report():
    html_parts = ["<!DOCTYPE html><html lang='ja'><head><meta charset='UTF-8'><title>ãƒ†ã‚­ã‚¹ãƒˆåˆ†æãƒ¬ãƒãƒ¼ãƒˆ</title>"]
    html_parts.append("<style>body{font-family:sans-serif;margin:20px}h1,h2,h3{color:#333;border-bottom:1px solid #ccc;padding-bottom:5px}h2{margin-top:30px}.result-section{margin-bottom:30px;padding:15px;border:1px solid #eee;border-radius:5px;background-color:#f9f9f9}img{max-width:100%;height:auto;border:1px solid #ddd;margin-top:10px}table{border-collapse:collapse;width:100%;margin-top:10px}th,td{border:1px solid #ddd;padding:8px;text-align:left}th{background-color:#f2f2f2}pre{background-color:#eee;padding:10px;border-radius:3px;white-space:pre-wrap;word-wrap:break-word}</style>")
    html_parts.append("</head><body><h1>ãƒ†ã‚­ã‚¹ãƒˆåˆ†æãƒ¬ãƒãƒ¼ãƒˆ</h1>")
    if 'ai_result_simple' in st.session_state: html_parts.append(f"<div class='result-section'><h2>ğŸ¤– AI ã‚µãƒãƒªãƒ¼ (ç°¡æ˜“)</h2><pre>{st.session_state.ai_result_simple}</pre></div>")
    
    if 'fig_sentiment_pie_display' in st.session_state and st.session_state.fig_sentiment_pie_display:
        img_base64 = fig_to_base64_png(st.session_state.fig_sentiment_pie_display);
        if img_base64: html_parts.append(f"<div class='result-section'><h2>ğŸ’– AI æ„Ÿæƒ…åˆ†æ</h2><img src='{img_base64}' alt='Sentiment Pie Chart'></div>")

    if 'fig_treemap_display' in st.session_state and st.session_state.fig_treemap_display:
        img_base64 = fig_to_base64_png(st.session_state.fig_treemap_display);
        if img_base64: html_parts.append(f"<div class='result-section'><h2>ğŸ“Š AI ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ†æ (Treemap)</h2><img src='{img_base64}' alt='Treemap'></div>")
    if 'ai_result_cluster_text' in st.session_state: html_parts.append(f"<div class='result-section'><h2>ğŸ“Š AI ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ†æ (è§£é‡ˆ)</h2><pre>{st.session_state.ai_result_cluster_text}</pre></div>")

    if 'fig_wc_display' in st.session_state and st.session_state.fig_wc_display:
        img_base64 = fig_to_base64_png(st.session_state.fig_wc_display);
        if img_base64: html_parts.append(f"<div class='result-section'><h2>â˜ï¸ WordCloud (å…¨ä½“)</h2><img src='{img_base64}' alt='WordCloud Overall'></div>")
    if 'overall_freq_df_display' in st.session_state and not st.session_state.overall_freq_df_display.empty:
        html_parts.append("<div class='result-section'><h2>ğŸ“Š å˜èªé »åº¦ãƒ©ãƒ³ã‚­ãƒ³ã‚° (å…¨ä½“ Top 50)</h2>" + st.session_state.overall_freq_df_display.to_html(index=False) + "</div>")
    if 'fig_net_display' in st.session_state and st.session_state.fig_net_display:
        img_base64 = fig_to_base64_png(st.session_state.fig_net_display);
        if img_base64: html_parts.append(f"<div class='result-section'><h2>ğŸ•¸ï¸ å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯</h2><img src='{img_base64}' alt='Co-occurrence Network'></div>")
    if 'chi2_results_display' in st.session_state and st.session_state.chi2_results_display and "error" not in st.session_state.chi2_results_display:
        if st.session_state.attribute_columns: # å±æ€§ãŒé¸æŠã•ã‚Œã¦ã„ã‚‹å ´åˆã®ã¿
            attr_col = st.session_state.attribute_columns[0]; html_parts.append(f"<div class='result-section'><h2>ğŸ“ˆ å±æ€§åˆ¥ ç‰¹å¾´èª ({attr_col})</h2>")
            for attr_value, words in st.session_state.chi2_results_display.items():
                html_parts.append(f"<h3>{attr_value}</h3>");
                if words: html_parts.append("<ul>" + "".join(f"<li>{w} (p={p:.3f})</li>" for w, p, c in words) + "</ul>")
                else: html_parts.append("<p>ç‰¹å¾´èªãªã—</p>")
            html_parts.append("</div>")
    if 'ai_result_academic' in st.session_state: html_parts.append(f"<div class='result-section'><h2>ğŸ“ AI å­¦è¡“è«–æ–‡</h2><pre>{st.session_state.ai_result_academic}</pre></div>")
    html_parts.append("</body></html>"); return "".join(html_parts)

# --- 9. ãƒ¡ã‚¤ãƒ³ç”»é¢ã®UI ---
uploaded_file = st.file_uploader("1. Excelãƒ•ã‚¡ã‚¤ãƒ« (xlsx) ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type=["xlsx"])

# def fig_to_bytes(fig): ... (ä¸Šã¸ç§»å‹•)

if uploaded_file:
    try:
        df = pd.read_excel(uploaded_file)
        if 'df_original' not in st.session_state or not st.session_state.df_original.equals(df):
             st.session_state.clear(); st.session_state.df_original = df
        st.subheader("èª­ã¿è¾¼ã¿ãƒ‡ãƒ¼ã‚¿ (å…ˆé ­5ä»¶)"); st.dataframe(df.head())
        all_columns = df.columns.tolist()

        # --- 10. åˆ†æè¨­å®š (ã‚µã‚¤ãƒ‰ãƒãƒ¼) ---
        with st.sidebar:
            st.header("âš™ï¸ åˆ†æè¨­å®š")
            text_column = st.selectbox("åˆ†æã—ãŸã„ãƒ†ã‚­ã‚¹ãƒˆåˆ—", all_columns, index=0)
            attribute_columns = st.multiselect("åˆ†æè»¸ (è¤‡æ•°OK: ä¾‹: å¹´ä»£, æ€§åˆ¥)", all_columns)
            
            st.info(f"AIåˆ†æã¯å…¨ä»¶ï¼ˆæœ€å¤§{MAX_AI_INPUT_CHARS:,}æ–‡å­—ï¼‰ã‚’å¯¾è±¡ã¨ã—ã¾ã™ã€‚çµ±è¨ˆåˆ†æã¯å¸¸ã«å…¨ä»¶ãŒå¯¾è±¡ã§ã™ã€‚", icon="â„¹ï¸")

            st.markdown("---")
            run_button = st.button("åˆ†æã‚’å®Ÿè¡Œ", type="primary", use_container_width=True)
            if 'df_analyzed' in st.session_state:
                st.markdown("---"); st.header("ğŸ“Š ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›")
                try: html_content = generate_html_report(); st.download_button("HTMLãƒ¬ãƒãƒ¼ãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", html_content, "text_analysis_report.html", "text/html", use_container_width=True)
                except Exception as e: st.error(f"ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")

        # --- 11. åˆ†æå®Ÿè¡Œ (å½¢æ…‹ç´ è§£æã®ã¿) ---
        if run_button:
            if not text_column: st.error("ã€Œåˆ†æã—ãŸã„ãƒ†ã‚­ã‚¹ãƒˆåˆ—ã€ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚")
            else:
                required_cols = [text_column] + attribute_columns
                if not all(col in df.columns for col in required_cols): st.error("é¸æŠã•ã‚ŒãŸåˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
                else:
                    with st.spinner("ã‚¹ãƒ†ãƒƒãƒ—1/1: å½¢æ…‹ç´ è§£æã‚’å®Ÿè¡Œä¸­..."):
                        df_analyzed = st.session_state.df_original.copy()
                        _tokenizer_instance = get_tokenizer()
                        df_analyzed['words'] = df_analyzed[text_column].apply(lambda x: extract_words(x, _tokenizer_instance))
                        st.session_state.df_analyzed = df_analyzed
                        st.session_state.text_column = text_column
                        st.session_state.attribute_columns = attribute_columns
                        
                        st.session_state.pop('ai_result_simple', None); st.session_state.pop('ai_result_academic', None)
                        st.session_state.pop('ai_result_cluster_json', None)
                        st.session_state.pop('ai_result_cluster_text', None)
                        st.session_state.pop('fig_treemap_display', None) 
                        st.session_state.pop('treemap_error_display', None)
                        st.session_state.pop('ai_result_sentiment_json', None) 
                        st.session_state.pop('fig_sentiment_pie_display', None)
                        st.session_state.pop('sentiment_pie_error_display', None) 
                        st.session_state.pop('fig_wc_display', None); st.session_state.pop('wc_error_display', None)
                        st.session_state.pop('fig_net_display', None); st.session_state.pop('net_error_display', None)
                        st.session_state.pop('chi2_results_display', None); st.session_state.pop('chi2_error_display', None)
                        st.session_state.pop('overall_freq_df_display', None)
                        st.session_state.pop('attribute_freq_dfs_display', None)
                        st.session_state.pop('dynamic_stopwords', None)
                        st.session_state.pop('chat_messages', None) # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚‚ã‚¯ãƒªã‚¢
                        st.success("å½¢æ…‹ç´ è§£æå®Œäº†ã€‚çµæœã‚¿ãƒ–ã§å„åˆ†æã‚’å®Ÿè¡Œãƒ»è¡¨ç¤ºã—ã¾ã™ã€‚")

        # --- 12. çµæœè¡¨ç¤º (ã‚ªãƒ³ãƒ‡ãƒãƒ³ãƒ‰ + å‹•çš„ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰å¯¾å¿œ) ---
        if 'df_analyzed' in st.session_state:
            st.subheader("ğŸ“Š åˆ†æçµæœ")
            df_analyzed = st.session_state.df_analyzed
            text_column = st.session_state.text_column
            attribute_columns = st.session_state.attribute_columns

            if 'font_path' not in st.session_state:
                try: font_path = matplotlib.font_manager.findfont('IPAexGothic'); st.session_state.font_path = font_path
                except Exception: st.session_state.font_path = None
            font_path = st.session_state.font_path
            if font_path is None: st.warning("æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆ 'IPAexGothic' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚", icon="âš ï¸")

            st.markdown("---")
            st.subheader("âš™ï¸ è¡¨ç¤ºç”¨ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰è¨­å®š")
            st.info("ä»¥ä¸‹ã®å˜èªãƒªã‚¹ãƒˆã¯ã€ä¸‹ã®å„åˆ†æã‚¿ãƒ–ã®è¡¨ç¤ºã«ã®ã¿å½±éŸ¿ã—ã¾ã™ï¼ˆAIåˆ†æã¯é™¤ãï¼‰ã€‚ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã§å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
            if 'dynamic_stopwords' not in st.session_state: st.session_state.dynamic_stopwords = ""
            dynamic_stopwords_input = st.text_area("è¿½åŠ ã®é™¤å¤–èª (ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Š)", value=st.session_state.dynamic_stopwords, key="dynamic_sw_input")
            if dynamic_stopwords_input != st.session_state.dynamic_stopwords:
                st.session_state.pop('fig_wc_display', None); st.session_state.pop('wc_error_display', None)
                st.session_state.pop('fig_net_display', None); st.session_state.pop('net_error_display', None)
                st.session_state.pop('chi2_results_display', None); st.session_state.pop('chi2_error_display', None)
                st.session_state.pop('overall_freq_df_display', None)
                st.session_state.pop('attribute_freq_dfs_display', None)
                st.session_state.dynamic_stopwords = dynamic_stopwords_input # æ–°ã—ã„å€¤ã‚’ä¿å­˜
            dynamic_sw_set = set(w.strip() for w in st.session_state.dynamic_stopwords.split(',') if w.strip())
            current_stopwords_set = BASE_STOPWORDS.union(dynamic_sw_set)
            st.markdown("---")

            # --- ã‚¿ãƒ–ã‚’10å€‹ã«å¢—ã‚„ã™ ---
            tab_names = ["ğŸ¤– AI ã‚µãƒãƒªãƒ¼", "ğŸ’– AI æ„Ÿæƒ…åˆ†æ", "ğŸ“Š AI ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼", "â˜ï¸ WordCloud", "ğŸ“Š å˜èªé »åº¦", "ğŸ•¸ï¸ å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯", "ğŸ” KWIC", "ğŸ“ˆ å±æ€§åˆ¥ç‰¹å¾´èª", "ğŸ“ AI å­¦è¡“è«–æ–‡", "ğŸ’¬ AI ãƒãƒ£ãƒƒãƒˆ"]
            tab1, tab_sentiment, tab_cluster, tab2, tab3, tab4, tab5, tab6, tab7, tab8 = st.tabs(tab_names)
            
            # --- (å…±é€š) AIã«æ¸¡ã™ãƒ†ã‚­ã‚¹ãƒˆã¨ä»¶æ•°ã‚’ç”Ÿæˆã™ã‚‹ãƒ­ã‚¸ãƒƒã‚¯ ---
            
            def format_for_ai(row):
                excel_row_num = row.name + 2 
                text = row[text_column] or ''; 
                attrs = [str(row[col] or 'N/A') for col in attribute_columns]
                id_str = f"[è¡Œç•ªå·: {excel_row_num}]"
                attr_str = f"[{' | '.join(attrs)}]" if attrs else ""
                return f"{id_str} {attr_str} || {text}"

            total_items = len(df_analyzed)
            ai_input_parts = []
            current_char_count = 0
            analyzed_items = 0

            for index, row in df_analyzed.iterrows():
                row_text = format_for_ai(row) + "\n" 
                if current_char_count + len(row_text) > MAX_AI_INPUT_CHARS:
                    break
                ai_input_parts.append(row_text)
                current_char_count += len(row_text)
                analyzed_items += 1
            
            ai_input_text = "".join(ai_input_parts)
            
            if analyzed_items < total_items:
                analysis_scope_instr = f"ã€é‡è¦ã€‘å…¨ {total_items:,} ä»¶ä¸­ã€å…ˆé ­ã® {analyzed_items:,} ä»¶ã®ãƒ‡ãƒ¼ã‚¿ãŒæä¾›ã•ã‚Œã¦ã„ã¾ã™ã€‚åˆ†æã‚„ä»¶æ•°ãƒ»å‰²åˆã®è¨ˆç®—ã¯ã€ã“ã® {analyzed_items:,} ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’ã€Œå…¨ä½“ã€ã¨ã—ã¦è¡Œã£ã¦ãã ã•ã„ã€‚"
                analysis_scope_warning = f"ãƒ‡ãƒ¼ã‚¿ãŒéå¸¸ã«å¤§ãã„ãŸã‚ã€AIåˆ†æã¯å…ˆé ­ã® {analyzed_items:,} ä»¶ï¼ˆå…¨ {total_items:,} ä»¶ä¸­ï¼‰ã‚’å¯¾è±¡ã«å®Ÿè¡Œã•ã‚Œã¾ã—ãŸã€‚å…¨ä»¶ã®å³å¯†ãªçµ±è¨ˆã¯ä»–ã®ã‚¿ãƒ–ã‚’ã”è¦§ãã ã•ã„ã€‚"
            else:
                analysis_scope_instr = f"ã€é‡è¦ã€‘å…¨ {total_items:,} ä»¶ã®ãƒ‡ãƒ¼ã‚¿ãŒæä¾›ã•ã‚Œã¦ã„ã¾ã™ã€‚åˆ†æã‚„ä»¶æ•°ãƒ»å‰²åˆã®è¨ˆç®—ã¯ã€ã“ã® {total_items:,} ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’ã€Œå…¨ä½“ã€ã¨ã—ã¦è¡Œã£ã¦ãã ã•ã„ã€‚"
                analysis_scope_warning = f"AIåˆ†æã¯å…¨ {total_items:,} ä»¶ã‚’å¯¾è±¡ã«å®Ÿè¡Œã•ã‚Œã¾ã—ãŸã€‚"
            # --- (å…±é€šãƒ­ã‚¸ãƒƒã‚¯ã“ã“ã¾ã§) ---


            # --- (å…±é€š) ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼JSONã‚’ï¼ˆå¿…è¦ãªã‚‰ç”Ÿæˆã—ã¤ã¤ï¼‰å–å¾—ã™ã‚‹ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•° ---
            def get_or_generate_cluster_json(ai_input_text, analysis_scope_instr, analyzed_items):
                if 'ai_result_cluster_json' not in st.session_state:
                    contents_json = [{"parts": [{"text": ai_input_text}]}]
                    schema = {
                        "type": "OBJECT",
                        "properties": {
                            "name": {"type": "STRING", "description": "å¸¸ã« 'å…¨ä½“' ã¾ãŸã¯ 'All Topics'"},
                            "children": {
                                "type": "ARRAY",
                                "description": "ä¸»è¦ãªã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ï¼ˆ3ã€œ5å€‹ï¼‰ã®é…åˆ—",
                                "items": {
                                    "type": "OBJECT",
                                    "properties": {
                                        "name": {"type": "STRING", "description": "ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼å (ä¾‹: 'ãƒã‚¸ãƒ†ã‚£ãƒ–ãªæ„è¦‹')"},
                                        "children": {
                                            "type": "ARRAY",
                                            "description": "ã‚µãƒ–ãƒˆãƒ”ãƒƒã‚¯ï¼ˆ3ã€œ5å€‹ï¼‰ã®é…åˆ—",
                                            "items": {
                                                "type": "OBJECT",
                                                "properties": {
                                                    "name": {"type": "STRING", "description": "ã‚µãƒ–ãƒˆãƒ”ãƒƒã‚¯å (ä¾‹: 'ãƒ‡ã‚¶ã‚¤ãƒ³ã¸ã®è¨€åŠ')"},
                                                    "value": {"type": "NUMBER", "description": "ã‚µãƒ–ãƒˆãƒ”ãƒƒã‚¯ã®å‰²åˆï¼ˆæ•°å€¤ã®ã¿ï¼‰"}
                                                },
                                                "required": ["name", "value"]
                                            }
                                        }
                                    },
                                    "required": ["name", "children"]
                                }
                            }
                        },
                        "required": ["name", "children"]
                    }
                    
                    gen_config_json = {
                        "response_mime_type": "application/json",
                        "response_schema": schema
                    }
                    
                    system_instr_json = SYSTEM_PROMPT_CLUSTER_JSON.format(
                        analysis_scope_instruction=analysis_scope_instr,
                        analyzed_items=analyzed_items
                    )
                    
                    json_str = call_gemini_api(contents_json, system_instruction=system_instr_json, generation_config=gen_config_json)
                    st.session_state.ai_result_cluster_json = json_str
                
                return st.session_state.ai_result_cluster_json


            # --- Tab 1: AI ã‚µãƒãƒªãƒ¼ (ç°¡æ˜“) ---
            with tab1:
                if 'ai_result_simple' not in st.session_state:
                    with st.spinner("AIã«ã‚ˆã‚‹ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ†æã¨è¦ç´„ã‚’ç”Ÿæˆä¸­..."):
                        
                        if analyzed_items < total_items: st.warning(analysis_scope_warning, icon="âš ï¸")
                        else: st.info(analysis_scope_warning, icon="âœ…")

                        try:
                            cluster_json_str = get_or_generate_cluster_json(ai_input_text, analysis_scope_instr, analyzed_items)
                        except Exception as e:
                            st.error(f"ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼JSONã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
                            cluster_json_str = '{"name": "ã‚¨ãƒ©ãƒ¼", "children": []}'

                        contents = [{"parts": [{"text": ai_input_text}]}]
                        has_attr = bool(attribute_columns)
                        has_attribute_str_s = "## 6. å±æ€§åˆ¥ã®å‚¾å‘ (ã‚‚ã—ã‚ã‚Œã°)\n(å±æ€§ã”ã¨ã®ç‰¹å¾´çš„ãªæ„è¦‹ã‚’æ¯”è¼ƒ)" if has_attr else ""
                        attr_instr_s = "ãƒ‡ãƒ¼ã‚¿ã¯ã€Œå±æ€§ || ãƒ†ã‚­ã‚¹ãƒˆã€ã®å½¢å¼ã§ã™ã€‚å±æ€§ã”ã¨ã®å‚¾å‘ã‚„é•ã„ã«ã‚‚ç€ç›®ã—ã¦åˆ†æã—ã¦ãã ã•ã„ã€‚" if has_attr else ""
                        
                        system_instr_s = SYSTEM_PROMPT_SIMPLE.format(
                            analysis_scope_instruction=analysis_scope_instr,
                            attributeInstruction=attr_instr_s, 
                            has_attribute=has_attribute_str_s,
                            cluster_json_data=cluster_json_str 
                        )
                        st.session_state.ai_result_simple = call_gemini_api(contents, system_instruction=system_instr_s)
                st.markdown(st.session_state.ai_result_simple)

            # --- (æ–°è¨­) AI æ„Ÿæƒ…åˆ†æã‚¿ãƒ– (JSON + Matplotlib Pie Chart) ---
            with tab_sentiment:
                st.subheader("AIã«ã‚ˆã‚‹æ„Ÿæƒ…åˆ†æï¼ˆå††ã‚°ãƒ©ãƒ•ï¼‰")
                st.info("AIãŒå…¨ãƒ†ã‚­ã‚¹ãƒˆã‚’ã€Œãƒã‚¸ãƒ†ã‚£ãƒ–ã€ã€Œãƒã‚¬ãƒ†ã‚£ãƒ–ã€ã€Œä¸­ç«‹ã€ã«åˆ†é¡ã—ã€ãã®æ§‹æˆæ¯”ï¼ˆä»¶æ•°ã¨å‰²åˆï¼‰ã‚’å¯è¦–åŒ–ã—ã¾ã™ã€‚")
                st.warning("ã“ã®åˆ†æã¯ã€AIã‚µãƒãƒªãƒ¼ã®ã€Œãƒã‚¸ãƒ†ã‚£ãƒ–/ãƒã‚¬ãƒ†ã‚£ãƒ–ãªæ„è¦‹ã€ã®æŠœç²‹ã¨ã¯ç•°ãªã‚Šã€å…¨ä»¶ã‚’å¯¾è±¡ã¨ã—ãŸAIã«ã‚ˆã‚‹åˆ†é¡é›†è¨ˆã§ã™ã€‚", icon="â„¹ï¸")

                # 1. æ„Ÿæƒ…åˆ†æJSONã®ç”Ÿæˆ (ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç¢ºèª)
                if 'ai_result_sentiment_json' not in st.session_state:
                    with st.spinner("AIã«ã‚ˆã‚‹æ„Ÿæƒ…åˆ†æJSONã‚’ç”Ÿæˆä¸­... (ã‚¹ãƒ†ãƒƒãƒ—1/2)"):
                        if analyzed_items < total_items: st.warning(analysis_scope_warning, icon="âš ï¸")
                        else: st.info(analysis_scope_warning, icon="âœ…")

                        contents_json = [{"parts": [{"text": ai_input_text}]}]
                        schema = {
                            "type": "ARRAY",
                            "items": {
                                "type": "OBJECT",
                                "properties": {
                                    "sentiment": {"type": "STRING", "description": "æ„Ÿæƒ…ãƒ©ãƒ™ãƒ« (ãƒã‚¸ãƒ†ã‚£ãƒ–, ãƒã‚¬ãƒ†ã‚£ãƒ–, ä¸­ç«‹)"},
                                    "count": {"type": "NUMBER", "description": "è©²å½“ã™ã‚‹ä»¶æ•°"},
                                    "percentage": {"type": "NUMBER", "description": "å…¨ä½“ã«å ã‚ã‚‹å‰²åˆ (xx.x)"}
                                },
                                "required": ["sentiment", "count", "percentage"]
                            }
                        }
                        
                        gen_config_json = {
                            "response_mime_type": "application/json",
                            "response_schema": schema
                        }
                        
                        system_instr_json = SYSTEM_PROMPT_SENTIMENT_JSON.format(
                            analysis_scope_instruction=analysis_scope_instr,
                            analyzed_items=analyzed_items
                        )
                        
                        json_str = call_gemini_api(contents_json, system_instruction=system_instr_json, generation_config=gen_config_json)
                        st.session_state.ai_result_sentiment_json = json_str
                
                # 2. å††ã‚°ãƒ©ãƒ•ã®æç”» (ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç¢ºèª)
                if 'fig_sentiment_pie_display' not in st.session_state and 'ai_result_sentiment_json' in st.session_state:
                    with st.spinner("æ„Ÿæƒ…åˆ†æå††ã‚°ãƒ©ãƒ•ã‚’ç”Ÿæˆä¸­... (ã‚¹ãƒ†ãƒƒãƒ—2/2)"):
                        json_data_str = st.session_state.ai_result_sentiment_json
                        fig_pie, pie_error = create_sentiment_pie_chart(json_data_str)
                        st.session_state.fig_sentiment_pie_display = fig_pie
                        st.session_state.sentiment_pie_error_display = pie_error
                
                # 3. æç”»
                if 'fig_sentiment_pie_display' in st.session_state and st.session_state.fig_sentiment_pie_display:
                    fig_pie = st.session_state.fig_sentiment_pie_display
                    st.pyplot(fig_pie)
                    
                    img_bytes = fig_to_bytes(fig_pie)
                    if img_bytes: st.download_button("ã“ã®ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ (PNG)", img_bytes, "sentiment_pie_chart.png", "image/png")

                elif 'sentiment_pie_error_display' in st.session_state:
                    st.error(st.session_state.sentiment_pie_error_display)
                    if 'ai_result_sentiment_json' in st.session_state:
                         st.text_area("AIã®JSONãƒ¬ã‚¹ãƒãƒ³ã‚¹ (ã‚¨ãƒ©ãƒ¼)", st.session_state.ai_result_sentiment_json, height=200)
                else:
                    st.info("æ„Ÿæƒ…åˆ†æãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆä¸­ã§ã™...")


            # --- AI ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ†æã‚¿ãƒ– (JSON + Plotly Treemap) ---
            with tab_cluster:
                st.subheader("AIã«ã‚ˆã‚‹è¨€èª¬ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ†æ (Treemap)")
                st.info("AIãŒãƒ†ã‚­ã‚¹ãƒˆã‚’éšå±¤çš„ãªãƒˆãƒ”ãƒƒã‚¯ã«åˆ†é¡ã—ã€ãã®æ§‹æˆæ¯”ï¼ˆé¢ç©ï¼‰ã‚’å¯è¦–åŒ–ã—ã¾ã™ã€‚ã‚°ãƒ©ãƒ•å³ä¸Šã®ã‚«ãƒ¡ãƒ©ã‚¢ã‚¤ã‚³ãƒ³ã‹ã‚‰ç”»åƒã‚’ä¿å­˜ã§ãã¾ã™ã€‚")

                # 1. JSONãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆ (ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç¢ºèª)
                if 'ai_result_cluster_json' not in st.session_state:
                    with st.spinner("AIã«ã‚ˆã‚‹ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼JSONã‚’ç”Ÿæˆä¸­... (ã‚¹ãƒ†ãƒƒãƒ—1/3)"):
                        if analyzed_items < total_items: st.warning(analysis_scope_warning, icon="âš ï¸")
                        else: st.info(analysis_scope_warning, icon="âœ…")
                        
                        try:
                            # å…±é€šé–¢æ•°ã‚’å‘¼ã³å‡ºã™
                            get_or_generate_cluster_json(ai_input_text, analysis_scope_instr, analyzed_items)
                        except Exception as e:
                            st.error(f"ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼JSONã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
                            if 'ai_result_cluster_json' in st.session_state: 
                                 st.text_area("AIã®ç”Ÿãƒ¬ã‚¹ãƒãƒ³ã‚¹ (ã‚¨ãƒ©ãƒ¼)", st.session_state.ai_result_cluster_json, height=100)


                # 2. ãƒ†ã‚­ã‚¹ãƒˆè§£é‡ˆã®ç”Ÿæˆ (ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç¢ºèª)
                if 'ai_result_cluster_text' not in st.session_state and 'ai_result_cluster_json' in st.session_state:
                     with st.spinner("AIã«ã‚ˆã‚‹ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã®è§£é‡ˆã‚’ç”Ÿæˆä¸­... (ã‚¹ãƒ†ãƒƒãƒ—2/3)"):
                        json_str = st.session_state.ai_result_cluster_json
                        
                        system_instr_text = SYSTEM_PROMPT_CLUSTER_TEXT.format(
                            analysis_scope_instruction=analysis_scope_instr,
                            json_data=json_str
                        )
                        contents_text = [{"parts": [{"text": "ã“ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ†æã®çµæœã‚’ã€ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³å½¢å¼ã§è©³ç´°ã«è§£é‡ˆãƒ»è¦ç´„ã—ã¦ãã ã•ã„ã€‚"}]}]
                        
                        text_summary = call_gemini_api(contents_text, system_instruction=system_instr_text)
                        st.session_state.ai_result_cluster_text = text_summary
                
                # 3. Treemap (Plotly) ã®æç”» (ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç¢ºèª)
                if 'fig_treemap_display' not in st.session_state and 'ai_result_cluster_json' in st.session_state:
                    with st.spinner("Treemapã‚’ç”Ÿæˆä¸­... (ã‚¹ãƒ†ãƒƒãƒ—3/3)"):
                        json_data_str = st.session_state.ai_result_cluster_json
                        fig_treemap, treemap_error = create_plotly_treemap(json_data_str) # Plotlyé–¢æ•°ã‚’å‘¼ã³å‡ºã™
                        st.session_state.fig_treemap_display = fig_treemap
                        st.session_state.treemap_error_display = treemap_error

                # 4. æç”»ã¨ãƒ†ã‚­ã‚¹ãƒˆè¡¨ç¤º
                if 'fig_treemap_display' in st.session_state and st.session_state.fig_treemap_display:
                    st.subheader("ãƒˆãƒ”ãƒƒã‚¯æ§‹æˆ (Treemap)")
                    fig_treemap = st.session_state.fig_treemap_display
                    st.plotly_chart(fig_treemap, use_container_width=True) # st.pyplot -> st.plotly_chart
                    
                    if 'ai_result_cluster_text' in st.session_state:
                        # å‡¡ä¾‹ã¨è§£é‡ˆã¯AIã®å¿œç­”ã«ä»»ã›ã‚‹
                        st.markdown(st.session_state.ai_result_cluster_text)
                    else:
                        st.info("ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã®è§£é‡ˆã‚’ç”Ÿæˆä¸­ã§ã™...")
                
                elif 'treemap_error_display' in st.session_state:
                    st.error(st.session_state.treemap_error_display)
                    if 'ai_result_cluster_json' in st.session_state:
                         st.text_area("AIã®JSONãƒ¬ã‚¹ãƒãƒ³ã‚¹", st.session_state.ai_result_cluster_json, height=200)
                else:
                    st.info("ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ†æãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆä¸­ã§ã™...")
            
            # --- Tab 2: WordCloud --- (tab2 ã«å¤‰æ›´)
            with tab2:
                st.subheader("å…¨ä½“ã®WordCloud")
                if 'fig_wc_display' not in st.session_state:
                    with st.spinner("WordCloudã‚’ç”Ÿæˆä¸­..."):
                        all_words_list = [word for sublist in df_analyzed['words'] for word in sublist]
                        fig_wc, wc_error = generate_wordcloud(all_words_list, font_path, current_stopwords_set)
                        st.session_state.fig_wc_display = fig_wc
                        st.session_state.wc_error_display = wc_error
                if st.session_state.fig_wc_display:
                    st.pyplot(st.session_state.fig_wc_display)
                    img_bytes = fig_to_bytes(st.session_state.fig_wc_display)
                    if img_bytes: st.download_button("ã“ã®ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ (PNG)", img_bytes, "wordcloud_overall.png", "image/png")
                else: st.warning(st.session_state.wc_error_display)

                with st.expander("åˆ†æãƒ—ãƒ­ã‚»ã‚¹ã¨è«–æ–‡è¨˜è¿°ä¾‹"):
                    st.markdown("""
                        #### 1. åˆ†æãƒ—ãƒ­ã‚»ã‚¹
                        1.  **å½¢æ…‹ç´ è§£æ**: ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã®æŒ‡å®šãƒ†ã‚­ã‚¹ãƒˆåˆ—ã«å¯¾ã—ã€`Janome` ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ç”¨ã„ã¦å½¢æ…‹ç´ è§£æã‚’å®Ÿè¡Œã—ã¾ã—ãŸã€‚
                        2.  **å˜èªæŠ½å‡º**: æŠ½å‡ºã™ã‚‹å“è©ã‚’ã€Œåè©ã€ã€Œå‹•è©ã€ã€Œå½¢å®¹è©ã€ã«é™å®šã—ã¾ã—ãŸã€‚
                        3.  **ãƒã‚¤ã‚ºé™¤å»**: ä¸€èˆ¬çš„ãªåŠ©è©ãƒ»åŠ©å‹•è©ï¼ˆä¾‹: ã€Œã®ã€ã€Œã§ã™ã€ï¼‰ãŠã‚ˆã³ã€ã€Œè¡¨ç¤ºç”¨ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰è¨­å®šã€ã§æŒ‡å®šã•ã‚ŒãŸå˜èªã€æ•°å­—ã€1æ–‡å­—ã®å˜èªã‚’ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ã¨ã—ã¦åˆ†æã‹ã‚‰é™¤å¤–ã—ã¾ã—ãŸã€‚
                        4.  **é »åº¦é›†è¨ˆ**: å‡ºç¾ã—ãŸã™ã¹ã¦ã®å˜èªï¼ˆåŸºæœ¬å½¢ï¼‰ã®é »åº¦ã‚’ã‚«ã‚¦ãƒ³ãƒˆã—ã¾ã—ãŸã€‚
                        5.  **å¯è¦–åŒ–**: ä¸Šè¨˜ã®é »åº¦ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ãã€`WordCloud` ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ç”¨ã„ã¦ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ï¼ˆä¸Šä½100èªï¼‰ã‚’ç”Ÿæˆã—ã¾ã—ãŸã€‚
                        
                        #### 2. è«–æ–‡è¨˜è¿°ä¾‹
                        > ...æœ¬ç ”ç©¶ã§ã¯ã€[ãƒ†ã‚­ã‚¹ãƒˆåˆ—å] ã®å…¨ä½“çš„ãªå‚¾å‘ã‚’æŠŠæ¡ã™ã‚‹ãŸã‚ã€å½¢æ…‹ç´ è§£æï¼ˆãƒ©ã‚¤ãƒ–ãƒ©ãƒª: Janomeï¼‰ã«ã‚ˆã‚Šãƒ†ã‚­ã‚¹ãƒˆã‚’å˜èªã«åˆ†ã‹ã¡æ›¸ãã—ãŸã€‚åˆ†æå¯¾è±¡ã¯åè©ã€å‹•è©ã€å½¢å®¹è©ã®åŸºæœ¬å½¢ã«é™å®šã—ã€ä¸€èˆ¬çš„ã™ãã‚‹åŠ©è©ãƒ»åŠ©å‹•è©ã‚„æ•°å­—ã€ãŠã‚ˆã³[ãƒ¦ãƒ¼ã‚¶ãƒ¼æŒ‡å®šã®å˜èª]ç­‰ã‚’ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ã¨ã—ã¦é™¤å¤–ã—ãŸã€‚ãã®ä¸Šã§ã€å‡ºç¾é »åº¦ä¸Šä½100å˜èªã‚’å¯¾è±¡ã«ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã‚’ç”Ÿæˆã—ãŸï¼ˆå›³1å‚ç…§ï¼‰ã€‚
                        >
                        > å›³1ã®çµæœã‹ã‚‰ã€[å˜èªA]ã‚„[å˜èªB]ã¨ã„ã£ãŸå˜èªãŒç‰¹ã«å¤§ããè¡¨ç¤ºã•ã‚Œã¦ãŠã‚Šã€[ãƒ‡ãƒ¼ã‚¿å…¨ä½“]ã«ãŠã„ã¦ã“ã‚Œã‚‰ã®ãƒˆãƒ”ãƒƒã‚¯ãŒé »ç¹ã«è¨€åŠã•ã‚Œã¦ã„ã‚‹ã“ã¨ãŒç¤ºå”†ã•ã‚ŒãŸã€‚
                    """)

                st.markdown("---")
                st.subheader("å±æ€§åˆ¥ã®WordCloud")
                if not attribute_columns: st.warning("å±æ€§åˆ¥WordCloudã‚’è¡¨ç¤ºã™ã‚‹ã«ã¯åˆ†æè»¸ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚")
                else:
                    selected_attr_for_wc = st.selectbox("WordCloudã®åˆ†æè»¸ã‚’é¸æŠ", attribute_columns, 0, key="wc_attr_select")
                    if selected_attr_for_wc:
                        try: unique_values = sorted(df_analyzed[selected_attr_for_wc].dropna().unique())
                        except TypeError: unique_values = sorted(df_analyzed[selected_attr_for_wc].dropna().astype(str).unique())
                        st.info(f"ã€Œ**{selected_attr_for_wc}**ã€ã®å€¤ã”ã¨ã«WordCloudã‚’ç”Ÿæˆã—ã¾ã™ã€‚")
                        for val in unique_values:
                            st.markdown(f"#### {selected_attr_for_wc} : **{val}**")
                            subset_df = df_analyzed[df_analyzed[selected_attr_for_wc] == val]
                            subset_words_list = [word for sublist in subset_df['words'] for word in sublist]
                            if not subset_words_list: st.info("å˜èªãªã—"); continue
                            fig_subset_wc, wc_subset_error = generate_wordcloud(subset_words_list, font_path, current_stopwords_set)
                            if fig_subset_wc:
                                st.pyplot(fig_subset_wc)
                                img_bytes = fig_to_bytes(fig_subset_wc)
                                if img_bytes: st.download_button(f"ã€Œ{val}ã€ã®ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", img_bytes, f"wordcloud_attr_{val}.png", "image/png")
                            else: st.warning(wc_subset_error)

            # --- Tab 3: å˜èªé »åº¦ãƒ©ãƒ³ã‚­ãƒ³ã‚° --- (tab3 ã«å¤‰æ›´)
            with tab3:
                st.subheader("å…¨ä½“ã®å˜èªé »åº¦ãƒ©ãƒ³ã‚­ãƒ³ã‚° (Top 50)")
                if 'overall_freq_df_display' not in st.session_state:
                     with st.spinner("å…¨ä½“ã®å˜èªé »åº¦ã‚’è¨ˆç®—ä¸­..."):
                        all_words_list = [word for sublist in df_analyzed['words'] for word in sublist]
                        overall_freq_df = calculate_frequency(all_words_list, current_stopwords_set)
                        st.session_state.overall_freq_df_display = overall_freq_df
                st.dataframe(st.session_state.overall_freq_df_display, use_container_width=True)

                with st.expander("åˆ†æãƒ—ãƒ­ã‚»ã‚¹ã¨è«–æ–‡è¨˜è¿°ä¾‹"):
                    st.markdown("""
                        #### 1. åˆ†æãƒ—ãƒ­ã‚»ã‚¹
                        1.  **å˜èªæŠ½å‡º**: WordCloudã¨åŒæ§˜ã«ã€åè©ãƒ»å‹•è©ãƒ»å½¢å®¹è©ã‹ã‚‰ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ã¨æ•°å­—ã‚’é™¤å¤–ã—ãŸå˜èªãƒªã‚¹ãƒˆã‚’ä½¿ç”¨ã—ã¾ã—ãŸã€‚
                        2.  **é »åº¦é›†è¨ˆ**: å…¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«å‡ºç¾ã—ãŸã™ã¹ã¦ã®å˜èªï¼ˆåŸºæœ¬å½¢ï¼‰ã®é »åº¦ã‚’ã‚«ã‚¦ãƒ³ãƒˆã—ã¾ã—ãŸã€‚
                        3.  **è¡¨ç¤º**: é »åº¦ãŒé«˜ã„é †ã«ã‚½ãƒ¼ãƒˆã—ã€ä¸Šä½50ä»¶ã‚’è¡¨å½¢å¼ã§è¡¨ç¤ºã—ã¾ã—ãŸã€‚
                        
                        #### 2. è«–æ–‡è¨˜è¿°ä¾‹
                        > ...åˆ†æå¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆå…¨ä½“ã«ãŠã‘ã‚‹ä¸»è¦ãªå˜èªã‚’ç‰¹å®šã™ã‚‹ãŸã‚ã€å‡ºç¾é »åº¦åˆ†æã‚’è¡Œã£ãŸã€‚å½¢æ…‹ç´ è§£æï¼ˆå‰è¿°ï¼‰ã«ã‚ˆã‚ŠæŠ½å‡ºã•ã‚ŒãŸå˜èªï¼ˆåè©ã€å‹•è©ã€å½¢å®¹è©ï¼‰ã®å‡ºç¾é »åº¦ã‚’é›†è¨ˆã—ãŸçµæœã€ä¸Šä½50å˜èªã¯è¡¨Xã®é€šã‚Šã§ã‚ã£ãŸã€‚
                        >
                        > è¡¨Xã‚ˆã‚Šã€[å˜èªA] (N=[é »åº¦])ã€[å˜èªB] (N=[é »åº¦]) ãŒç‰¹ã«é«˜é »åº¦ã§å‡ºç¾ã—ã¦ãŠã‚Šã€...
                    """)

                st.markdown("---")
                st.subheader("å±æ€§åˆ¥ã®å˜èªé »åº¦ãƒ©ãƒ³ã‚­ãƒ³ã‚° (Top 50)")
                if not attribute_columns: st.warning("å±æ€§åˆ¥é »åº¦ã‚’è¡¨ç¤ºã™ã‚‹ã«ã¯åˆ†æè»¸ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚")
                else:
                    selected_attr_for_freq = st.selectbox("é »åº¦ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã®åˆ†æè»¸ã‚’é¸æŠ", attribute_columns, 0, key="freq_attr_select")
                    if selected_attr_for_freq:
                        if ('attribute_freq_dfs_display' not in st.session_state or
                            st.session_state.get('attribute_freq_col_display') != selected_attr_for_freq or
                            st.session_state.get('attribute_freq_sw_display') != st.session_state.dynamic_stopwords):
                            with st.spinner(f"ã€Œ{selected_attr_for_freq}ã€åˆ¥ã®å˜èªé »åº¦ã‚’è¨ˆç®—ä¸­..."):
                                st.session_state.attribute_freq_dfs_display = {}
                                try: unique_values = sorted(df_analyzed[selected_attr_for_freq].dropna().unique())
                                except TypeError: unique_values = sorted(df_analyzed[selected_attr_for_freq].dropna().astype(str).unique())
                                for val in unique_values:
                                    subset_df = df_analyzed[df_analyzed[selected_attr_for_freq] == val]
                                    subset_words_list = [word for sublist in subset_df['words'] for word in sublist]
                                    st.session_state.attribute_freq_dfs_display[val] = calculate_frequency(subset_words_list, current_stopwords_set)
                                st.session_state.attribute_freq_col_display = selected_attr_for_freq
                                st.session_state.attribute_freq_sw_display = st.session_state.dynamic_stopwords
                        attribute_freq_dfs = st.session_state.attribute_freq_dfs_display
                        st.info(f"ã€Œ**{selected_attr_for_freq}**ã€ã®å€¤ã”ã¨ã«å˜èªé »åº¦ãƒ©ãƒ³ã‚­ãƒ³ã‚° (Top 50) ã‚’è¡¨ç¤ºã—ã¾ã™ã€‚")
                        for val, freq_df in attribute_freq_dfs.items():
                             with st.expander(f"å±æ€§: **{val}** ã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°"):
                                if freq_df.empty: st.info("å˜èªãªã—")
                                else: st.dataframe(freq_df, use_container_width=True)

            # --- Tab 4: å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ --- (tab4 ã«å¤‰æ›´)
            with tab4:
                if 'fig_net_display' not in st.session_state:
                    with st.spinner("å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ç”Ÿæˆä¸­..."):
                        fig_net, net_error = generate_network(df_analyzed['words'], font_path, current_stopwords_set)
                        st.session_state.fig_net_display = fig_net
                        st.session_state.net_error_display = net_error
                if st.session_state.fig_net_display:
                    st.pyplot(st.session_state.fig_net_display)
                    img_bytes = fig_to_bytes(st.session_state.fig_net_display)
                    if img_bytes: st.download_button("ã“ã®ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ (PNG)", img_bytes, "network.png", "image/png")
                else: st.warning(st.session_state.net_error_display)

                with st.expander("åˆ†æãƒ—ãƒ­ã‚»ã‚¹ã¨è«–æ–‡è¨˜è¿°ä¾‹"):
                    st.markdown("""
                        #### 1. åˆ†æãƒ—ãƒ­ã‚»ã‚¹
                        1.  **å˜èªæŠ½å‡º**: WordCloudã¨åŒæ§˜ã«ã€åè©ãƒ»å‹•è©ãƒ»å½¢å®¹è©ã‹ã‚‰ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ã¨æ•°å­—ã‚’é™¤å¤–ã—ãŸå˜èªãƒªã‚¹ãƒˆã‚’ä½¿ç”¨ã—ã¾ã—ãŸã€‚
                        2.  **å…±èµ·ã®å®šç¾©**: 1ã¤ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆï¼ˆExcelã®1è¡Œï¼‰å†…ã§åŒæ™‚ã«å‡ºç¾ã—ãŸå˜èªãƒšã‚¢ã‚’ã€Œå…±èµ·ã€ã¨ã—ã¦å®šç¾©ã—ã¾ã—ãŸã€‚
                        3.  **é »åº¦é›†è¨ˆ**: å…¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å¯¾è±¡ã«ã€å…±èµ·ã™ã‚‹å˜èªãƒšã‚¢ã®å‡ºç¾é »åº¦ã‚’é›†è¨ˆã—ã¾ã—ãŸã€‚
                        4.  **ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ§‹ç¯‰**: å…±èµ·é »åº¦ãŒé«˜ã‹ã£ãŸä¸Šä½70ãƒšã‚¢ã‚’æŠ½å‡ºã—ã€`NetworkX` ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ç”¨ã„ã¦ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’æ§‹ç¯‰ã—ã¾ã—ãŸã€‚
                        5.  **ãƒãƒ¼ãƒ‰ã®ã‚µã‚¤ã‚º**: å„å˜èªã®**å…¨ä½“ã§ã®å‡ºç¾é »åº¦**ã«åŸºã¥ãã€ãƒãƒ¼ãƒ‰ï¼ˆå††ï¼‰ã®ã‚µã‚¤ã‚ºã‚’å‹•çš„ã«å¤‰æ›´ã—ã¾ã—ãŸã€‚
                        6.  **ãƒãƒ¼ãƒ‰ã®è‰²**: `greedy_modularity_communities` ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ç”¨ã„ã€é–¢é€£æ€§ã®é«˜ã„å˜èªã‚°ãƒ«ãƒ¼ãƒ—ï¼ˆã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ï¼‰ã‚’æ¤œå‡ºã—ã€ã‚°ãƒ«ãƒ¼ãƒ—ã”ã¨ã«è‰²åˆ†ã‘ã—ã¾ã—ãŸã€‚
                        7.  **ã‚¨ãƒƒã‚¸ã®å¤ªã•**: å…±èµ·é »åº¦ï¼ˆé–¢ä¿‚ã®å¼·ã•ï¼‰ã«åŸºã¥ãã€ã‚¨ãƒƒã‚¸ï¼ˆç·šï¼‰ã®å¤ªã•ã‚’å‹•çš„ã«å¤‰æ›´ã—ã¾ã—ãŸï¼ˆä¿‚æ•°: 0.3ï¼‰ã€‚
                        
                        #### 2. è«–æ–‡è¨˜è¿°ä¾‹
                        > ...æ¬¡ã«ã€å˜èªé–“ã®é–¢é€£æ€§ã‚’æ¢ç´¢ã™ã‚‹ãŸã‚ã€å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åˆ†æã‚’å®Ÿæ–½ã—ãŸã€‚å…±èµ·é »åº¦ä¸Šä½70ãƒšã‚¢ã«åŸºã¥ããƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆå›³2ï¼‰ã‚’æç”»ã—ãŸã€‚ãƒãƒ¼ãƒ‰ã®ã‚µã‚¤ã‚ºã¯å„å˜èªã®å‡ºç¾é »åº¦ã‚’ã€ã‚¨ãƒƒã‚¸ã®å¤ªã•ã¯å…±èµ·é »åº¦ã‚’åæ˜ ã—ã¦ã„ã‚‹ã€‚ã¾ãŸã€`greedy_modularity_communities` ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«ã‚ˆã‚‹ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£æ¤œå‡ºã‚’å®Ÿè¡Œã—ã€æŠ½å‡ºã•ã‚ŒãŸã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã”ã¨ã«è‰²åˆ†ã‘ã‚’è¡Œã£ãŸã€‚
                        >
                        > å›³2ã‚ˆã‚Šã€[å˜èªA]ã‚„[å˜èªB]ãŒï¼ˆå¤§ããªãƒãƒ¼ãƒ‰ã§ç¤ºã•ã‚Œã‚‹ã‚ˆã†ã«ï¼‰é«˜é »åº¦ã§å‡ºç¾ã—ã¦ã„ã‚‹ã“ã¨ãŒã‚ã‹ã‚‹ã€‚ã¾ãŸã€[å˜èªC, D, E]ãŒåŒã˜è‰²ï¼ˆ[è‰²]ï¼‰ã®ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã‚’å½¢æˆã—ã¦ãŠã‚Šã€ã“ã‚Œã‚‰ãŒå¯†æ¥ã«é–¢é€£ã™ã‚‹ãƒˆãƒ”ãƒƒã‚¯ç¾¤ã§ã‚ã‚‹ã“ã¨ãŒç¤ºå”†ã•ã‚ŒãŸã€‚
                    """)

            # --- Tab 5: KWIC (æ–‡è„ˆæ¤œç´¢) --- (tab5 ã«å¤‰æ›´)
            with tab5:
                st.subheader("KWIC (æ–‡è„ˆæ¤œç´¢)")
                st.info("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã« `*` ã‚’å«ã‚ã‚‹ã¨ãƒ¯ã‚¤ãƒ«ãƒ‰ã‚«ãƒ¼ãƒ‰æ¤œç´¢ãŒå¯èƒ½ã§ã™ (ä¾‹: `é¡§å®¢*`)ã€‚")
                kwic_keyword = st.text_input("æ–‡è„ˆã‚’æ¤œç´¢ã—ãŸã„å˜èªã‚’å…¥åŠ›ã—ã¦ãã ã•ã„", key="kwic_input")
                if kwic_keyword:
                    kwic_html_content = generate_kwic_html(df_analyzed, text_column, kwic_keyword)
                    html(kwic_html_content, height=400, scrolling=True)

                with st.expander("åˆ†æãƒ—ãƒ­ã‚»ã‚¹ã¨è«–æ–‡è¨˜è¿°ä¾‹"):
                    st.markdown("""
                        #### 1. åˆ†æãƒ—ãƒ­ã‚»ã‚¹
                        1.  **ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢**: æŒ‡å®šã•ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆ`*` ãƒ¯ã‚¤ãƒ«ãƒ‰ã‚«ãƒ¼ãƒ‰åˆ©ç”¨å¯ï¼‰ã«åŸºã¥ãã€åˆ†æå¯¾è±¡ã®ãƒ†ã‚­ã‚¹ãƒˆåˆ—ï¼ˆåŸæ–‡ï¼‰ã«å¯¾ã—ã¦æ­£è¦è¡¨ç¾æ¤œç´¢ã‚’å®Ÿè¡Œã—ã¾ã—ãŸã€‚
                        2.  **æ–‡è„ˆæŠ½å‡º**: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¨ä¸€è‡´ã—ãŸç®‡æ‰€ã®å‰å¾Œ40æ–‡å­—ã‚’ã€Œæ–‡è„ˆï¼ˆã‚³ãƒ³ãƒ†ã‚¯ã‚¹ãƒˆï¼‰ã€ã¨ã—ã¦æŠ½å‡ºã—ã€ä¸€è¦§è¡¨ç¤ºï¼ˆã‚³ãƒ³ã‚³ãƒ¼ãƒ€ãƒ³ã‚¹ãƒ»ãƒ©ã‚¤ãƒ³ï¼‰ã—ã¾ã—ãŸã€‚
                        
                        #### 2. è«–æ–‡è¨˜è¿°ä¾‹
                        > ...å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åˆ†æã§æ³¨ç›®ã•ã‚ŒãŸ[å˜èªA]ã«ã¤ã„ã¦ã€å®Ÿéš›ã®æ–‡è„ˆã‚’è©³ç´°ã«ç¢ºèªã™ã‚‹ãŸã‚ã€KWICï¼ˆKeyWord In Contextï¼‰åˆ†æã‚’è¡Œã£ãŸã€‚æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€Œ[å˜èªA]ã€ã§åŸæ–‡ã‚’æ¤œç´¢ã—ãŸçµæœï¼ˆè¡¨1ï¼‰ã€...
                        >
                        > è¡¨1ï¼ˆ*KWICã®çµæœã‚’è«–æ–‡ã«å¼•ç”¨*ï¼‰
                        > ... [å˜èªA]ã¯ã€ä¸»ã«ã€Œ...ã€ã¨ã„ã£ãŸæ–‡è„ˆã§ãƒã‚¸ãƒ†ã‚£ãƒ–ã«ä½¿ç”¨ã•ã‚Œã‚‹ä¸€æ–¹ã€ã€Œ...ã€ã¨ã„ã†ãƒã‚¬ãƒ†ã‚£ãƒ–ãªæ–‡è„ˆã§ã‚‚å‡ºç¾ã—ã¦ãŠã‚Šã€...
                    """)

            # --- Tab 6: å±æ€§åˆ¥ ç‰¹å¾´èª --- (tab6 ã«å¤‰æ›´)
            with tab6:
                st.subheader("å±æ€§åˆ¥ ç‰¹å¾´èªï¼ˆã‚«ã‚¤äºŒä¹—æ¤œå®šï¼‰")
                if not attribute_columns: st.warning("ã“ã®åˆ†æã‚’è¡Œã†ã«ã¯åˆ†æè»¸ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚")
                else:
                    attr_col_for_chi2 = attribute_columns[0]
                    st.info(f"å±æ€§ ã€Œ**{attr_col_for_chi2}**ã€ ã®å€¤ã”ã¨ã«ç‰¹å¾´çš„ãªå˜èªã‚’è¨ˆç®—ã—ã¾ã™ã€‚på€¤<0.05ã®æœ‰æ„ãªå˜èªã‚’è¡¨ç¤ºã€‚")
                    if ('chi2_results_display' not in st.session_state or
                        st.session_state.get('chi2_sw_display') != st.session_state.dynamic_stopwords):
                        with st.spinner(f"ã€Œ{attr_col_for_chi2}ã€ã®ç‰¹å¾´èªã‚’è¨ˆç®—ä¸­..."):
                            chi2_results = calculate_characteristic_words(df_analyzed, attr_col_for_chi2, text_column, current_stopwords_set)
                            st.session_state.chi2_results_display = chi2_results
                            st.session_state.chi2_sw_display = st.session_state.dynamic_stopwords
                    chi2_results = st.session_state.chi2_results_display
                    if "error" in chi2_results: st.error(chi2_results["error"])
                    else:
                        if not chi2_results or all(not words for words in chi2_results.values()):
                            st.info(f"å±æ€§ã€Œ{attr_col_for_chi2}ã€ã«ã¯çµ±è¨ˆçš„ã«æœ‰æ„ãªç‰¹å¾´èªã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
                        else:
                            cols = st.columns(len(chi2_results))
                            for i, (attr_value, words) in enumerate(chi2_results.items()):
                                with cols[i % len(cols)]:
                                    st.markdown(f"**{attr_value}** ã®ç‰¹å¾´èª (Top 20)")
                                    if words:
                                        for word, p_value, chi2_val in words: st.write(f"- {word} (p={p_value:.3f})")
                                    else: st.info("ç‰¹å¾´èªãªã—")

                with st.expander("åˆ†æãƒ—ãƒ­ã‚»ã‚¹ã¨è«–æ–‡è¨˜è¿°ä¾‹"):
                    st.markdown("""
                        #### 1. åˆ†æãƒ—ãƒ­ã‚»ã‚¹
                        1.  **ã‚¯ãƒ­ã‚¹é›†è¨ˆ**: é¸æŠã•ã‚ŒãŸå±æ€§ï¼ˆä¾‹: ã€Œå¹´ä»£ã€ï¼‰ã®å„ã‚«ãƒ†ã‚´ãƒªï¼ˆä¾‹: ã€Œ20ä»£ã€ï¼‰ã¨ã€åˆ†æå¯¾è±¡ã®å…¨å˜èªï¼ˆã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ç­‰é™¤å¤–å¾Œï¼‰ã«ã¤ã„ã¦ã€2x2ã®ã‚¯ãƒ­ã‚¹é›†è¨ˆè¡¨ï¼ˆåˆ†å‰²è¡¨ï¼‰ã‚’ä½œæˆã—ã¾ã—ãŸã€‚
                            * a: ã€Œ20ä»£ã€ã®æ–‡æ›¸ã§ã€å˜èªXã‚’ã€Œå«ã‚€ã€
                            * b: ã€Œ20ä»£ä»¥å¤–ã€ã®æ–‡æ›¸ã§ã€å˜èªXã‚’ã€Œå«ã‚€ã€
                            * c: ã€Œ20ä»£ã€ã®æ–‡æ›¸ã§ã€å˜èªXã‚’ã€Œå«ã¾ãªã„ã€
                            * d: ã€Œ20ä»£ä»¥å¤–ã€ã®æ–‡æ›¸ã§ã€å˜èªXã‚’ã€Œå«ã¾ãªã„ã€
                        2.  **çµ±è¨ˆæ¤œå®š**: ã“ã®é›†è¨ˆè¡¨ã«å¯¾ã—ã€`Scipy` ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ç”¨ã„ã¦ã‚«ã‚¤äºŒä¹—æ¤œå®šï¼ˆç‹¬ç«‹æ€§ã®æ¤œå®šï¼‰ã‚’å®Ÿè¡Œã—ã¾ã—ãŸã€‚
                        3.  **ç‰¹å¾´èªæŠ½å‡º**: å„ã‚«ãƒ†ã‚´ãƒªï¼ˆã€Œ20ä»£ã€ï¼‰ã«ãŠã„ã¦ã€(1) på€¤ãŒ 0.05 æœªæº€ï¼ˆçµ±è¨ˆçš„ã«æœ‰æ„ï¼‰ã§ã‚ã‚Šã€(2) å®Ÿéš›ã®å‡ºç¾æ•°(a)ãŒæœŸå¾…å€¤ã‚ˆã‚Šã‚‚é«˜ã‹ã£ãŸå˜èªã‚’ã€ãã®ã‚«ãƒ†ã‚´ãƒªã®ã€Œç‰¹å¾´èªã€ã¨ã—ã¦æŠ½å‡ºã—ã¾ã—ãŸã€‚
                        4.  **è¡¨ç¤º**: på€¤ãŒä½ã„é †ï¼ˆï¼ã‚ˆã‚Šæœ‰æ„ãªé †ï¼‰ã«ã‚½ãƒ¼ãƒˆã—ã€ä¸Šä½20ä»¶ã‚’è¡¨ç¤ºã—ã¾ã—ãŸã€‚
                        
                        #### 2. è«–æ–‡è¨˜è¿°ä¾‹
                        > ...å±æ€§[å±æ€§å]ã«ã‚ˆã‚‹ãƒ†ã‚­ã‚¹ãƒˆå†…å®¹ã®å·®ç•°ã‚’çµ±è¨ˆçš„ã«æ¤œè¨¼ã™ã‚‹ãŸã‚ã€ç‰¹å¾´èªæŠ½å‡ºã‚’è¡Œã£ãŸã€‚å„å±æ€§ã‚«ãƒ†ã‚´ãƒªï¼ˆä¾‹: ã€ŒAç¾¤ã€ï¼‰ã¨å…¨å˜èªã«ã¤ã„ã¦2x2ã®åˆ†å‰²è¡¨ã‚’ä½œæˆã—ã€ã‚«ã‚¤äºŒä¹—æ¤œå®šï¼ˆç‹¬ç«‹æ€§ã®æ¤œå®šï¼‰ã‚’å®Ÿæ–½ã—ãŸã€‚ãã®çµæœã€på€¤ãŒ0.05æœªæº€ã‹ã¤æ®‹å·®ãŒæ­£ã§ã‚ã£ãŸå˜èªã‚’ã€å„ã‚«ãƒ†ã‚´ãƒªã®ã€Œç‰¹å¾´èªã€ã¨ã—ã¦æŠ½å‡ºã—ãŸï¼ˆè¡¨2å‚ç…§ï¼‰ã€‚
                        >
                        > è¡¨2ã®çµæœã‚ˆã‚Šã€ã€ŒAç¾¤ã€ã§ã¯[å˜èªX, Y]ãŒã€ã€ŒBç¾¤ã€ã§ã¯[å˜èªZ]ãŒç‰¹å¾´çš„ã«å‡ºç¾ã—ã¦ãŠã‚Šã€...
                    """)

            # --- Tab 7: AI å­¦è¡“è«–æ–‡ --- (tab7 ã«å¤‰æ›´)
            with tab7:
                st.subheader("AIã«ã‚ˆã‚‹å­¦è¡“è«–æ–‡é¢¨ã‚µãƒãƒªãƒ¼")
                if 'ai_result_academic' not in st.session_state:
                    with st.spinner("AIã«ã‚ˆã‚‹ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ†æã¨å­¦è¡“è«–æ–‡é¢¨ã‚µãƒãƒªãƒ¼ã‚’ç”Ÿæˆä¸­..."):

                        if analyzed_items < total_items: st.warning(analysis_scope_warning, icon="âš ï¸")
                        else: st.info(analysis_scope_warning, icon="âœ…")
                        
                        try:
                            cluster_json_str = get_or_generate_cluster_json(ai_input_text, analysis_scope_instr, analyzed_items)
                        except Exception as e:
                            st.error(f"ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼JSONã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
                            cluster_json_str = '{"name": "ã‚¨ãƒ©ãƒ¼", "children": []}'

                        contents_acad = [{"parts": [{"text": ai_input_text}]}]
                        has_attr_a = bool(attribute_columns)
                        has_attribute_str_a = "## 4. å±æ€§é–“ã®æ¯”è¼ƒåˆ†æ (Comparative Analysis)\n(å±æ€§ï¼ˆã‚«ãƒ†ã‚´ãƒªï¼‰é–“ã§è¦‹ã‚‰ã‚ŒãŸé¡•è‘—ãªå·®ç•°ã‚„ç‰¹å¾´çš„ãªå‚¾å‘ã«ã¤ã„ã¦ã€å…·ä½“çš„ã«æ¯”è¼ƒãƒ»è¨˜è¿°ã™ã‚‹)" if has_attr_a else ""
                        attr_instr_a = "ãƒ‡ãƒ¼ã‚¿ã¯ã€Œå±æ€§ || ãƒ†ã‚­ã‚¹ãƒˆã€ã®å½¢å¼ã§ã™ã€‚å±æ€§ã”ã¨ã®å‚¾å‘ã‚„é•ã„ã«ã‚‚ç€ç›®ã—ã¦åˆ†æã—ã¦ãã ã•ã„ã€‚" if has_attr_a else ""
                        
                        system_instr_a = SYSTEM_PROMPT_ACADEMIC.format(
                            analysis_scope_instruction=analysis_scope_instr,
                            attributeInstruction=attr_instr_a, 
                            has_attribute=has_attribute_str_a,
                            cluster_json_data=cluster_json_str 
                        )
                        st.session_state.ai_result_academic = call_gemini_api(contents_acad, system_instruction=system_instr_a)
                st.markdown(st.session_state.ai_result_academic)
                
            # --- Tab 8: AI ãƒãƒ£ãƒƒãƒˆ --- (tab8 ã«å¤‰æ›´)
            with tab8:
                st.subheader("ğŸ’¬ AI ãƒãƒ£ãƒƒãƒˆ (ãƒ‡ãƒ¼ã‚¿åˆ†æ)")
                st.info("AIã«è³ªå•ã§ãã¾ã™ã€‚") 
                if "chat_messages" not in st.session_state: st.session_state.chat_messages = []
                for message in st.session_state.chat_messages:
                    with st.chat_message(message["role"]): st.markdown(message["content"])
                if prompt := st.chat_input("AIã«è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ (ä¾‹: ä¸»ãªèª²é¡Œã¯ä½•ã§ã™ã‹ï¼Ÿ)"):
                    st.session_state.chat_messages.append({"role": "user", "content": prompt})
                    with st.chat_message("user"): st.markdown(prompt)
                    with st.spinner("AIãŒå¿œç­”ã‚’ç”Ÿæˆä¸­..."):
                        
                        if analyzed_items < total_items: 
                            with st.chat_message("assistant", avatar="âš ï¸"):
                                st.warning(f"ï¼ˆAIã¸ã®å‚ç…§ãƒ‡ãƒ¼ã‚¿ã¯ã€å…¨{total_items:,}ä»¶ä¸­ã€å…ˆé ­{analyzed_items:,}ä»¶ã«åˆ¶é™ã•ã‚Œã¦ã„ã¾ã™ï¼‰")
                        
                        context_text = ai_input_text

                        api_contents = []
                        first_user_message = f"""ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ï¼ˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼‰ã«ã¤ã„ã¦è³ªå•ãŒã‚ã‚Šã¾ã™ã€‚\n\n--- ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ ---\n{context_text}\n\n--- è³ªå• ---\n{prompt}"""
                        is_first_turn = len(st.session_state.chat_messages) == 1
                        
                        for msg in st.session_state.chat_messages[:-1]:
                            api_contents.append({"role": "user" if msg["role"] == "user" else "model", "parts": [{"text": msg["content"]}]})
                        api_contents.append({"role": "user", "parts": [{"text": first_user_message if is_first_turn else prompt}]})

                        response = call_gemini_api(api_contents, system_instruction=SYSTEM_PROMPT_CHAT)
                        st.session_state.chat_messages.append({"role": "assistant", "content": response})
                        with st.chat_message("assistant"): st.markdown(response)

    except Exception as e:
        st.error(f"ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã¾ãŸã¯åˆ†æä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        st.exception(e) # è©³ç´°ãªã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã‚’è¡¨ç¤º
