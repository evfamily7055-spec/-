import streamlit as st
import pandas as pd
import re
import requests # Gemini APIå‘¼ã³å‡ºã—ç”¨
import time # ãƒªãƒˆãƒ©ã‚¤ç”¨
from janome.tokenizer import Tokenizer
from wordcloud import WordCloud
import networkx as nx
import matplotlib.pyplot as plt
import matplotlib.font_manager
from collections import Counter
from itertools import combinations
import japanize_matplotlib # Matplotlibã®æ—¥æœ¬èªåŒ–
import numpy as np # --- çµ±è¨ˆè¨ˆç®—ã®ãŸã‚ã«è¿½åŠ  ---
from scipy.stats import chi2_contingency # --- ã‚«ã‚¤äºŒä¹—æ¤œå®šã®ãŸã‚ã«è¿½åŠ  ---
from streamlit_extras.row import row # --- å±æ€§åˆ¥ç‰¹å¾´èªã®è¡¨ç¤ºæ”¹å–„ã®ãŸã‚ã«è¿½åŠ  ---
import prince # --- å¯¾å¿œåˆ†æã®ãŸã‚ã«è¿½åŠ  ---

# --- 1. ã‚¢ãƒ—ãƒªã®åŸºæœ¬è¨­å®š ---
st.set_page_config(page_title="çµ±è¨ˆï¼‹AI çµ±åˆã‚¢ãƒŠãƒ©ã‚¤ã‚¶ãƒ¼", layout="wide")
st.title("çµ±è¨ˆï¼‹AI çµ±åˆãƒ†ã‚­ã‚¹ãƒˆã‚¢ãƒŠãƒ©ã‚¤ã‚¶ãƒ¼ ğŸ“ŠğŸ¤–")
st.write("Excelã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã€ãƒ†ã‚­ã‚¹ãƒˆåˆ—ã¨åˆ†æè»¸ï¼ˆå±æ€§ï¼‰ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚çµ±è¨ˆåˆ†æã¨AIã«ã‚ˆã‚‹è¦ç´„ã‚’åŒæ™‚ã«å®Ÿè¡Œã—ã¾ã™ã€‚")

# --- 2. å½¢æ…‹ç´ è§£æï¼†ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰è¨­å®š (ã‚­ãƒ£ãƒƒã‚·ãƒ¥) ---
@st.cache_resource # å½¢æ…‹ç´ è§£æå™¨ã¯é‡ã„ã®ã§ã‚­ãƒ£ãƒƒã‚·ãƒ¥
def get_tokenizer():
    return Tokenizer()

# ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ï¼ˆä¸€èˆ¬çš„ã™ãã‚‹å˜èªï¼‰
STOPWORDS = set([
    'ã®', 'ã«', 'ã¯', 'ã‚’', 'ãŸ', 'ã§ã™', 'ã¾ã™', 'ãŒ', 'ã§', 'ã‚‚', 'ã¦', 'ã¨', 'ã—', 'ã‚Œ', 'ã•', 'ã‚ã‚‹', 'ã„ã‚‹', 'ã™ã‚‹',
    'ãªã„', 'ã“ã¨', 'ã‚‚ã®', 'ã“ã‚Œ', 'ãã‚Œ', 'ã‚ã‚Œ', 'ã‚ˆã†', 'ãŸã‚', 'äºº', 'ä¸­', 'ç­‰', 'æ€ã†', 'ã„ã†', 'ãªã‚‹', 'æ—¥', 'æ™‚'
])

# ãƒ†ã‚­ã‚¹ãƒˆã‚’å˜èªãƒªã‚¹ãƒˆã«åˆ†å‰²ã™ã‚‹é–¢æ•°
@st.cache_data # ãƒ†ã‚­ã‚¹ãƒˆã¨Tokenizerã«å¤‰åŒ–ãŒãªã‘ã‚Œã°ã‚­ãƒ£ãƒƒã‚·ãƒ¥
def extract_words(text, _tokenizer): # _tokenizerå¼•æ•°ã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ã‚­ãƒ¼ã¨ã—ã¦ä½¿ã†
    if not isinstance(text, str):
        return []
    tokenizer = get_tokenizer()
    tokens = tokenizer.tokenize(text)
    words = []
    for token in tokens:
        part_of_speech = token.part_of_speech.split(',')[0]
        # åè©ã€å‹•è©ã€å½¢å®¹è©ã®ã¿ã‚’æŠ½å‡º
        if part_of_speech in ['åè©', 'å‹•è©', 'å½¢å®¹è©']:
            # ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ã§ãªãã€1æ–‡å­—ä»¥ä¸Šãªã‚‰è¿½åŠ 
            if token.base_form not in STOPWORDS and len(token.base_form) > 1:
                words.append(token.base_form)
    return words

# --- 3. Gemini AI åˆ†æé–¢æ•° ---
@st.cache_data # AIåˆ†æçµæœã‚‚ã‚­ãƒ£ãƒƒã‚·ãƒ¥
def call_gemini_api(text_data, has_attribute):
    try:
        apiKey = st.secrets["GEMINI_API_KEY"]
        if not apiKey:
            return "AIåˆ†æã‚¨ãƒ©ãƒ¼: Streamlit Cloudã®Secretsã« `GEMINI_API_KEY` ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚"
    except Exception:
         return "AIåˆ†æã‚¨ãƒ©ãƒ¼: Streamlit Cloudã®Secretsã« `GEMINI_API_KEY` ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ã‚¢ãƒ—ãƒªå³ä¸‹ã® 'Manage app' > 'Settings' > 'Secrets' ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"

    apiUrl = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-09-2025:generateContent?key={apiKey}"

    attributeInstruction = ("ãƒ‡ãƒ¼ã‚¿ã¯ã€Œå±æ€§ || ãƒ†ã‚­ã‚¹ãƒˆã€ã®å½¢å¼ã§ã™ã€‚å±æ€§ã”ã¨ã®å‚¾å‘ã‚„é•ã„ã«ã‚‚ç€ç›®ã—ã¦åˆ†æã—ã¦ãã ã•ã„ã€‚"
                            if has_attribute
                            else "")
    
    systemPrompt = f"""ã‚ãªãŸã¯ã€ãƒ†ã‚­ã‚¹ãƒˆãƒã‚¤ãƒ‹ãƒ³ã‚°ã®å°‚é–€å®¶ã§ã™ã€‚ä¸ãˆã‚‰ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆç¾¤ã‚’åˆ†æã—ã€çµæœã‚’è©³ç´°ã‹ã¤åˆ†ã‹ã‚Šã‚„ã™ããƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³å½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
{attributeInstruction}
## 1. åˆ†æã‚µãƒãƒªãƒ¼
(å…¨ä½“ã®å‚¾å‘ã‚’ç°¡æ½”ã«è¦ç´„)
## 2. ä¸»è¦ãªãƒ†ãƒ¼ãƒ
(é »å‡ºã™ã‚‹ãƒˆãƒ”ãƒƒã‚¯ã‚„æ„è¦‹ã®ã‚«ãƒ†ã‚´ãƒªã‚’3ã€œ5å€‹æç¤º)
## 3. ãƒã‚¸ãƒ†ã‚£ãƒ–ãªæ„è¦‹
(å…·ä½“çš„ãªè‰¯ã„ç‚¹ã‚’å¼•ç”¨ã—ã¤ã¤ãƒªã‚¹ãƒˆã‚¢ãƒƒãƒ—)
## 4. ãƒã‚¬ãƒ†ã‚£ãƒ–ãªæ„è¦‹ãƒ»èª²é¡Œ
(å…·ä½“çš„ãªä¸æº€ã‚„æ”¹å–„ç‚¹ã‚’å¼•ç”¨ã—ã¤ã¤ãƒªã‚¹ãƒˆã‚¢ãƒƒãƒ—)
{has_attribute and '## 5. å±æ€§åˆ¥ã®å‚¾å‘ (ã‚‚ã—ã‚ã‚Œã°)\n(å±æ€§ã”ã¨ã®ç‰¹å¾´çš„ãªæ„è¦‹ã‚’æ¯”è¼ƒ)'}
## 6. ç·è©•ã¨ãƒã‚¯ã‚¹ãƒˆã‚¢ã‚¯ã‚·ãƒ§ãƒ³
(åˆ†æã‹ã‚‰è¨€ãˆã‚‹ã“ã¨ã€æ¬¡ã«è¡Œã†ã¹ãã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’ææ¡ˆ)
"""
    
    payload = {
        "systemInstruction": {"parts": [{"text": systemPrompt}]},
        "contents": [{"parts": [{"text": text_data}]}]
    }

    try:
        response = None
        delay = 1000
        for i in range(5): # Exponential backoff retry
            response = requests.post(apiUrl, json=payload, headers={'Content-Type': 'application/json'})
            if response.status_code == 200:
                break
            elif response.status_code == 429 or response.status_code >= 500:
                time.sleep(delay / 1000)
                delay *= 2
            else:
                response.raise_for_status() # ä»–ã®ã‚¨ãƒ©ãƒ¼
        
        if response.status_code != 200:
             return f"AIåˆ†æã«å¤±æ•—ã—ã¾ã—ãŸã€‚ãƒªãƒˆãƒ©ã‚¤å¾Œã‚‚ã‚¨ãƒ©ãƒ¼ãŒè§£æ¶ˆã—ã¾ã›ã‚“ã§ã—ãŸã€‚ (Status: {response.status_code})"

        result = response.json()
        text = result.get('candidates', [{}])[0].get('content', {}).get('parts', [{}])[0].get('text', '')
        return text if text else "AIã‹ã‚‰ã®å¿œç­”ãŒç©ºã§ã—ãŸã€‚"

    except Exception as e:
        if "403" in str(e):
            return "AIåˆ†æä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: 403 Forbidden. APIã‚­ãƒ¼ãŒæ­£ã—ã„ã‹ã€Google Cloudã§Gemini APIãŒæœ‰åŠ¹ã«ãªã£ã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
        return f"AIåˆ†æä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"

# --- 4. KWICï¼ˆæ–‡è„ˆæ¤œç´¢ï¼‰é–¢æ•° ---
def generate_kwic_html(df, text_column, keyword, max_results=100):
    if not keyword:
        return "<p>ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚</p>"
    
    # å¤§æ–‡å­—å°æ–‡å­—ã‚’åŒºåˆ¥ã—ãªã„æ­£è¦è¡¨ç¾
    try:
        # ãƒ¯ã‚¤ãƒ«ãƒ‰ã‚«ãƒ¼ãƒ‰ã‚’è€ƒæ…®
        search_pattern = keyword.replace('*', '.*')
        kwic_pattern = re.compile(f'(.{{0,40}})({search_pattern})(.{{0,40}})', re.IGNORECASE)
    except re.error as e:
        return f"<p>ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}</p>"

    results = []
    for text in df[text_column].dropna():
        for match in kwic_pattern.finditer(text):
            if len(results) >= max_results:
                break
            left, center, right = match.groups()
            html_row = f"""
            <div style="margin-bottom: 10px; padding: 5px; border-bottom: 1px solid #eee; font-family: sans-serif;">
                <span style="text-align: right; display: inline-block; width: 45%; color: #555; overflow: hidden; text-overflow: ellipsis; white-space: nowrap;">...{left}</span>
                <span style="background-color: yellow; font-weight: bold; padding: 2px 0;">{center}</span>
                <span style="text-align: left; display: inline-block; width: 45%; color: #555; overflow: hidden; text-overflow: ellipsis; white-space: nowrap;">{right}...</span>
            </div>
            """
            results.append(html_row)
        if len(results) >= max_results:
            break
            
    if not results:
        return f"<p>ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€Œ{keyword}ã€ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚</p>"
        
    return f"<h4>ã€Œ{keyword}ã€ã®æ¤œç´¢çµæœ ({len(results)} ä»¶)</h4>" + "".join(results)

# --- 5. å±æ€§åˆ¥ ç‰¹å¾´èªï¼ˆã‚«ã‚¤äºŒä¹—æ¤œå®šï¼‰é–¢æ•° ---
@st.cache_data
def calculate_characteristic_words(_df, attribute_col, text_col): # _df ã‚’å—ã‘å–ã‚‹
    results = {}
    
    # å±æ€§ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªå€¤ã‚’å–å¾—
    try:
        unique_attrs = _df[attribute_col].dropna().unique()
    except KeyError:
        return {"error": "å±æ€§åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"}
        
    if len(unique_attrs) < 2:
        return {"error": "æ¯”è¼ƒå¯¾è±¡ã®å±æ€§ãŒ2ã¤æœªæº€ã§ã™ã€‚"}
    
    # å…¨ä½“ã®èªå½™ï¼ˆå˜èªãƒªã‚¹ãƒˆï¼‰ã‚’ä½œæˆ
    all_words = set(word for sublist in _df['words'] for word in sublist)
    total_docs = len(_df)
    
    for attr_value in unique_attrs:
        
        attr_df = _df[_df[attribute_col] == attr_value]
        non_attr_df = _df[_df[attribute_col] != attr_value]
        
        total_docs_in_attr = len(attr_df)
        total_docs_not_in_attr = total_docs - total_docs_in_attr
        
        if total_docs_in_attr == 0 or total_docs_not_in_attr == 0:
            continue
            
        characteristic_words = []
        
        for word in all_words:
            # ã‚«ã‚¤äºŒä¹—æ¤œå®šã®ãŸã‚ã®åˆ†å‰²è¡¨ã‚’ä½œæˆ
            # a: å±æ€§Aã«å«ã¾ã‚Œã€å˜èªXã‚‚å«ã‚€æ–‡æ›¸æ•°
            a = sum(1 for words_list in attr_df['words'] if word in words_list)
            # b: å±æ€§Aä»¥å¤–ã«å«ã¾ã‚Œã€å˜èªXã‚’å«ã‚€æ–‡æ›¸æ•°
            b = sum(1 for words_list in non_attr_df['words'] if word in words_list)
            
            # c: å±æ€§Aã«å«ã¾ã‚Œã€å˜èªXã‚’å«ã¾ãªã„æ–‡æ›¸æ•°
            c = total_docs_in_attr - a
            # d: å±æ€§Aä»¥å¤–ã«å«ã¾ã‚Œã€å˜èªXã‚’å«ã¾ãªã„æ–‡æ›¸æ•°
            d = total_docs_not_in_attr - b
            
            # æœŸå¾…å€¤ãŒ0ã®å ´åˆã‚’é¿ã‘ã‚‹ (chi2_contingencyã®ValueErrorå¯¾ç­–)
            if (a+b) == 0 or (a+c) == 0 or (b+d) == 0 or (c+d) == 0:
                continue

            contingency_table = np.array([[a, b], [c, d]])
            
            try:
                # è­¦å‘Šã‚’ç„¡è¦–ã™ã‚‹è¨­å®š
                with np.errstate(divide='ignore', invalid='ignore'):
                    chi2, p, dof, expected = chi2_contingency(contingency_table)
                
                # å˜èªãŒã“ã®å±æ€§ã§ã€Œã‚ˆã‚Šå¤šãã€ä½¿ã‚ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯ (æœŸå¾…å€¤ã¨æ¯”è¼ƒ)
                # på€¤ãŒæœ‰æ„æ°´æº–ä»¥ä¸‹ (ä¾‹: 0.05) ã‹ã¤ã€å®Ÿéš›ã®å‡ºç¾é »åº¦ãŒæœŸå¾…å€¤ã‚ˆã‚Šé«˜ã„å ´åˆ
                if p < 0.05 and a > expected[0, 0]:
                    characteristic_words.append((word, p, chi2))
            except ValueError:
                continue # æœŸå¾…å€¤ã«0ãŒå«ã¾ã‚Œã‚‹å ´åˆãªã©

        # på€¤ã§ã‚½ãƒ¼ãƒˆ (på€¤ãŒå°ã•ã„ã»ã©çµ±è¨ˆçš„ã«æœ‰æ„)
        characteristic_words.sort(key=lambda x: x[1])
        results[attr_value] = characteristic_words[:20] # ä¸Šä½20ä»¶
        
    return results

# --- 6. å¯¾å¿œåˆ†æ é–¢æ•° ---
@st.cache_data
def run_correspondence_analysis(_df, attribute_col): # _df ã‚’å—ã‘å–ã‚‹
    try:
        # æœ€ã‚‚é »ç¹ã«å‡ºç¾ã™ã‚‹å˜èª Top 30 ã‚’é¸å®š
        all_words_list = [word for sublist in _df['words'] for word in sublist]
        top_words = [word for word, freq in Counter(all_words_list).most_common(30)]
        
        # å±æ€§ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªå€¤ã‚’å–å¾—
        unique_attrs = _df[attribute_col].dropna().unique()
        
        if len(unique_attrs) < 2:
            return None, "å¯¾å¿œåˆ†æã‚¨ãƒ©ãƒ¼: æ¯”è¼ƒå¯¾è±¡ã®å±æ€§ãŒ2ã¤æœªæº€ã§ã™ã€‚"
        if not top_words:
            return None, "å¯¾å¿œåˆ†æã‚¨ãƒ©ãƒ¼: åˆ†æå¯¾è±¡ã®å˜èªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"

        # ã€Œå˜èª Ã— å±æ€§ã€ã®ã‚¯ãƒ­ã‚¹é›†è¨ˆè¡¨ã‚’ä½œæˆ
        cross_tab = pd.DataFrame(index=top_words, columns=unique_attrs)
        for word in top_words:
            for attr in unique_attrs:
                # ãã®å±æ€§ã‚’æŒã¡ã€ã‹ã¤ãã®å˜èªã‚’å«ã‚€æ–‡æ›¸ã®æ•°
                count = sum(1 for i, row in _df[_df[attribute_col] == attr].iterrows() if word in row['words'])
                cross_tab.loc[word, attr] = count
        
        # NaNã‚’0ã§åŸ‹ã‚ã‚‹
        cross_tab = cross_tab.fillna(0).astype(int)
        
        # åˆè¨ˆãŒ0ã®è¡Œãƒ»åˆ—ãŒã‚ã‚‹ã¨ã‚¨ãƒ©ãƒ¼ã«ãªã‚‹ãŸã‚å‰Šé™¤
        cross_tab = cross_tab.loc[cross_tab.sum(axis=1) > 0]
        cross_tab = cross_tab.loc[:, cross_tab.sum(axis=0) > 0]
        
        if cross_tab.empty:
            return None, "å¯¾å¿œåˆ†æã‚¨ãƒ©ãƒ¼: æœ‰åŠ¹ãªã‚¯ãƒ­ã‚¹é›†è¨ˆè¡¨ã‚’ä½œæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"

        # å¯¾å¿œåˆ†æã‚’å®Ÿè¡Œ
        ca = prince.CA(
            n_components=2,
            n_iter=3,
            copy=True,
            check_input=True,
            random_state=42
        )
        ca = ca.fit(cross_tab)

        # ãƒ—ãƒ­ãƒƒãƒˆã‚’ä½œæˆ
        fig = ca.plot(
            cross_tab,
            x_component=0,
            y_component=1,
            show_row_labels=True,
            show_col_labels=True,
            figsize=(12, 12)
        )
        
        return fig, None

    except Exception as e:
        return None, f"å¯¾å¿œåˆ†æã‚¨ãƒ©ãƒ¼: {e}"

# --- 7. WordCloudç”Ÿæˆé–¢æ•° (ã‚­ãƒ£ãƒƒã‚·ãƒ¥) ---
@st.cache_data
def generate_wordcloud(_words_list, font_path):
    if not _words_list:
        return None, "å˜èªãŒã‚ã‚Šã¾ã›ã‚“"
    
    word_freq = Counter(_words_list)
    try:
        if not font_path:
            fig_wc, ax = plt.subplots(figsize=(12, 6))
            ax.text(0.5, 0.5, "æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“", ha='center', va='center', fontsize=16)
            ax.axis('off')
            return fig_wc, None
        else:
            wc = WordCloud(width=800, height=400, background_color='white',
                           font_path=font_path).generate_from_frequencies(word_freq)
            fig_wc, ax = plt.subplots(figsize=(12, 6))
            ax.imshow(wc, interpolation='bilinear')
            ax.axis('off')
            return fig_wc, None
    except Exception as e:
        return None, f"WordCloudã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚ã‚¨ãƒ©ãƒ¼: {e}"

# --- 8. å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ç”Ÿæˆé–¢æ•° (ã‚­ãƒ£ãƒƒã‚·ãƒ¥) ---
@st.cache_data
def generate_network(_words_df, font_path):
    co_occur_counter = Counter()
    for words in _words_df: # df['words'] ã‚’å—ã‘å–ã‚‹
        unique_words = sorted(list(set(words)))
        for w1, w2 in combinations(unique_words, 2):
            co_occur_counter[(w1, w2)] += 1
    
    top_pairs = co_occur_counter.most_common(50)
    
    if top_pairs:
        G = nx.Graph()
        for (w1, w2), weight in top_pairs:
            G.add_edge(w1, w2, weight=weight)
        
        fig_net, ax = plt.subplots(figsize=(14, 14))
        pos = nx.spring_layout(G, k=0.8, iterations=50) # ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆè¨ˆç®—å›æ•°ã‚’å¢—ã‚„ã—ã¦å®‰å®šåŒ–
        
        nx.draw_networkx_nodes(G, pos, node_size=2000, node_color='lightblue', alpha=0.8)
        
        # ã‚¨ãƒƒã‚¸ã®å¤ªã•ã®ä¿‚æ•°ã‚’ 0.2 ã«
        edge_weights = [d['weight'] * 0.2 for u,v,d in G.edges(data=True)] # ç·šã®å¤ªã•èª¿æ•´
        nx.draw_networkx_edges(G, pos, width=edge_weights, alpha=0.4, edge_color='gray')
        
        # æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆæŒ‡å®š
        if font_path:
            nx.draw_networkx_labels(G, pos, font_size=10, font_family='IPAexGothic')
        else:
            nx.draw_networkx_labels(G, pos, font_size=10) # ãƒ•ã‚©ãƒ³ãƒˆãŒãªã„å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§

        ax.axis('off')
        return fig_net, None
    return None, "å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸï¼ˆå…±èµ·ãƒšã‚¢ä¸è¶³ï¼‰ã€‚"


# --- 9. ãƒ¡ã‚¤ãƒ³ç”»é¢ã®UI ---
uploaded_file = st.file_uploader("1. Excelãƒ•ã‚¡ã‚¤ãƒ« (xlsx) ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type=["xlsx"])

if uploaded_file:
    try:
        df = pd.read_excel(uploaded_file)
        # --- session_stateã®åˆæœŸåŒ– ---
        st.session_state.clear() # ãƒ•ã‚¡ã‚¤ãƒ«ãŒå¤‰ã‚ã£ãŸã‚‰ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢
        st.session_state.df_original = df
        st.subheader("èª­ã¿è¾¼ã¿ãƒ‡ãƒ¼ã‚¿ (å…ˆé ­5ä»¶)")
        st.dataframe(df.head())
        
        all_columns = df.columns.tolist()
        
        # --- 10. åˆ†æè¨­å®š (ã‚µã‚¤ãƒ‰ãƒãƒ¼) ---
        with st.sidebar:
            st.header("âš™ï¸ åˆ†æè¨­å®š")
            text_column = st.selectbox("åˆ†æã—ãŸã„ãƒ†ã‚­ã‚¹ãƒˆåˆ—", all_columns, index=0)
            
            attribute_columns = st.multiselect("åˆ†æè»¸ (è¤‡æ•°OK: ä¾‹: å¹´ä»£, æ€§åˆ¥)", all_columns)
            
            st.markdown("---")
            run_button = st.button("åˆ†æã‚’å®Ÿè¡Œ", type="primary", use_container_width=True)

        # --- 11. åˆ†æå®Ÿè¡Œ (å½¢æ…‹ç´ è§£æã®ã¿) ---
        if run_button:
            if not text_column:
                st.error("ã€Œåˆ†æã—ãŸã„ãƒ†ã‚­ã‚¹ãƒˆåˆ—ã€ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚")
            else:
                required_cols = [text_column] + attribute_columns
                if not all(col in df.columns for col in required_cols):
                    st.error(f"é¸æŠã•ã‚ŒãŸåˆ—ã®ä¸€éƒ¨ãŒExcelå†…ã«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚åˆ—åã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
                else:
                    # --- 11a. çµ±è¨ˆåˆ†æ (å½¢æ…‹ç´ è§£æ) ---
                    with st.spinner("ã‚¹ãƒ†ãƒƒãƒ—1/1: å½¢æ…‹ç´ è§£æã‚’å®Ÿè¡Œä¸­... (å…¨ä»¶å‡¦ç†)"):
                        df_analyzed = st.session_state.df_original.copy()
                        _tokenizer_instance = get_tokenizer() # ãƒ€ãƒŸãƒ¼ã§ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’æ¸¡ã™
                        df_analyzed['words'] = df_analyzed[text_column].apply(lambda x: extract_words(x, _tokenizer_instance))
                        
                        # --- 11b. å¿…é ˆæƒ…å ±ã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«ä¿å­˜ ---
                        st.session_state.df_analyzed = df_analyzed
                        st.session_state.text_column = text_column
                        st.session_state.attribute_columns = attribute_columns
                        
                        # å¤ã„è¨ˆç®—çµæœã‚’ã‚¯ãƒªã‚¢
                        st.session_state.pop('ai_result', None)
                        st.session_state.pop('fig_wc', None)
                        st.session_state.pop('fig_net', None)
                        st.session_state.pop('chi2_results', None)
                        st.session_state.pop('fig_ca', None)

                        st.success("å½¢æ…‹ç´ è§£æãŒå®Œäº†ã—ã¾ã—ãŸã€‚çµæœã‚¿ãƒ–ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦å„åˆ†æã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")

        # --- 12. çµæœè¡¨ç¤º (ã‚ªãƒ³ãƒ‡ãƒãƒ³ãƒ‰è¨ˆç®—) ---
        if 'df_analyzed' in st.session_state:
            st.subheader("ğŸ“Š åˆ†æçµæœ")
            
            # å¿…è¦ãªå¤‰æ•°ã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‹ã‚‰å–å¾—
            df_analyzed = st.session_state.df_analyzed
            text_column = st.session_state.text_column
            attribute_columns = st.session_state.attribute_columns
            
            # ãƒ•ã‚©ãƒ³ãƒˆãƒ‘ã‚¹ï¼ˆå…¨ã‚°ãƒ©ãƒ•ã§å…±æœ‰ï¼‰
            if 'font_path' not in st.session_state:
                try:
                    font_path = matplotlib.font_manager.findfont('IPAexGothic')
                    st.session_state.font_path = font_path
                except Exception:
                    st.session_state.font_path = None
            font_path = st.session_state.font_path
            if font_path is None:
                 st.warning("æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆ 'IPAexGothic' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ã‚°ãƒ©ãƒ•ã®æ—¥æœ¬èªãŒæ–‡å­—åŒ–ã‘ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚", icon="âš ï¸")

            tab_names = ["ğŸ¤– AI ã‚µãƒãƒªãƒ¼", "â˜ï¸ WordCloud", "ğŸ•¸ï¸ å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯", "ğŸ” KWIC (æ–‡è„ˆæ¤œç´¢)", "ğŸ“ˆ å±æ€§åˆ¥ ç‰¹å¾´èª", "ğŸ—ºï¸ å¯¾å¿œåˆ†æ"]
            tab1, tab2, tab3, tab4, tab5, tab6 = st.tabs(tab_names)
            
            # --- Tab 1: AI ã‚µãƒãƒªãƒ¼ ---
            with tab1:
                if 'ai_result' not in st.session_state:
                    with st.spinner("AIã«ã‚ˆã‚‹è¦ç´„ã‚’ç”Ÿæˆä¸­..."):
                        sample_df = df_analyzed.sample(n=min(len(df_analyzed), 100))
                        
                        def format_for_ai(row):
                            text = row[text_column] or ''
                            attrs = [str(row[col] or 'N/A') for col in attribute_columns]
                            if attrs:
                                return f"[{' | '.join(attrs)}] || {text}"
                            return text
                            
                        ai_input_text = "\n".join(sample_df.apply(format_for_ai, axis=1))
                        st.session_state.ai_result = call_gemini_api(ai_input_text, has_attribute=bool(attribute_columns))
                
                st.markdown(st.session_state.ai_result)
            
            # --- Tab 2: WordCloud ---
            with tab2:
                st.subheader("å…¨ä½“ã®WordCloud")
                if 'fig_wc' not in st.session_state:
                    with st.spinner("WordCloudã‚’è¨ˆç®—ä¸­..."):
                        all_words_list = [word for sublist in df_analyzed['words'] for word in sublist]
                        fig_wc, wc_error = generate_wordcloud(all_words_list, font_path)
                        st.session_state.fig_wc = fig_wc
                        st.session_state.wc_error = wc_error
                
                if st.session_state.fig_wc:
                    st.pyplot(st.session_state.fig_wc)
                else:
                    st.warning(st.session_state.wc_error)
                
                st.markdown("---")
                st.subheader("å±æ€§åˆ¥ã®WordCloud")
                
                if not attribute_columns:
                    st.warning("å±æ€§åˆ¥WordCloudã‚’è¡¨ç¤ºã™ã‚‹ã«ã¯ã€ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§ã€Œåˆ†æè»¸ã€ã‚’1ã¤ä»¥ä¸Šé¸æŠã—ã¦ãã ã•ã„ã€‚")
                else:
                    selected_attr_for_wc = st.selectbox(
                        "WordCloudã®åˆ†æè»¸ã‚’é¸æŠ", 
                        options=attribute_columns, 
                        index=0, 
                        key="wc_attr_select"
                    )
                    
                    if selected_attr_for_wc:
                        try:
                            unique_values = sorted(df_analyzed[selected_attr_for_wc].dropna().unique())
                        except TypeError: # æ•°å€¤ã¨æ–‡å­—åˆ—ãŒæ··åœ¨ã—ã¦ã„ã‚‹å ´åˆ
                            unique_values = sorted(df_analyzed[selected_attr_for_wc].dropna().astype(str).unique())

                        st.info(f"ã€Œ**{selected_attr_for_wc}**ã€ã®å€¤ã”ã¨ã«WordCloudã‚’ç”Ÿæˆã—ã¾ã™ã€‚")
                        
                        for val in unique_values:
                            st.markdown(f"#### {selected_attr_for_wc} : **{val}**")
                            subset_df = df_analyzed[df_analyzed[selected_attr_for_wc] == val]
                            subset_words_list = [word for sublist in subset_df['words'] for word in sublist]
                            
                            if not subset_words_list:
                                st.info("ã“ã®ã‚«ãƒ†ã‚´ãƒªã«ã¯è¡¨ç¤ºã™ã‚‹å˜èªãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
                                continue
                            
                            # å±æ€§åˆ¥WordCloudã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã—ãªã„ï¼ˆã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–æ€§é‡è¦–ï¼‰
                            fig_subset_wc, ax = plt.subplots(figsize=(10, 5))
                            if not font_path:
                                ax.text(0.5, 0.5, "æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“", ha='center', va='center', fontsize=14)
                                ax.axis('off')
                            else:
                                try:
                                    wc = WordCloud(width=600, height=300, background_color='white',
                                                   font_path=font_path).generate_from_frequencies(Counter(subset_words_list))
                                    ax.imshow(wc, interpolation='bilinear')
                                    ax.axis('off')
                                except Exception as e:
                                    st.error(f"WordCloud(å±æ€§:{val})ã®ç”Ÿæˆã«å¤±æ•—: {e}")
                            
                            st.pyplot(fig_subset_wc)
            
            # --- Tab 3: å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ ---
            with tab3:
                if 'fig_net' not in st.session_state:
                     with st.spinner("å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’è¨ˆç®—ä¸­..."):
                        fig_net, net_error = generate_network(df_analyzed['words'], font_path)
                        st.session_state.fig_net = fig_net
                        st.session_state.net_error = net_error

                if st.session_state.fig_net:
                    st.pyplot(st.session_state.fig_net)
                else:
                    st.warning(st.session_state.net_error)
            
            # --- Tab 4: KWIC (æ–‡è„ˆæ¤œç´¢) ---
            with tab4: 
                st.subheader("KWIC (æ–‡è„ˆæ¤œç´¢)")
                st.info("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã« `*` ã‚’å«ã‚ã‚‹ã¨ãƒ¯ã‚¤ãƒ«ãƒ‰ã‚«ãƒ¼ãƒ‰æ¤œç´¢ãŒå¯èƒ½ã§ã™ (ä¾‹: `é¡§å®¢*`)ã€‚")
                kwic_keyword = st.text_input("æ–‡è„ˆã‚’æ¤œç´¢ã—ãŸã„å˜èªã‚’å…¥åŠ›ã—ã¦ãã ã•ã„", key="kwic_input")
                if kwic_keyword:
                    # KWICã¯æ¯å›å†è¨ˆç®—ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã—ãªã„ï¼‰
                    kwic_html = generate_kwic_html(df_analyzed, text_column, kwic_keyword)
                    st.markdown(kwic_html, unsafe_allow_html=True) 
            
            # --- Tab 5: å±æ€§åˆ¥ ç‰¹å¾´èª ---
            with tab5: 
                st.subheader("å±æ€§åˆ¥ ç‰¹å¾´èªï¼ˆã‚«ã‚¤äºŒä¹—æ¤œå®šï¼‰")
                if not attribute_columns:
                    st.warning("ã“ã®åˆ†æã‚’è¡Œã†ã«ã¯ã€ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§ã€Œåˆ†æè»¸ã€ã‚’1ã¤ä»¥ä¸Šé¸æŠã—ã¦ãã ã•ã„ã€‚")
                else:
                    attr_col_for_chi2 = attribute_columns[0] 
                    st.info(f"å±æ€§ ã€Œ**{attr_col_for_chi2}**ã€ ã®å€¤ã”ã¨ã«ç‰¹å¾´çš„ãªå˜èªã‚’è¨ˆç®—ã—ã¾ã™ã€‚på€¤ãŒ0.05æœªæº€ã®æœ‰æ„ãªå˜èªã‚’è¡¨ç¤ºã—ã¾ã™ã€‚")
                    
                    if 'chi2_results' not in st.session_state:
                        with st.spinner(f"ã€Œ{attr_col_for_chi2}ã€ã®ç‰¹å¾´èªã‚’è¨ˆç®—ä¸­..."):
                            st.session_state.chi2_results = calculate_characteristic_words(df_analyzed, attr_col_for_chi2, text_column)
                    
                    chi2_results = st.session_state.chi2_results
                    if "error" in chi2_results:
                        st.error(chi2_results["error"])
                    else:
                        if not chi2_results:
                            st.info(f"å±æ€§ã€Œ{attr_col_for_chi2}ã€ã«ã¯ã€çµ±è¨ˆçš„ã«æœ‰æ„ãªç‰¹å¾´èªã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
                        else:
                            r = row(*[1 for _ in range(len(chi2_results))], vertical_align="top")
                            for i, (attr_value, words) in enumerate(chi2_results.items()):
                                with r.columns[i]:
                                    st.markdown(f"**{attr_value}** ã®ç‰¹å¾´èª (Top 20)")
                                    if words:
                                        for word, p_value, chi2_val in words:
                                            st.write(f"- {word} (p={p_value:.3f})")
                                    else:
                                        st.info("ç‰¹å¾´èªãªã—")
            
            # --- Tab 6: å¯¾å¿œåˆ†æ ---
            with tab6: 
                st.subheader("å¯¾å¿œåˆ†æï¼ˆã‚³ãƒ¬ã‚¹ãƒãƒ³ãƒ‡ãƒ³ã‚¹åˆ†æï¼‰")
                if not attribute_columns:
                    st.warning("ã“ã®åˆ†æã‚’è¡Œã†ã«ã¯ã€ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§ã€Œåˆ†æè»¸ã€ã‚’1ã¤ä»¥ä¸Šé¸æŠã—ã¦ãã ã•ã„ã€‚")
                else:
                    attr_col_for_ca = attribute_columns[0]
                    st.info(f"å±æ€§ ã€Œ**{attr_col_for_ca}**ã€ ã¨ã€é »å‡ºå˜èª (Top 30) ã¨ã®é–¢ä¿‚æ€§ã‚’ãƒ—ãƒ­ãƒƒãƒˆã—ã¾ã™ã€‚")
                    
                    if 'fig_ca' not in st.session_state:
                        with st.spinner("å¯¾å¿œåˆ†æã‚’è¨ˆç®—ä¸­..."):
                            fig_ca, ca_error = run_correspondence_analysis(df_analyzed, attr_col_for_ca)
                            st.session_state.fig_ca = fig_ca
                            st.session_state.ca_error = ca_error

                    if st.session_state.ca_error:
                        st.error(st.session_state.ca_error)
                    elif st.session_state.fig_ca:
                        st.pyplot(st.session_state.fig_ca)
                    else:
                        st.warning("å¯¾å¿œåˆ†æã®ãƒ—ãƒ­ãƒƒãƒˆã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")

    except Exception as e:
        st.error(f"ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã¾ãŸã¯åˆ†æä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

