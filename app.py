import streamlit as st
import pandas as pd
import re
import requests # Gemini APIå‘¼ã³å‡ºã—ç”¨
import time # ãƒªãƒˆãƒ©ã‚¤ç”¨
from janome.tokenizer import Tokenizer
from wordcloud import WordCloud
import networkx as nx
import matplotlib.pyplot as plt
from collections import Counter
from itertools import combinations
import japanize_matplotlib # Matplotlibã®æ—¥æœ¬èªåŒ–

# --- 1. ã‚¢ãƒ—ãƒªã®åŸºæœ¬è¨­å®š ---
st.set_page_config(page_title="çµ±è¨ˆï¼‹AI çµ±åˆã‚¢ãƒŠãƒ©ã‚¤ã‚¶ãƒ¼", layout="wide")
st.title("çµ±è¨ˆï¼‹AI çµ±åˆãƒ†ã‚­ã‚¹ãƒˆã‚¢ãƒŠãƒ©ã‚¤ã‚¶ãƒ¼ ğŸ“ŠğŸ¤–")
st.write("Excelã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã€ãƒ†ã‚­ã‚¹ãƒˆåˆ—ã¨åˆ†æè»¸ï¼ˆå±æ€§ï¼‰ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚çµ±è¨ˆåˆ†æã¨AIã«ã‚ˆã‚‹è¦ç´„ã‚’åŒæ™‚ã«å®Ÿè¡Œã—ã¾ã™ã€‚")

# --- 2. å½¢æ…‹ç´ è§£æï¼†ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰è¨­å®š (ã‚­ãƒ£ãƒƒã‚·ãƒ¥) ---
@st.cache_resource # å½¢æ…‹ç´ è§£æå™¨ã¯é‡ã„ã®ã§ã‚­ãƒ£ãƒƒã‚·ãƒ¥
def get_tokenizer():
    return Tokenizer()

# ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ï¼ˆä¸€èˆ¬çš„ã™ãã‚‹å˜èªï¼‰
STOPWORDS = set([
    'ã®', 'ã«', 'ã¯', 'ã‚’', 'ãŸ', 'ã§ã™', 'ã¾ã™', 'ãŒ', 'ã§', 'ã‚‚', 'ã¦', 'ã¨', 'ã—', 'ã‚Œ', 'ã•', 'ã‚ã‚‹', 'ã„ã‚‹', 'ã™ã‚‹',
    'ãªã„', 'ã“ã¨', 'ã‚‚ã®', 'ã“ã‚Œ', 'ãã‚Œ', 'ã‚ã‚Œ', 'ã‚ˆã†', 'ãŸã‚', 'äºº', 'ä¸­', 'ç­‰', 'æ€ã†', 'ã„ã†', 'ãªã‚‹', 'æ—¥', 'æ™‚'
])

# ãƒ†ã‚­ã‚¹ãƒˆã‚’å˜èªãƒªã‚¹ãƒˆã«åˆ†å‰²ã™ã‚‹é–¢æ•°
def extract_words(text, tokenizer):
    if not isinstance(text, str):
        return []
    tokens = tokenizer.tokenize(text)
    words = []
    for token in tokens:
        part_of_speech = token.part_of_speech.split(',')[0]
        # åè©ã€å‹•è©ã€å½¢å®¹è©ã®ã¿ã‚’æŠ½å‡º
        if part_of_speech in ['åè©', 'å‹•è©', 'å½¢å®¹è©']:
            # ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ã§ãªãã€1æ–‡å­—ä»¥ä¸Šãªã‚‰è¿½åŠ 
            if token.base_form not in STOPWORDS and len(token.base_form) > 1:
                words.append(token.base_form)
    return words

# --- 3. Gemini AI åˆ†æé–¢æ•° ---
def call_gemini_api(text_data, has_attribute):
    apiKey = "" # APIã‚­ãƒ¼ã¯ç©ºã®ã¾ã¾ã«ã—ã¦ãã ã•ã„
    apiUrl = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-09-2025:generateContent?key={apiKey}"

    attributeInstruction = has_attribute \
        ? "ãƒ‡ãƒ¼ã‚¿ã¯ã€Œå±æ€§ || ãƒ†ã‚­ã‚¹ãƒˆã€ã®å½¢å¼ã§ã™ã€‚å±æ€§ã”ã¨ã®å‚¾å‘ã‚„é•ã„ã«ã‚‚ç€ç›®ã—ã¦åˆ†æã—ã¦ãã ã•ã„ã€‚" \
        : ""
    
    systemPrompt = f"""ã‚ãªãŸã¯ã€ãƒ†ã‚­ã‚¹ãƒˆãƒã‚¤ãƒ‹ãƒ³ã‚°ã®å°‚é–€å®¶ã§ã™ã€‚ä¸ãˆã‚‰ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆç¾¤ã‚’åˆ†æã—ã€çµæœã‚’è©³ç´°ã‹ã¤åˆ†ã‹ã‚Šã‚„ã™ããƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³å½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
{attributeInstruction}
## 1. åˆ†æã‚µãƒãƒªãƒ¼
(å…¨ä½“ã®å‚¾å‘ã‚’ç°¡æ½”ã«è¦ç´„)
## 2. ä¸»è¦ãªãƒ†ãƒ¼ãƒ
(é »å‡ºã™ã‚‹ãƒˆãƒ”ãƒƒã‚¯ã‚„æ„è¦‹ã®ã‚«ãƒ†ã‚´ãƒªã‚’3ã€œ5å€‹æç¤º)
## 3. ãƒã‚¸ãƒ†ã‚£ãƒ–ãªæ„è¦‹
(å…·ä½“çš„ãªè‰¯ã„ç‚¹ã‚’å¼•ç”¨ã—ã¤ã¤ãƒªã‚¹ãƒˆã‚¢ãƒƒãƒ—)
## 4. ãƒã‚¬ãƒ†ã‚£ãƒ–ãªæ„è¦‹ãƒ»èª²é¡Œ
(å…·ä½“çš„ãªä¸æº€ã‚„æ”¹å–„ç‚¹ã‚’å¼•ç”¨ã—ã¤ã¤ãƒªã‚¹ãƒˆã‚¢ãƒƒãƒ—)
{has_attribute and '## 5. å±æ€§åˆ¥ã®å‚¾å‘ (ã‚‚ã—ã‚ã‚Œã°)\n(å±æ€§ã”ã¨ã®ç‰¹å¾´çš„ãªæ„è¦‹ã‚’æ¯”è¼ƒ)'}
## 6. ç·è©•ã¨ãƒã‚¯ã‚¹ãƒˆã‚¢ã‚¯ã‚·ãƒ§ãƒ³
(åˆ†æã‹ã‚‰è¨€ãˆã‚‹ã“ã¨ã€æ¬¡ã«è¡Œã†ã¹ãã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’ææ¡ˆ)
"""
    
    payload = {
        "systemInstruction": {"parts": [{"text": systemPrompt}]},
        "contents": [{"parts": [{"text": textData}]}]
    }

    try:
        response = None
        delay = 1000
        for i in range(5):
            response = requests.post(apiUrl, json=payload, headers={'Content-Type': 'application/json'})
            if response.status_code == 200:
                break
            elif response.status_code == 429 or response.status_code >= 500:
                time.sleep(delay / 1000)
                delay *= 2
            else:
                response.raise_for_status() # ä»–ã®ã‚¨ãƒ©ãƒ¼
        
        if response.status_code != 200:
             return f"AIåˆ†æã«å¤±æ•—ã—ã¾ã—ãŸã€‚ãƒªãƒˆãƒ©ã‚¤å¾Œã‚‚ã‚¨ãƒ©ãƒ¼ãŒè§£æ¶ˆã—ã¾ã›ã‚“ã§ã—ãŸã€‚ (Status: {response.status_code})"

        result = response.json()
        text = result.get('candidates', [{}])[0].get('content', {}).get('parts', [{}])[0].get('text', '')
        return text if text else "AIã‹ã‚‰ã®å¿œç­”ãŒç©ºã§ã—ãŸã€‚"

    except Exception as e:
        return f"AIåˆ†æä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"

# --- 4. ãƒ¡ã‚¤ãƒ³ç”»é¢ã®UI ---
uploaded_file = st.file_uploader("1. Excelãƒ•ã‚¡ã‚¤ãƒ« (xlsx) ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type=["xlsx"])

if uploaded_file:
    try:
        df = pd.read_excel(uploaded_file)
        st.subheader("èª­ã¿è¾¼ã¿ãƒ‡ãƒ¼ã‚¿ (å…ˆé ­5ä»¶)")
        st.dataframe(df.head())
        
        all_columns = df.columns.tolist()
        
        # --- 5. åˆ†æè¨­å®š (ã‚µã‚¤ãƒ‰ãƒãƒ¼) ---
        with st.sidebar:
            st.header("âš™ï¸ åˆ†æè¨­å®š")
            text_column = st.selectbox("åˆ†æã—ãŸã„ãƒ†ã‚­ã‚¹ãƒˆåˆ—", all_columns, index=0)
            
            # è¤‡æ•°å±æ€§ã®é¸æŠã«å¯¾å¿œï¼
            attribute_columns = st.multiselect("åˆ†æè»¸ (è¤‡æ•°OK: ä¾‹: å¹´ä»£, æ€§åˆ¥)", all_columns)
            
            st.markdown("---")
            run_button = st.button("åˆ†æã‚’å®Ÿè¡Œ", type="primary", use_container_width=True)

        # --- 6. åˆ†æå®Ÿè¡Œ ---
        if run_button:
            if not text_column:
                st.error("ã€Œåˆ†æã—ãŸã„ãƒ†ã‚­ã‚¹ãƒˆåˆ—ã€ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚")
            else:
                # --- 6a. çµ±è¨ˆåˆ†æ (å½¢æ…‹ç´ è§£æ) ---
                with st.spinner("ã‚¹ãƒ†ãƒƒãƒ—1/3: å½¢æ…‹ç´ è§£æã‚’å®Ÿè¡Œä¸­... (å…¨ä»¶å‡¦ç†)"):
                    tokenizer = get_tokenizer()
                    # 'words' åˆ—ã«ã€åˆ†å‰²ã—ãŸå˜èªãƒªã‚¹ãƒˆã‚’è¿½åŠ 
                    df['words'] = df[text_column].apply(lambda x: extract_words(x, tokenizer))
                    st.success("å½¢æ…‹ç´ è§£æãŒå®Œäº†ã—ã¾ã—ãŸã€‚")

                # --- 6b. AIåˆ†æ (ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°) ---
                with st.spinner("ã‚¹ãƒ†ãƒƒãƒ—2/3: AIã«ã‚ˆã‚‹è¦ç´„ã‚’ç”Ÿæˆä¸­..."):
                    # AIã«ã¯å…¨ä»¶é€ã‚‹ã¨å¤šã™ãã‚‹ãŸã‚ã€100ä»¶ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
                    sample_df = df.sample(n=min(len(df), 100))
                    
                    def format_for_ai(row):
                        text = row[text_column] or ''
                        attrs = [str(row[col] or 'N/A') for col in attribute_columns]
                        if attrs:
                            return f"[{' | '.join(attrs)}] || {text}"
                        return text
                        
                    ai_input_text = "\n".join(sample_df.apply(format_for_ai, axis=1))
                    ai_result = call_gemini_api(ai_input_text, has_attribute=bool(attribute_columns))
                    st.success("AIè¦ç´„ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")

                # --- 6c. çµ±è¨ˆåˆ†æ (WordCloud, Network) ---
                with st.spinner("ã‚¹ãƒ†ãƒƒãƒ—3/3: çµ±è¨ˆã‚°ãƒ©ãƒ•ã‚’ç”Ÿæˆä¸­..."):
                    # å…¨å˜èªã‚’ãƒ•ãƒ©ãƒƒãƒˆãªãƒªã‚¹ãƒˆã«
                    all_words_list = [word for sublist in df['words'] for word in sublist]
                    
                    # 1. WordCloud
                    fig_wc = None
                    if all_words_list:
                        word_freq = Counter(all_words_list)
                        try:
                            # Streamlit Cloud ã«ã¯æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆãŒãªã„ãŸã‚ã€japanize_matplotlib ã«æœŸå¾…
                            wc = WordCloud(width=800, height=400, background_color='white',
                                           font_path=None).generate_from_frequencies(word_freq)
                            fig_wc, ax = plt.subplots(figsize=(12, 6))
                            ax.imshow(wc, interpolation='bilinear')
                            ax.axis('off')
                        except Exception as e:
                            st.error(f"WordCloudã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚ãƒ•ã‚©ãƒ³ãƒˆã®å•é¡Œã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚ã‚¨ãƒ©ãƒ¼: {e}")

                    # 2. å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
                    fig_net = None
                    co_occur_counter = Counter()
                    for words in df['words']:
                        unique_words = sorted(list(set(words)))
                        for w1, w2 in combinations(unique_words, 2):
                            co_occur_counter[(w1, w2)] += 1
                    
                    top_pairs = co_occur_counter.most_common(50)
                    
                    if top_pairs:
                        G = nx.Graph()
                        for (w1, w2), weight in top_pairs:
                            G.add_edge(w1, w2, weight=weight)
                        
                        fig_net, ax = plt.subplots(figsize=(14, 14))
                        pos = nx.spring_layout(G, k=0.8)
                        edge_weights = [d['weight'] * 0.5 for u,v,d in G.edges(data=True)]
                        
                        nx.draw_networkx_nodes(G, pos, node_size=1500, node_color='lightblue', alpha=0.8)
                        nx.draw_networkx_edges(G, pos, width=edge_weights, alpha=0.4, edge_color='gray')
                        nx.draw_networkx_labels(G, pos, font_size=10) # japanize_matplotlibãŒæ—¥æœ¬èªã‚’è¡¨ç¤º
                        ax.axis('off')
                    
                    st.success("çµ±è¨ˆã‚°ãƒ©ãƒ•ã®ç”ŸæˆãŒå®Œäº†ã—ã¾ã—ãŸã€‚")

                # --- 7. çµæœè¡¨ç¤º (ã‚¿ãƒ–) ---
                st.subheader("ğŸ“Š åˆ†æçµæœ")
                tab1, tab2, tab3 = st.tabs(["ğŸ¤– AI ã‚µãƒãƒªãƒ¼", "â˜ï¸ WordCloud", "ğŸ•¸ï¸ å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯"])
                
                with tab1:
                    st.markdown(ai_result)
                
                with tab2:
                    if fig_wc:
                        st.pyplot(fig_wc)
                    else:
                        st.warning("WordCloudã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
                
                with tab3:
                    if fig_net:
                        st.pyplot(fig_net)
                    else:
                        st.warning("å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")

    except Exception as e:
        st.error(f"ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã¾ãŸã¯åˆ†æä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

