import streamlit as st
import pandas as pd
import re
import requests # Gemini APIå‘¼ã³å‡ºã—ç”¨
import time # ãƒªãƒˆãƒ©ã‚¤ç”¨
from janome.tokenizer import Tokenizer
from wordcloud import WordCloud
import networkx as nx
import matplotlib.pyplot as plt
import matplotlib.font_manager
from collections import Counter
from itertools import combinations
import japanize_matplotlib # Matplotlibã®æ—¥æœ¬èªåŒ–
import numpy as np # --- çµ±è¨ˆè¨ˆç®—ã®ãŸã‚ã«è¿½åŠ  ---
from scipy.stats import chi2_contingency # --- ã‚«ã‚¤äºŒä¹—æ¤œå®šã®ãŸã‚ã«è¿½åŠ  ---
import io # --- ç”»åƒãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ©Ÿèƒ½ã®ãŸã‚ã«è¿½åŠ  ---
import base64 # --- HTMLãƒ¬ãƒãƒ¼ãƒˆã®ç”»åƒåŸ‹ã‚è¾¼ã¿ã®ãŸã‚ã«è¿½åŠ  ---
from streamlit.components.v1 import html # --- â–¼ è¿½åŠ : KWICè¡¨ç¤ºç”¨ã®htmlã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ â–¼ ---

# --- 1. ã‚¢ãƒ—ãƒªã®åŸºæœ¬è¨­å®š ---
st.set_page_config(page_title="çµ±è¨ˆï¼‹AI çµ±åˆã‚¢ãƒŠãƒ©ã‚¤ã‚¶ãƒ¼", layout="wide")
st.title("çµ±è¨ˆï¼‹AI çµ±åˆãƒ†ã‚­ã‚¹ãƒˆã‚¢ãƒŠãƒ©ã‚¤ã‚¶ãƒ¼ ğŸ“ŠğŸ¤–")
st.write("Excelã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã€ãƒ†ã‚­ã‚¹ãƒˆåˆ—ã¨åˆ†æè»¸ï¼ˆå±æ€§ï¼‰ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚çµ±è¨ˆåˆ†æã¨AIã«ã‚ˆã‚‹è¦ç´„ã‚’åŒæ™‚ã«å®Ÿè¡Œã—ã¾ã™ã€‚")

# --- 2. å½¢æ…‹ç´ è§£æï¼†ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰è¨­å®š (ã‚­ãƒ£ãƒƒã‚·ãƒ¥) ---
@st.cache_resource # å½¢æ…‹ç´ è§£æå™¨ã¯é‡ã„ã®ã§ã‚­ãƒ£ãƒƒã‚·ãƒ¥
def get_tokenizer():
    return Tokenizer()

BASE_STOPWORDS = set([
    'ã®', 'ã«', 'ã¯', 'ã‚’', 'ãŸ', 'ã§ã™', 'ã¾ã™', 'ãŒ', 'ã§', 'ã‚‚', 'ã¦', 'ã¨', 'ã—', 'ã‚Œ', 'ã•', 'ã‚ã‚‹', 'ã„ã‚‹', 'ã™ã‚‹',
    'ãªã„', 'ã“ã¨', 'ã‚‚ã®', 'ã“ã‚Œ', 'ãã‚Œ', 'ã‚ã‚Œ', 'ã‚ˆã†', 'ãŸã‚', 'äºº', 'ä¸­', 'ç­‰', 'æ€ã†', 'ã„ã†', 'ãªã‚‹', 'æ—¥', 'æ™‚',
    'ãã ã•ã‚‹', 'ã„ãŸã ã', 'ã—ã‚Œã‚‹', 'ãã‚‹', 'ãŠã‚‹', 'ã‚Œã‚‹', 'ã‚‰ã‚Œã‚‹', 'ã›ã‚‹', 'ã•ã›ã‚‹', 'ã§ãã‚‹', 'ãªã‚‹', 'ã‚„ã‚‹', 'ã„ã',
    'è¡Œã†', 'è¨€ã†', 'ç”³ã—ä¸Šã’ã‚‹', 'ã¾ã„ã‚‹', 'è¦‹ã‚‹', 'ã“ã“', 'ãã“', 'ã‚ãã“', 'ã“ã¡ã‚‰', 'ãã¡ã‚‰', 'ã‚ã¡ã‚‰', 'ã“ã®', 'ãã®', 'ã‚ã®'
])

@st.cache_data # ãƒ†ã‚­ã‚¹ãƒˆã¨Tokenizerã«å¤‰åŒ–ãŒãªã‘ã‚Œã°ã‚­ãƒ£ãƒƒã‚·ãƒ¥
def extract_words(text, _tokenizer): # _tokenizerå¼•æ•°ã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ã‚­ãƒ¼ã¨ã—ã¦ä½¿ã†
    if not isinstance(text, str):
        return []
    tokenizer = get_tokenizer()
    tokens = tokenizer.tokenize(text)
    words = []
    for token in tokens:
        if token.surface.isdigit(): continue
        if token.base_form.isdigit(): continue
        part_of_speech = token.part_of_speech.split(',')[0]
        if part_of_speech in ['åè©', 'å‹•è©', 'å½¢å®¹è©']:
            if len(token.base_form) > 1: # æ–‡å­—æ•°ãƒã‚§ãƒƒã‚¯ã®ã¿è¡Œã†
                 words.append(token.base_form)
    return words

# --- 3. Gemini AI åˆ†æé–¢æ•° ---
SYSTEM_PROMPT_SIMPLE = """ã‚ãªãŸã¯ã€ãƒ†ã‚­ã‚¹ãƒˆãƒã‚¤ãƒ‹ãƒ³ã‚°ã®å°‚é–€å®¶ã§ã™ã€‚ä¸ãˆã‚‰ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆç¾¤ã‚’åˆ†æã—ã€çµæœã‚’è©³ç´°ã‹ã¤åˆ†ã‹ã‚Šã‚„ã™ããƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³å½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
{attributeInstruction}
## 1. åˆ†æã‚µãƒãƒªãƒ¼
(å…¨ä½“ã®å‚¾å‘ã‚’ç°¡æ½”ã«è¦ç´„)
## 2. ä¸»è¦ãªãƒ†ãƒ¼ãƒ
(é »å‡ºã™ã‚‹ãƒˆãƒ”ãƒƒã‚¯ã‚„æ„è¦‹ã®ã‚«ãƒ†ã‚´ãƒªã‚’3ã€œ5å€‹æç¤º)
## 3. ãƒã‚¸ãƒ†ã‚£ãƒ–ãªæ„è¦‹
(å…·ä½“çš„ãªè‰¯ã„ç‚¹ã‚’å¼•ç”¨ã—ã¤ã¤ãƒªã‚¹ãƒˆã‚¢ãƒƒãƒ—)
## 4. ãƒã‚¬ãƒ†ã‚£ãƒ–ãªæ„è¦‹ãƒ»èª²é¡Œ
(å…·ä½“çš„ãªä¸æº€ã‚„æ”¹å–„ç‚¹ã‚’å¼•ç”¨ã—ã¤ã¤ãƒªã‚¹ãƒˆã‚¢ãƒƒãƒ—)
{has_attribute}
## 6. ç·è©•ã¨ãƒã‚¯ã‚¹ãƒˆã‚¢ã‚¯ã‚·ãƒ§ãƒ³
(åˆ†æã‹ã‚‰è¨€ãˆã‚‹ã“ã¨ã€æ¬¡ã«è¡Œã†ã¹ãã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’ææ¡ˆ)
"""
SYSTEM_PROMPT_ACADEMIC = """ã‚ãªãŸã¯ã€ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æã™ã‚‹è¨ˆé‡ãƒ†ã‚­ã‚¹ãƒˆåˆ†æï¼ˆãƒ†ã‚­ã‚¹ãƒˆãƒã‚¤ãƒ‹ãƒ³ã‚°ï¼‰ã®å°‚é–€å®¶ã§ã™ã€‚
ä¸ãˆã‚‰ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆç¾¤ã‚’åˆ†æã—ã€ãã®çµæœã‚’å­¦è¡“è«–æ–‡ã®ã€Œçµæœã€ãŠã‚ˆã³ã€Œè€ƒå¯Ÿã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«è¨˜è¿°ã™ã‚‹ã®ã«é©ã—ãŸã€å®¢è¦³çš„ã‹ã¤ãƒ•ã‚©ãƒ¼ãƒãƒ«ãªæ–‡ä½“ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
{attributeInstruction}
ä»¥ä¸‹ã®æ§‹æˆã«å¾“ã£ã¦ã€ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³å½¢å¼ã§è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚

## 1. åˆ†æã®æ¦‚è¦ (Abstract)
(ãƒ‡ãƒ¼ã‚¿å…¨ä½“ã®ä¸»è¦ãªå‚¾å‘ã‚„ç‰¹ç­†ã™ã¹ãç‚¹ã‚’ã€å®¢è¦³çš„ãªè¦ç´„ã¨ã—ã¦2ã€œ3æ–‡ã§è¨˜è¿°ã™ã‚‹)

## 2. ä¸»è¦ãªçŸ¥è¦‹ (Key Findings)
(ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æŠ½å‡ºã•ã‚ŒãŸä¸»è¦ãªãƒ†ãƒ¼ãƒã‚„ãƒˆãƒ”ãƒƒã‚¯ã€é »å‡ºã™ã‚‹æ„è¦‹ã®ã‚«ãƒ†ã‚´ãƒªã‚’3ã€œ5ç‚¹ã€ç®‡æ¡æ›¸ãã§æç¤ºã™ã‚‹ã€‚å¯èƒ½ã§ã‚ã‚Œã°ã€å…·ä½“çš„ãªãƒ†ã‚­ã‚¹ãƒˆæ–­ç‰‡ã‚’ã€Œã€ã§å¼•ç”¨ã—ã€æ‰€è¦‹ã‚’è£œå¼·ã™ã‚‹)
{has_attribute}
## 4. è€ƒå¯Ÿã¨ä»Šå¾Œã®èª²é¡Œ (Discussion and Limitations)
(åˆ†æçµæœã‹ã‚‰å°ã‹ã‚Œã‚‹è€ƒå¯Ÿã‚„ç¤ºå”†ã‚’è¨˜è¿°ã™ã‚‹ã€‚ã¾ãŸã€ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰è¦‹ã‚‰ã‚Œã‚‹æ½œåœ¨çš„ãªèª²é¡Œã‚„ã€ã•ã‚‰ãªã‚‹åˆ†æã®æ–¹å‘æ€§ã«ã¤ã„ã¦ã‚‚è¨€åŠã™ã‚‹)
"""

@st.cache_data # AIåˆ†æçµæœã‚‚ã‚­ãƒ£ãƒƒã‚·ãƒ¥
def call_gemini_api(text_data, has_attribute, prompt_type='simple'):
    try: apiKey = st.secrets["GEMINI_API_KEY"]
    except Exception: return "AIåˆ†æã‚¨ãƒ©ãƒ¼: Streamlit Cloudã®Secretsã« `GEMINI_API_KEY` ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚"
    if not apiKey: return "AIåˆ†æã‚¨ãƒ©ãƒ¼: Streamlit Cloudã®Secretsã« `GEMINI_API_KEY` ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚"

    apiUrl = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-09-2025:generateContent?key={apiKey}"
    attributeInstruction = "ãƒ‡ãƒ¼ã‚¿ã¯ã€Œå±æ€§ || ãƒ†ã‚­ã‚¹ãƒˆã€ã®å½¢å¼ã§ã™ã€‚å±æ€§ã”ã¨ã®å‚¾å‘ã‚„é•ã„ã«ã‚‚ç€ç›®ã—ã¦åˆ†æã—ã¦ãã ã•ã„ã€‚" if has_attribute else ""

    if prompt_type == 'academic':
        has_attribute_str = "## 3. å±æ€§é–“ã®æ¯”è¼ƒåˆ†æ (Comparative Analysis)\n(å±æ€§ï¼ˆã‚«ãƒ†ã‚´ãƒªï¼‰é–“ã§è¦‹ã‚‰ã‚ŒãŸé¡•è‘—ãªå·®ç•°ã‚„ç‰¹å¾´çš„ãªå‚¾å‘ã«ã¤ã„ã¦ã€å…·ä½“çš„ã«æ¯”è¼ƒãƒ»è¨˜è¿°ã™ã‚‹)" if has_attribute else ""
        systemPrompt = SYSTEM_PROMPT_ACADEMIC.format(attributeInstruction=attributeInstruction, has_attribute=has_attribute_str)
    else:
        has_attribute_str = "## 5. å±æ€§åˆ¥ã®å‚¾å‘ (ã‚‚ã—ã‚ã‚Œã°)\n(å±æ€§ã”ã¨ã®ç‰¹å¾´çš„ãªæ„è¦‹ã‚’æ¯”è¼ƒ)" if has_attribute else ""
        systemPrompt = SYSTEM_PROMPT_SIMPLE.format(attributeInstruction=attributeInstruction, has_attribute=has_attribute_str)

    payload = {"systemInstruction": {"parts": [{"text": systemPrompt}]}, "contents": [{"parts": [{"text": text_data}]}]}

    try:
        response = None; delay = 1000
        for i in range(5):
            response = requests.post(apiUrl, json=payload, headers={'Content-Type': 'application/json'})
            if response.status_code == 200: break
            elif response.status_code == 429 or response.status_code >= 500: time.sleep(delay / 1000); delay *= 2
            else: response.raise_for_status()
        if response.status_code != 200: return f"AIåˆ†æå¤±æ•— (Status: {response.status_code})"

        result = response.json()
        candidates = result.get('candidates')
        if not candidates: return "AIå¿œç­”ã‚¨ãƒ©ãƒ¼: candidates is missing"
        content = candidates[0].get('content')
        if not content or not content.get('parts'): return "AIå¿œç­”ã‚¨ãƒ©ãƒ¼: content or parts is missing"
        text = content['parts'][0].get('text', '')
        if not text: return "AIã‹ã‚‰ã®å¿œç­”ãŒç©ºã§ã—ãŸã€‚"
        return text
    except Exception as e:
        if "403" in str(e): return "AIåˆ†æã‚¨ãƒ©ãƒ¼: 403 Forbidden. APIã‚­ãƒ¼/è¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
        return f"AIåˆ†æã‚¨ãƒ©ãƒ¼: {e}"

# --- 4. KWICï¼ˆæ–‡è„ˆæ¤œç´¢ï¼‰é–¢æ•° ---
def generate_kwic_html(df, text_column, keyword, max_results=100):
    if not keyword: return "<p>ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚</p>"
    try: search_pattern = keyword.replace('*', '.*'); kwic_pattern = re.compile(f'(.{{0,40}})({search_pattern})(.{{0,40}})', re.IGNORECASE)
    except re.error as e: return f"<p>ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}</p>"
    results = []
    for text in df[text_column].dropna():
        for match in kwic_pattern.finditer(text):
            if len(results) >= max_results: break
            left, center, right = match.groups()
            # HTMLã‚¨ã‚¹ã‚±ãƒ¼ãƒ—ã‚’è¿½åŠ 
            left_esc = st.markdown(left).strip() # Streamlitã®æ©Ÿèƒ½ã‚’ä½¿ã£ã¦ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—
            center_esc = st.markdown(center).strip()
            right_esc = st.markdown(right).strip()
            html_row = f'<div style="margin-bottom: 10px; padding: 5px; border-bottom: 1px solid #eee; font-family: sans-serif;"><span style="text-align: right; display: inline-block; width: 45%; color: #555; overflow: hidden; text-overflow: ellipsis; white-space: nowrap;">...{left_esc}</span><span style="background-color: yellow; font-weight: bold; padding: 2px 0;">{center_esc}</span><span style="text-align: left; display: inline-block; width: 45%; color: #555; overflow: hidden; text-overflow: ellipsis; white-space: nowrap;">{right_esc}...</span></div>'
            results.append(html_row)
        if len(results) >= max_results: break
    if not results: return f"<p>ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€Œ{keyword}ã€ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚</p>"
    # ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«å¯èƒ½ãªã‚³ãƒ³ãƒ†ãƒŠã‚’è¿½åŠ 
    return f"<h4>ã€Œ{keyword}ã€ã®æ¤œç´¢çµæœ ({len(results)} ä»¶)</h4><div style='height:400px; overflow-y:scroll; border:1px solid #eee; padding:10px;'>" + "".join(results) + "</div>"


# --- 5. å±æ€§åˆ¥ ç‰¹å¾´èªï¼ˆã‚«ã‚¤äºŒä¹—æ¤œå®šï¼‰é–¢æ•° ---
@st.cache_data
def calculate_characteristic_words(_df, attribute_col, text_col, _stopwords_set):
    results = {}
    try: unique_attrs = _df[attribute_col].dropna().unique()
    except KeyError: return {"error": "å±æ€§åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"}
    if len(unique_attrs) < 2: return {"error": "æ¯”è¼ƒå¯¾è±¡ã®å±æ€§ãŒ2ã¤æœªæº€ã§ã™ã€‚"}
    all_words = set(word for sublist in _df['words'] for word in sublist if word not in _stopwords_set)
    total_docs = len(_df)
    for attr_value in unique_attrs:
        attr_df = _df[_df[attribute_col] == attr_value]; non_attr_df = _df[_df[attribute_col] != attr_value]
        total_docs_in_attr = len(attr_df); total_docs_not_in_attr = total_docs - total_docs_in_attr
        if total_docs_in_attr == 0 or total_docs_not_in_attr == 0: continue
        characteristic_words = []
        for word in all_words:
            a = sum(1 for words_list in attr_df['words'] if word in words_list); b = sum(1 for words_list in non_attr_df['words'] if word in words_list)
            c = total_docs_in_attr - a; d = total_docs_not_in_attr - b
            if (a+b) == 0 or (a+c) == 0 or (b+d) == 0 or (c+d) == 0: continue
            contingency_table = np.array([[a, b], [c, d]])
            try:
                with np.errstate(divide='ignore', invalid='ignore'): chi2, p, dof, expected = chi2_contingency(contingency_table)
                if p < 0.05 and a > expected[0, 0]: characteristic_words.append((word, p, chi2))
            except ValueError: continue
        characteristic_words.sort(key=lambda x: x[1]); results[attr_value] = characteristic_words[:20]
    return results

# --- 7. WordCloudç”Ÿæˆé–¢æ•° ---
def generate_wordcloud(_words_list, font_path, _stopwords_set):
    filtered_words = [word for word in _words_list if word not in _stopwords_set]
    if not filtered_words: return None, "è¡¨ç¤ºã™ã‚‹å˜èªãŒã‚ã‚Šã¾ã›ã‚“ï¼ˆã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰é™¤å»å¾Œï¼‰"
    word_freq = Counter(filtered_words)
    try:
        if not font_path:
            fig_wc, ax = plt.subplots(figsize=(12, 6)); ax.text(0.5, 0.5, "æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“", ha='center', va='center', fontsize=16); ax.axis('off')
            return fig_wc, "æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
        else:
            wc = WordCloud(width=800, height=400, background_color='white', font_path=font_path, max_words=100).generate_from_frequencies(word_freq)
            fig_wc, ax = plt.subplots(figsize=(12, 6)); ax.imshow(wc, interpolation='bilinear'); ax.axis('off')
            return fig_wc, None
    except Exception as e: return None, f"WordCloudç”Ÿæˆå¤±æ•—: {e}"

# --- 8. å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ç”Ÿæˆé–¢æ•° ---
def generate_network(_words_df, font_path, _stopwords_set):
    co_occur_counter = Counter()
    for words in _words_df:
        unique_words = sorted(list(set(word for word in words if word not in _stopwords_set)))
        for w1, w2 in combinations(unique_words, 2): co_occur_counter[(w1, w2)] += 1
    top_pairs = co_occur_counter.most_common(50)
    if top_pairs:
        G = nx.Graph()
        for (w1, w2), weight in top_pairs: G.add_edge(w1, w2, weight=weight)
        fig_net, ax = plt.subplots(figsize=(14, 14)); pos = nx.spring_layout(G, k=0.8, iterations=50)
        nx.draw_networkx_nodes(G, pos, node_size=2000, node_color='lightblue', alpha=0.8)
        edge_weights = [d['weight'] * 0.2 for u,v,d in G.edges(data=True)]
        nx.draw_networkx_edges(G, pos, width=edge_weights, alpha=0.4, edge_color='gray')
        labels_kwargs = {'font_size': 10, 'font_family': 'IPAexGothic'} if font_path else {'font_size': 10}
        nx.draw_networkx_labels(G, pos, **labels_kwargs)
        ax.axis('off'); return fig_net, None
    return None, "å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ç”Ÿæˆä¸å¯ï¼ˆå…±èµ·ãƒšã‚¢ä¸è¶³ï¼‰ã€‚"

# å˜èªé »åº¦è¨ˆç®—é–¢æ•°
def calculate_frequency(_words_list, _stopwords_set, top_n=50):
    filtered_words = [word for word in _words_list if word not in _stopwords_set]
    if not filtered_words: return pd.DataFrame(columns=['Rank', 'Word', 'Frequency'])
    word_freq = Counter(filtered_words).most_common(top_n)
    freq_df = pd.DataFrame(word_freq, columns=['Word', 'Frequency'])
    freq_df['Rank'] = freq_df.index + 1
    return freq_df[['Rank', 'Word', 'Frequency']]

# HTMLãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆé–¢æ•°
def fig_to_base64_png(fig):
    if fig is None: return None
    buf = io.BytesIO(); fig.savefig(buf, format="png", bbox_inches="tight"); buf.seek(0)
    return f"data:image/png;base64,{base64.b64encode(buf.getvalue()).decode('utf-8')}"

def generate_html_report():
    html_parts = ["<!DOCTYPE html><html lang='ja'><head><meta charset='UTF-8'><title>ãƒ†ã‚­ã‚¹ãƒˆåˆ†æãƒ¬ãƒãƒ¼ãƒˆ</title>"]
    html_parts.append("<style>body{font-family:sans-serif;margin:20px}h1,h2,h3{color:#333;border-bottom:1px solid #ccc;padding-bottom:5px}h2{margin-top:30px}.result-section{margin-bottom:30px;padding:15px;border:1px solid #eee;border-radius:5px;background-color:#f9f9f9}img{max-width:100%;height:auto;border:1px solid #ddd;margin-top:10px}table{border-collapse:collapse;width:100%;margin-top:10px}th,td{border:1px solid #ddd;padding:8px;text-align:left}th{background-color:#f2f2f2}pre{background-color:#eee;padding:10px;border-radius:3px;white-space:pre-wrap;word-wrap:break-word}</style>")
    html_parts.append("</head><body><h1>ãƒ†ã‚­ã‚¹ãƒˆåˆ†æãƒ¬ãƒãƒ¼ãƒˆ</h1>")
    if 'ai_result_simple' in st.session_state: html_parts.append(f"<div class='result-section'><h2>ğŸ¤– AI ã‚µãƒãƒªãƒ¼ (ç°¡æ˜“)</h2><pre>{st.session_state.ai_result_simple}</pre></div>")
    if 'fig_wc_display' in st.session_state and st.session_state.fig_wc_display:
        img_base64 = fig_to_base64_png(st.session_state.fig_wc_display);
        if img_base64: html_parts.append(f"<div class='result-section'><h2>â˜ï¸ WordCloud (å…¨ä½“)</h2><img src='{img_base64}' alt='WordCloud Overall'></div>")
    if 'overall_freq_df_display' in st.session_state and not st.session_state.overall_freq_df_display.empty:
        html_parts.append("<div class='result-section'><h2>ğŸ“Š å˜èªé »åº¦ãƒ©ãƒ³ã‚­ãƒ³ã‚° (å…¨ä½“ Top 50)</h2>" + st.session_state.overall_freq_df_display.to_html(index=False) + "</div>")
    if 'fig_net_display' in st.session_state and st.session_state.fig_net_display:
        img_base64 = fig_to_base64_png(st.session_state.fig_net_display);
        if img_base64: html_parts.append(f"<div class='result-section'><h2>ğŸ•¸ï¸ å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯</h2><img src='{img_base64}' alt='Co-occurrence Network'></div>")
    if 'chi2_results_display' in st.session_state and st.session_state.chi2_results_display and "error" not in st.session_state.chi2_results_display:
        attr_col = st.session_state.attribute_columns[0]; html_parts.append(f"<div class='result-section'><h2>ğŸ“ˆ å±æ€§åˆ¥ ç‰¹å¾´èª ({attr_col})</h2>")
        for attr_value, words in st.session_state.chi2_results_display.items():
            html_parts.append(f"<h3>{attr_value}</h3>");
            if words: html_parts.append("<ul>" + "".join(f"<li>{w} (p={p:.3f})</li>" for w, p, c in words) + "</ul>")
            else: html_parts.append("<p>ç‰¹å¾´èªãªã—</p>")
        html_parts.append("</div>")
    if 'ai_result_academic' in st.session_state: html_parts.append(f"<div class='result-section'><h2>ğŸ“ AI å­¦è¡“è«–æ–‡</h2><pre>{st.session_state.ai_result_academic}</pre></div>")
    html_parts.append("</body></html>"); return "".join(html_parts)

# --- 9. ãƒ¡ã‚¤ãƒ³ç”»é¢ã®UI ---
uploaded_file = st.file_uploader("1. Excelãƒ•ã‚¡ã‚¤ãƒ« (xlsx) ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type=["xlsx"])

def fig_to_bytes(fig):
    if fig is None: return None
    buf = io.BytesIO(); fig.savefig(buf, format="png", bbox_inches="tight"); buf.seek(0)
    return buf.getvalue()

if uploaded_file:
    try:
        df = pd.read_excel(uploaded_file)
        if 'df_original' not in st.session_state or not st.session_state.df_original.equals(df):
             st.session_state.clear(); st.session_state.df_original = df
        st.subheader("èª­ã¿è¾¼ã¿ãƒ‡ãƒ¼ã‚¿ (å…ˆé ­5ä»¶)"); st.dataframe(df.head())
        all_columns = df.columns.tolist()

        # --- 10. åˆ†æè¨­å®š (ã‚µã‚¤ãƒ‰ãƒãƒ¼) ---
        with st.sidebar:
            st.header("âš™ï¸ åˆ†æè¨­å®š")
            text_column = st.selectbox("åˆ†æã—ãŸã„ãƒ†ã‚­ã‚¹ãƒˆåˆ—", all_columns, index=0)
            attribute_columns = st.multiselect("åˆ†æè»¸ (è¤‡æ•°OK: ä¾‹: å¹´ä»£, æ€§åˆ¥)", all_columns)
            st.markdown("---")
            run_button = st.button("åˆ†æã‚’å®Ÿè¡Œ", type="primary", use_container_width=True)
            if 'df_analyzed' in st.session_state:
                st.markdown("---"); st.header("ğŸ“Š ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›")
                try: html_content = generate_html_report(); st.download_button("HTMLãƒ¬ãƒãƒ¼ãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", html_content, "text_analysis_report.html", "text/html", use_container_width=True)
                except Exception as e: st.error(f"ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")

        # --- 11. åˆ†æå®Ÿè¡Œ (å½¢æ…‹ç´ è§£æã®ã¿) ---
        if run_button:
            if not text_column: st.error("ã€Œåˆ†æã—ãŸã„ãƒ†ã‚­ã‚¹ãƒˆåˆ—ã€ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚")
            else:
                required_cols = [text_column] + attribute_columns
                if not all(col in df.columns for col in required_cols): st.error("é¸æŠã•ã‚ŒãŸåˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
                else:
                    with st.spinner("ã‚¹ãƒ†ãƒƒãƒ—1/1: å½¢æ…‹ç´ è§£æã‚’å®Ÿè¡Œä¸­..."):
                        df_analyzed = st.session_state.df_original.copy()
                        _tokenizer_instance = get_tokenizer()
                        df_analyzed['words'] = df_analyzed[text_column].apply(lambda x: extract_words(x, _tokenizer_instance))
                        st.session_state.df_analyzed = df_analyzed
                        st.session_state.text_column = text_column
                        st.session_state.attribute_columns = attribute_columns
                        st.session_state.pop('ai_result_simple', None); st.session_state.pop('ai_result_academic', None)
                        st.session_state.pop('fig_wc_display', None); st.session_state.pop('wc_error_display', None)
                        st.session_state.pop('fig_net_display', None); st.session_state.pop('net_error_display', None)
                        st.session_state.pop('chi2_results_display', None); st.session_state.pop('chi2_error_display', None)
                        st.session_state.pop('overall_freq_df_display', None)
                        st.session_state.pop('attribute_freq_dfs_display', None)
                        st.session_state.pop('dynamic_stopwords', None)
                        st.success("å½¢æ…‹ç´ è§£æå®Œäº†ã€‚çµæœã‚¿ãƒ–ã§å„åˆ†æã‚’å®Ÿè¡Œãƒ»è¡¨ç¤ºã—ã¾ã™ã€‚")

        # --- 12. çµæœè¡¨ç¤º (ã‚ªãƒ³ãƒ‡ãƒãƒ³ãƒ‰ + å‹•çš„ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰å¯¾å¿œ) ---
        if 'df_analyzed' in st.session_state:
            st.subheader("ğŸ“Š åˆ†æçµæœ")
            df_analyzed = st.session_state.df_analyzed
            text_column = st.session_state.text_column
            attribute_columns = st.session_state.attribute_columns

            if 'font_path' not in st.session_state:
                try: font_path = matplotlib.font_manager.findfont('IPAexGothic'); st.session_state.font_path = font_path
                except Exception: st.session_state.font_path = None
            font_path = st.session_state.font_path
            if font_path is None: st.warning("æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆ 'IPAexGothic' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚", icon="âš ï¸")

            st.markdown("---")
            st.subheader("âš™ï¸ è¡¨ç¤ºç”¨ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰è¨­å®š")
            st.info("ä»¥ä¸‹ã®å˜èªãƒªã‚¹ãƒˆã¯ã€ä¸‹ã®å„åˆ†æã‚¿ãƒ–ã®è¡¨ç¤ºã«ã®ã¿å½±éŸ¿ã—ã¾ã™ï¼ˆAIåˆ†æã¯é™¤ãï¼‰ã€‚ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã§å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
            if 'dynamic_stopwords' not in st.session_state: st.session_state.dynamic_stopwords = ""
            dynamic_stopwords_input = st.text_area("è¿½åŠ ã®é™¤å¤–èª (ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Š)", value=st.session_state.dynamic_stopwords, key="dynamic_sw_input")
            st.session_state.dynamic_stopwords = dynamic_stopwords_input
            dynamic_sw_set = set(w.strip() for w in st.session_state.dynamic_stopwords.split(',') if w.strip())
            current_stopwords_set = BASE_STOPWORDS.union(dynamic_sw_set)
            st.markdown("---")

            tab_names = ["ğŸ¤– AI ã‚µãƒãƒªãƒ¼ (ç°¡æ˜“)", "â˜ï¸ WordCloud", "ğŸ“Š å˜èªé »åº¦ãƒ©ãƒ³ã‚­ãƒ³ã‚°", "ğŸ•¸ï¸ å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯", "ğŸ” KWIC (æ–‡è„ˆæ¤œç´¢)", "ğŸ“ˆ å±æ€§åˆ¥ ç‰¹å¾´èª", "ğŸ“ AI å­¦è¡“è«–æ–‡"]
            tab1, tab2, tab3, tab4, tab5, tab6, tab7 = st.tabs(tab_names)

            # --- Tab 1: AI ã‚µãƒãƒªãƒ¼ (ç°¡æ˜“) ---
            with tab1:
                if 'ai_result_simple' not in st.session_state:
                    with st.spinner("AIã«ã‚ˆã‚‹è¦ç´„ã‚’ç”Ÿæˆä¸­..."):
                        sample_df = df_analyzed.sample(n=min(len(df_analyzed), 100))
                        def format_for_ai(row):
                            text = row[text_column] or ''; attrs = [str(row[col] or 'N/A') for col in attribute_columns]
                            return f"[{' | '.join(attrs)}] || {text}" if attrs else text
                        ai_input_text = "\n".join(sample_df.apply(format_for_ai, axis=1))
                        st.session_state.ai_result_simple = call_gemini_api(ai_input_text, has_attribute=bool(attribute_columns), prompt_type='simple')
                st.markdown(st.session_state.ai_result_simple)

            # --- Tab 2: WordCloud ---
            with tab2:
                st.subheader("å…¨ä½“ã®WordCloud")
                with st.spinner("WordCloudã‚’ç”Ÿæˆä¸­..."):
                    all_words_list = [word for sublist in df_analyzed['words'] for word in sublist]
                    fig_wc, wc_error = generate_wordcloud(all_words_list, font_path, current_stopwords_set)
                    st.session_state.fig_wc_display = fig_wc
                    st.session_state.wc_error_display = wc_error
                if st.session_state.fig_wc_display:
                    st.pyplot(st.session_state.fig_wc_display)
                    img_bytes = fig_to_bytes(st.session_state.fig_wc_display)
                    if img_bytes: st.download_button("ã“ã®ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ (PNG)", img_bytes, "wordcloud_overall.png", "image/png")
                else: st.warning(st.session_state.wc_error_display)
                with st.expander("åˆ†æãƒ—ãƒ­ã‚»ã‚¹ã¨è«–æ–‡è¨˜è¿°ä¾‹"): st.markdown("...") # çœç•¥
                st.markdown("---")
                st.subheader("å±æ€§åˆ¥ã®WordCloud")
                if not attribute_columns: st.warning("å±æ€§åˆ¥WordCloudã‚’è¡¨ç¤ºã™ã‚‹ã«ã¯åˆ†æè»¸ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚")
                else:
                    selected_attr_for_wc = st.selectbox("WordCloudã®åˆ†æè»¸ã‚’é¸æŠ", attribute_columns, 0, key="wc_attr_select")
                    if selected_attr_for_wc:
                        try: unique_values = sorted(df_analyzed[selected_attr_for_wc].dropna().unique())
                        except TypeError: unique_values = sorted(df_analyzed[selected_attr_for_wc].dropna().astype(str).unique())
                        st.info(f"ã€Œ**{selected_attr_for_wc}**ã€ã®å€¤ã”ã¨ã«WordCloudã‚’ç”Ÿæˆã—ã¾ã™ã€‚")
                        for val in unique_values:
                            st.markdown(f"#### {selected_attr_for_wc} : **{val}**")
                            subset_df = df_analyzed[df_analyzed[selected_attr_for_wc] == val]
                            subset_words_list = [word for sublist in subset_df['words'] for word in sublist]
                            if not subset_words_list: st.info("å˜èªãªã—"); continue
                            fig_subset_wc, wc_subset_error = generate_wordcloud(subset_words_list, font_path, current_stopwords_set)
                            if fig_subset_wc:
                                st.pyplot(fig_subset_wc)
                                img_bytes = fig_to_bytes(fig_subset_wc)
                                if img_bytes: st.download_button(f"ã€Œ{val}ã€ã®ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", img_bytes, f"wordcloud_attr_{val}.png", "image/png")
                            else: st.warning(wc_subset_error)

            # --- Tab 3: å˜èªé »åº¦ãƒ©ãƒ³ã‚­ãƒ³ã‚° ---
            with tab3:
                st.subheader("å…¨ä½“ã®å˜èªé »åº¦ãƒ©ãƒ³ã‚­ãƒ³ã‚° (Top 50)")
                with st.spinner("å…¨ä½“ã®å˜èªé »åº¦ã‚’è¨ˆç®—ä¸­..."):
                    all_words_list = [word for sublist in df_analyzed['words'] for word in sublist]
                    overall_freq_df = calculate_frequency(all_words_list, current_stopwords_set)
                    st.session_state.overall_freq_df_display = overall_freq_df
                st.dataframe(st.session_state.overall_freq_df_display, use_container_width=True)
                with st.expander("åˆ†æãƒ—ãƒ­ã‚»ã‚¹ã¨è«–æ–‡è¨˜è¿°ä¾‹"): st.markdown("...") # çœç•¥
                st.markdown("---")
                st.subheader("å±æ€§åˆ¥ã®å˜èªé »åº¦ãƒ©ãƒ³ã‚­ãƒ³ã‚° (Top 50)")
                if not attribute_columns: st.warning("å±æ€§åˆ¥é »åº¦ã‚’è¡¨ç¤ºã™ã‚‹ã«ã¯åˆ†æè»¸ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚")
                else:
                    selected_attr_for_freq = st.selectbox("é »åº¦ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã®åˆ†æè»¸ã‚’é¸æŠ", attribute_columns, 0, key="freq_attr_select")
                    if selected_attr_for_freq:
                        if ('attribute_freq_dfs_display' not in st.session_state or
                            st.session_state.get('attribute_freq_col_display') != selected_attr_for_freq or
                            st.session_state.get('attribute_freq_sw_display') != st.session_state.dynamic_stopwords):
                            with st.spinner(f"ã€Œ{selected_attr_for_freq}ã€åˆ¥ã®å˜èªé »åº¦ã‚’è¨ˆç®—ä¸­..."):
                                st.session_state.attribute_freq_dfs_display = {}
                                try: unique_values = sorted(df_analyzed[selected_attr_for_freq].dropna().unique())
                                except TypeError: unique_values = sorted(df_analyzed[selected_attr_for_freq].dropna().astype(str).unique())
                                for val in unique_values:
                                    subset_df = df_analyzed[df_analyzed[selected_attr_for_freq] == val]
                                    subset_words_list = [word for sublist in subset_df['words'] for word in sublist]
                                    st.session_state.attribute_freq_dfs_display[val] = calculate_frequency(subset_words_list, current_stopwords_set)
                                st.session_state.attribute_freq_col_display = selected_attr_for_freq
                                st.session_state.attribute_freq_sw_display = st.session_state.dynamic_stopwords
                        attribute_freq_dfs = st.session_state.attribute_freq_dfs_display
                        st.info(f"ã€Œ**{selected_attr_for_freq}**ã€ã®å€¤ã”ã¨ã«å˜èªé »åº¦ãƒ©ãƒ³ã‚­ãƒ³ã‚° (Top 50) ã‚’è¡¨ç¤ºã—ã¾ã™ã€‚")
                        for val, freq_df in attribute_freq_dfs.items():
                             with st.expander(f"å±æ€§: **{val}** ã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°"):
                                if freq_df.empty: st.info("å˜èªãªã—")
                                else: st.dataframe(freq_df, use_container_width=True)

            # --- Tab 4: å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ ---
            with tab4:
                with st.spinner("å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ç”Ÿæˆä¸­..."):
                    fig_net, net_error = generate_network(df_analyzed['words'], font_path, current_stopwords_set)
                    st.session_state.fig_net_display = fig_net
                    st.session_state.net_error_display = net_error
                if st.session_state.fig_net_display:
                    st.pyplot(st.session_state.fig_net_display)
                    img_bytes = fig_to_bytes(st.session_state.fig_net_display)
                    if img_bytes: st.download_button("ã“ã®ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ (PNG)", img_bytes, "network.png", "image/png")
                else: st.warning(st.session_state.net_error_display)
                with st.expander("åˆ†æãƒ—ãƒ­ã‚»ã‚¹ã¨è«–æ–‡è¨˜è¿°ä¾‹"): st.markdown("...") # çœç•¥

            # --- Tab 5: KWIC (æ–‡è„ˆæ¤œç´¢) ---
            with tab5:
                st.subheader("KWIC (æ–‡è„ˆæ¤œç´¢)")
                st.info("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã« `*` ã‚’å«ã‚ã‚‹ã¨ãƒ¯ã‚¤ãƒ«ãƒ‰ã‚«ãƒ¼ãƒ‰æ¤œç´¢ãŒå¯èƒ½ã§ã™ (ä¾‹: `é¡§å®¢*`)ã€‚")
                kwic_keyword = st.text_input("æ–‡è„ˆã‚’æ¤œç´¢ã—ãŸã„å˜èªã‚’å…¥åŠ›ã—ã¦ãã ã•ã„", key="kwic_input")
                if kwic_keyword:
                    kwic_html_content = generate_kwic_html(df_analyzed, text_column, kwic_keyword)
                    # --- â–¼ ä¿®æ­£ç‚¹: st.markdown ã‹ã‚‰ html() ã«æˆ»ã™ â–¼ ---
                    html(kwic_html_content, height=400, scrolling=True)
                    # --- â–² ä¿®æ­£å®Œäº† â–² ---
                with st.expander("åˆ†æãƒ—ãƒ­ã‚»ã‚¹ã¨è«–æ–‡è¨˜è¿°ä¾‹"): st.markdown("...") # çœç•¥

            # --- Tab 6: å±æ€§åˆ¥ ç‰¹å¾´èª ---
            with tab6:
                st.subheader("å±æ€§åˆ¥ ç‰¹å¾´èªï¼ˆã‚«ã‚¤äºŒä¹—æ¤œå®šï¼‰")
                if not attribute_columns: st.warning("ã“ã®åˆ†æã‚’è¡Œã†ã«ã¯åˆ†æè»¸ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚")
                else:
                    attr_col_for_chi2 = attribute_columns[0]
                    st.info(f"å±æ€§ ã€Œ**{attr_col_for_chi2}**ã€ ã®å€¤ã”ã¨ã«ç‰¹å¾´çš„ãªå˜èªã‚’è¨ˆç®—ã—ã¾ã™ã€‚på€¤<0.05ã®æœ‰æ„ãªå˜èªã‚’è¡¨ç¤ºã€‚")
                    if ('chi2_results_display' not in st.session_state or
                        st.session_state.get('chi2_sw_display') != st.session_state.dynamic_stopwords):
                        with st.spinner(f"ã€Œ{attr_col_for_chi2}ã€ã®ç‰¹å¾´èªã‚’è¨ˆç®—ä¸­..."):
                            chi2_results = calculate_characteristic_words(df_analyzed, attr_col_for_chi2, text_column, current_stopwords_set)
                            st.session_state.chi2_results_display = chi2_results
                            st.session_state.chi2_sw_display = st.session_state.dynamic_stopwords
                    chi2_results = st.session_state.chi2_results_display
                    if "error" in chi2_results: st.error(chi2_results["error"])
                    else:
                        if not chi2_results or all(not words for words in chi2_results.values()):
                            st.info(f"å±æ€§ã€Œ{attr_col_for_chi2}ã€ã«ã¯çµ±è¨ˆçš„ã«æœ‰æ„ãªç‰¹å¾´èªã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
                        else:
                            cols = st.columns(len(chi2_results))
                            for i, (attr_value, words) in enumerate(chi2_results.items()):
                                with cols[i % len(cols)]:
                                    st.markdown(f"**{attr_value}** ã®ç‰¹å¾´èª (Top 20)")
                                    if words:
                                        for word, p_value, chi2_val in words: st.write(f"- {word} (p={p_value:.3f})")
                                    else: st.info("ç‰¹å¾´èªãªã—")
                with st.expander("åˆ†æãƒ—ãƒ­ã‚»ã‚¹ã¨è«–æ–‡è¨˜è¿°ä¾‹"): st.markdown("...") # çœç•¥

            # --- Tab 7: AI å­¦è¡“è«–æ–‡ ---
            with tab7:
                st.subheader("AIã«ã‚ˆã‚‹å­¦è¡“è«–æ–‡é¢¨ã‚µãƒãƒªãƒ¼")
                if 'ai_result_academic' not in st.session_state:
                    with st.spinner("AIã«ã‚ˆã‚‹å­¦è¡“è«–æ–‡é¢¨ã®è¦ç´„ã‚’ç”Ÿæˆä¸­..."):
                        sample_df = df_analyzed.sample(n=min(len(df_analyzed), 100))
                        def format_for_ai(row):
                             text = row[text_column] or ''; attrs = [str(row[col] or 'N/A') for col in attribute_columns]
                             return f"[{' | '.join(attrs)}] || {text}" if attrs else text
                        ai_input_text = "\n".join(sample_df.apply(format_for_ai, axis=1))
                        st.session_state.ai_result_academic = call_gemini_api(ai_input_text, has_attribute=bool(attribute_columns), prompt_type='academic')
                st.markdown(st.session_state.ai_result_academic)

    except Exception as e:
        st.error(f"ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã¾ãŸã¯åˆ†æä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

